#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NOC Config Maker - AI Backend Server
Secure OpenAI API integration for RouterOS config generation and validation
"""

import sys
import io
# Fix Windows console encoding for Unicode - but only if not already wrapped
# In PyInstaller, stdout/stderr might already be wrapped, so check first
if sys.platform == 'win32':
    try:
        # Only wrap if not already a TextIOWrapper
        if not isinstance(sys.stdout, io.TextIOWrapper):
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        if not isinstance(sys.stderr, io.TextIOWrapper):
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    except (AttributeError, ValueError):
        # If wrapping fails (e.g., in PyInstaller), just continue
        pass

from flask import Flask, request, jsonify, send_file, send_from_directory
from flask import make_response, abort, Response
from flask_cors import CORS
import os
import builtins
try:
    from dotenv import load_dotenv
    if load_dotenv():
        print("[ENV] Loaded .env")
except Exception:
    pass
import re
import ipaddress
import json
import requests
import zipfile
from datetime import datetime, timedelta, timezone
import time
try:
    from zoneinfo import ZoneInfo
except ImportError:
    ZoneInfo = None
import sqlite3
import hashlib
import secrets
from functools import wraps
import threading
import queue
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from ftth_renderer import render_ftth_config

# JWT support (optional - install with: pip install PyJWT)
try:
    import jwt
    HAS_JWT = True
except ImportError:
    HAS_JWT = False
    print("[WARNING] PyJWT not installed. Install with: pip install PyJWT")
    print("[WARNING] Authentication will use simple token system instead.")
try:
    import pytz
except ImportError:
    pytz = None

# Allow tests or alternate environments to override timezone name
TIMEZONE_NAME = os.environ.get('TIMEZONE_NAME', 'America/Chicago')

# When running automated tests, skip heavy timezone resource loading which can hang on
# some Windows environments where tzdata resource loading is slow or blocked.
if os.environ.get('NOC_CONFIGMAKER_TESTS') == '1':
    CST_ZONEINFO = None
    CST_PYTZ = None
else:
    if ZoneInfo is not None:
        try:
            CST_ZONEINFO = ZoneInfo(TIMEZONE_NAME)
        except Exception:
            CST_ZONEINFO = None
    else:
        CST_ZONEINFO = None

    if pytz is not None:
        try:
            CST_PYTZ = pytz.timezone(TIMEZONE_NAME)
        except Exception:
            CST_PYTZ = None
    else:
        CST_PYTZ = None


def _manual_cst_now():
    """Fallback CST/CDT calculation when zoneinfo/pytz are unavailable."""
    utc_now = datetime.now(timezone.utc)
    # Approximate DST window (Mar-Nov). Good enough until tzdata available.
    is_dst = 3 <= utc_now.month <= 11
    offset_hours = -5 if is_dst else -6
    return utc_now + timedelta(hours=offset_hours)


def get_cst_now():
    """Get current time in CST/CDT timezone (America/Chicago)."""
    if CST_ZONEINFO is not None:
        return datetime.now(timezone.utc).astimezone(CST_ZONEINFO)
    if CST_PYTZ is not None:
        return pytz.utc.localize(datetime.utcnow()).astimezone(CST_PYTZ)
    return _manual_cst_now()


def get_cst_timestamp():
    """Get current timestamp in CST/CDT as ISO format string."""
    return get_cst_now().isoformat()


def get_cst_datetime_string():
    """Get current datetime in CST/CDT as formatted string (YYYY-MM-DD HH:MM:SS)."""
    return get_cst_now().strftime('%Y-%m-%d %H:%M:%S')


def get_utc_now():
    return datetime.now(timezone.utc)


def get_utc_timestamp():
    """Get current timestamp in UTC as ISO string with Z suffix."""
    return get_utc_now().isoformat().replace('+00:00', 'Z')


def get_unix_timestamp():
    return int(get_utc_now().timestamp())


from pathlib import Path
# Import NextLink standards (for migration rules, error detection, etc.)
from nextlink_standards import (
    NEXTLINK_COMMON_ERRORS,
    NEXTLINK_NAMING,
    NEXTLINK_IP_RANGES,
    NEXTLINK_ROUTEROS_VERSIONS,
    NEXTLINK_MIGRATION_6X_TO_7X,
    NEXTLINK_DEVICE_ROLES,
    NEXTLINK_AUTO_DETECTABLE_ERRORS
)

# Import NextLink enterprise reference (standard blocks for non-MPLS)
try:
    from nextlink_enterprise_reference import get_all_standard_blocks
    HAS_REFERENCE = True
except ImportError:
    HAS_REFERENCE = False

# Import NextLink compliance reference (RFC-09-10-25)
try:
    from nextlink_compliance_reference import get_all_compliance_blocks, validate_compliance
    HAS_COMPLIANCE = True
except ImportError:
    HAS_COMPLIANCE = False
    # Compliance reference not available - RFC-09-10-25 compliance will not be enforced

# Aviat backhaul updater integration (merged into NOC backend)
try:
    from aviat_config import (
        process_radio as aviat_process_radio,
        process_radios_parallel as aviat_process_radios_parallel,
        check_device_status as aviat_check_device_status,
        CONFIG as AVIAT_CONFIG,
        get_firmware_version as aviat_get_firmware_version,
        get_inactive_firmware_version as aviat_get_inactive_firmware_version,
        get_uptime_days as aviat_get_uptime_days,
        AviatSSHClient,
        wait_for_device_ready_and_reconnect,
    )
    HAS_AVIAT = True
except Exception:
    HAS_AVIAT = False

# Safe print function for PyInstaller compatibility (defined early for use throughout)
def safe_print(*args, **kwargs):
    """Print with error handling for PyInstaller compatibility"""
    try:
        builtins.print(*args, **kwargs, flush=True)
    except UnicodeEncodeError:
        try:
            encoding = getattr(sys.stdout, "encoding", None) or "utf-8"
            msg = " ".join(str(a) for a in args)
            if hasattr(sys.stdout, "buffer"):
                sys.stdout.buffer.write(msg.encode(encoding, errors="replace") + b"\n")
                sys.stdout.flush()
            else:
                builtins.print(msg.encode(encoding, errors="replace").decode(encoding, errors="replace"), flush=True)
        except Exception:
            pass
    except (IOError, ValueError, OSError):
        # If stdout is closed or unavailable, silently skip
        pass

# Make the module's print resilient on Windows consoles that can't encode certain Unicode characters.
print = safe_print

# ========================================
# AVIAT BACKHAUL UPDATER STATE
# ========================================
aviat_tasks = {}
aviat_log_queues = {}
aviat_global_log_queues = set()
aviat_global_log_history = []
aviat_shared_queue = []
AVIAT_SHARED_QUEUE_STORE = Path(__file__).resolve().parent / "aviat_shared_queue.json"
AVIAT_GLOBAL_LOG_LIMIT = 2000
aviat_scheduled_queue = []
AVIAT_SCHEDULED_STORE = Path(__file__).resolve().parent / "aviat_scheduled_queue.json"
aviat_loading_queue = []
AVIAT_LOADING_STORE = Path(__file__).resolve().parent / "aviat_loading_queue.json"
aviat_reboot_queue = []
AVIAT_REBOOT_STORE = Path(__file__).resolve().parent / "aviat_reboot_queue.json"
aviat_activation_lock = threading.Lock()
aviat_loading_lock = threading.Lock()
AVIAT_AUTO_ACTIVATE = os.getenv("AVIAT_AUTO_ACTIVATE", "true").lower() in ("1", "true", "yes")
AVIAT_AUTO_ACTIVATE_POLL = int(os.getenv("AVIAT_AUTO_ACTIVATE_POLL", "60"))
AVIAT_LOADING_CHECK_INTERVAL = int(os.getenv("AVIAT_LOADING_CHECK_INTERVAL", "900"))
AVIAT_LOADING_MAX_WAIT = int(os.getenv("AVIAT_LOADING_MAX_WAIT", "5400"))


def _aviat_load_scheduled_queue():
    if not AVIAT_SCHEDULED_STORE.exists():
        return
    try:
        with open(AVIAT_SCHEDULED_STORE, "r", encoding="utf-8") as handle:
            data = json.load(handle)
        if isinstance(data, list):
            aviat_scheduled_queue.extend(data)
    except Exception:
        pass


def _aviat_save_scheduled_queue():
    try:
        with open(AVIAT_SCHEDULED_STORE, "w", encoding="utf-8") as handle:
            json.dump(aviat_scheduled_queue, handle)
    except Exception:
        pass


_aviat_load_scheduled_queue()


def _aviat_load_loading_queue():
    if not AVIAT_LOADING_STORE.exists():
        return
    try:
        with open(AVIAT_LOADING_STORE, "r", encoding="utf-8") as handle:
            data = json.load(handle)
        if isinstance(data, list):
            aviat_loading_queue.extend(data)
    except Exception:
        pass


def _aviat_save_loading_queue():
    try:
        with open(AVIAT_LOADING_STORE, "w", encoding="utf-8") as handle:
            json.dump(aviat_loading_queue, handle)
    except Exception:
        pass


_aviat_load_loading_queue()

def _aviat_load_reboot_queue():
    if not AVIAT_REBOOT_STORE.exists():
        return
    try:
        with AVIAT_REBOOT_STORE.open("r") as handle:
            data = json.load(handle)
        if isinstance(data, list):
            aviat_reboot_queue.extend(data)
    except Exception:
        pass

def _aviat_save_reboot_queue():
    try:
        with AVIAT_REBOOT_STORE.open("w") as handle:
            json.dump(aviat_reboot_queue, handle)
    except Exception:
        pass

_aviat_load_reboot_queue()

def _aviat_load_shared_queue():
    if not AVIAT_SHARED_QUEUE_STORE.exists():
        return
    try:
        with AVIAT_SHARED_QUEUE_STORE.open("r") as handle:
            data = json.load(handle) or []
        aviat_shared_queue.clear()
        aviat_shared_queue.extend(data)
    except Exception:
        pass

def _aviat_save_shared_queue():
    try:
        with AVIAT_SHARED_QUEUE_STORE.open("w") as handle:
            json.dump(aviat_shared_queue, handle)
    except Exception:
        pass

def _aviat_queue_find(ip):
    for entry in aviat_shared_queue:
        if entry.get("ip") == ip:
            return entry
    return None

def _aviat_queue_upsert(ip, updates=None):
    entry = _aviat_queue_find(ip)
    if entry is None:
        entry = {"ip": ip}
        aviat_shared_queue.append(entry)
    if updates:
        entry.update(updates)
    entry["updated_at"] = datetime.utcnow().isoformat() + "Z"
    return entry

def _aviat_expand_tasks(task_types):
    if not task_types:
        return []
    tasks = list(task_types)
    if "all" in tasks:
        return ["firmware", "password", "snmp", "buffer", "sop", "activate"]
    return [t for t in tasks if t]


def _aviat_clean_remaining_tasks(task_types):
    if not task_types:
        return []
    # For scheduled/loaded flows, firmware + activate already handled.
    return [task for task in _aviat_expand_tasks(task_types) if task not in ("firmware", "activate")]


def _aviat_remaining_tasks_for_reboot(task_types):
    # For reboot-required flows, keep firmware/activate so the upgrade continues after reboot.
    return _aviat_expand_tasks(task_types)

def _aviat_queue_remove(ip):
    aviat_shared_queue[:] = [e for e in aviat_shared_queue if e.get("ip") != ip]

def _aviat_status_from_result(result):
    status = (result or {}).get("status")
    success = result.get("success")
    if status == "reboot_required":
        return "reboot_required"
    if status in ("scheduled", "manual", "loading"):
        return status
    if status == "aborted":
        return "aborted"
    if success:
        return "success"
    return "error"

def _aviat_substatus(flag, scheduled=False, loading=False):
    if loading:
        return "loading"
    if scheduled:
        return "scheduled"
    return "success" if flag else "pending"


def _aviat_extract_version(value):
    if not value:
        return None
    match = re.search(r"\b(\d+\.\d+\.\d+)\b", str(value))
    return match.group(1) if match else None


def _aviat_version_tuple(version):
    if not version:
        return (0, 0, 0)
    parts = [int(p) for p in re.findall(r"\d+", str(version))[:3]]
    while len(parts) < 3:
        parts.append(0)
    return tuple(parts)

def _aviat_queue_update_from_result(result, username=None):
    if not result:
        return
    ip = result.get("ip")
    if not ip:
        return
    status = _aviat_status_from_result(result)
    updates = {
        "status": status,
        "firmwareStatus": _aviat_substatus(
            result.get("firmware_downloaded") or result.get("firmware_activated"),
            scheduled=result.get("firmware_scheduled"),
            loading=status == "loading",
        ),
        "passwordStatus": _aviat_substatus(result.get("password_changed")),
        "snmpStatus": _aviat_substatus(result.get("snmp_configured")),
        "bufferStatus": _aviat_substatus(result.get("buffer_configured")),
        "sopStatus": _aviat_substatus(result.get("sop_passed")),
    }
    if username:
        updates["username"] = username
    _aviat_queue_upsert(ip, updates)

_aviat_load_shared_queue()

def _short_username(raw):
    if not raw:
        return "unknown"
    if "@" in raw:
        return raw.split("@", 1)[0]
    return raw

def _aviat_connect_with_fallback(ip, callback=None):
    client = None
    try:
        client = AviatSSHClient(
            ip,
            username=AVIAT_CONFIG.default_username,
            password=AVIAT_CONFIG.new_password,
        )
        client.connect()
        if callback:
            callback(f"[{ip}] Connected with new password", "info")
        return client
    except Exception:
        if client:
            try:
                client.close()
            except Exception:
                pass
    client = AviatSSHClient(
        ip,
        username=AVIAT_CONFIG.default_username,
        password=AVIAT_CONFIG.default_password,
    )
    client.connect()
    if callback:
        callback(f"[{ip}] Connected with default password", "info")
    return client

def _aviat_reboot_device(ip, callback=None):
    client = None
    try:
        client = _aviat_connect_with_fallback(ip, callback=callback)
        # Aviat WTM uses "restart" with confirmation.
        output = client.send_command("restart", wait_for=['?', ':', '#', '>', ']'], timeout=8)
        if "Are you sure" in output or "no,yes" in output or "[no,yes]" in output:
            client.send_command("yes", wait_for=['#', '>', ']'], timeout=5)
        if callback:
            callback(f"[{ip}] Reboot command sent.", "info")
        return True, None
    except Exception as e:
        if callback:
            callback(f"[{ip}] Reboot failed: {e}", "error")
        return False, str(e)
    finally:
        if client:
            try:
                client.close()
            except Exception:
                pass


def _aviat_loading_check_loop():
    while True:
        if not HAS_AVIAT:
            time.sleep(AVIAT_LOADING_CHECK_INTERVAL)
            continue
        if not aviat_loading_queue:
            time.sleep(AVIAT_LOADING_CHECK_INTERVAL)
            continue
        now = datetime.utcnow()
        to_schedule = []
        still_loading = []
        failed = []
        with aviat_loading_lock:
            for entry in list(aviat_loading_queue):
                ip = entry.get("ip")
                if not ip:
                    continue
                started_at = entry.get("started_at")
                if started_at:
                    try:
                        started_dt = datetime.fromisoformat(started_at.replace("Z", ""))
                        if (now - started_dt).total_seconds() > AVIAT_LOADING_MAX_WAIT:
                            failed.append(entry)
                            continue
                    except Exception:
                        pass
                try:
                    def local_log(message, level):
                        _aviat_broadcast_log(message, level)
                    client = _aviat_connect_with_fallback(ip, callback=local_log)
                    active_version = aviat_get_firmware_version(client, callback=local_log)
                    inactive_raw = aviat_get_inactive_firmware_version(client, callback=local_log)
                    # Enforce reboot-first if uptime exceeds threshold
                    try:
                        uptime_days = aviat_get_uptime_days(client, callback=local_log)
                    except Exception:
                        uptime_days = None
                    client.close()
                except Exception as exc:
                    _aviat_broadcast_log(f"[{ip}] Loading check failed: {exc}", "warning")
                    still_loading.append(entry)
                    continue
                if uptime_days is not None and uptime_days > 250:
                    _aviat_broadcast_log(
                        f"[{ip}] Uptime {uptime_days} days exceeds 250; reboot required before upgrade.",
                        "warning",
                    )
                    if not any(e.get("ip") == ip for e in aviat_reboot_queue):
                        aviat_reboot_queue.append({
                            "ip": ip,
                            "reason": f"Uptime {uptime_days} days exceeds 250; reboot required before upgrade.",
                            "remaining_tasks": entry.get("remaining_tasks", []),
                            "maintenance_params": entry.get("maintenance_params", {}),
                            "username": entry.get("username") or "aviat-tool",
                            "started_at": datetime.utcnow().isoformat() + "Z",
                        })
                    _aviat_queue_upsert(ip, {
                        "status": "reboot_required",
                        "username": entry.get("username") or "aviat-tool",
                    })
                    _aviat_save_reboot_queue()
                    continue
                if isinstance(inactive_raw, str) and inactive_raw.lower() == "loadok":
                    _aviat_broadcast_log(
                        f"[{ip}] Firmware loadOk detected; moving to scheduled queue.",
                        "success",
                    )
                    to_schedule.append(entry)
                    continue
                inactive_version = _aviat_extract_version(inactive_raw)
                active_version = _aviat_extract_version(active_version)
                target_version = _aviat_extract_version(entry.get("target_version"))
                ready_versions = {
                    _aviat_extract_version(AVIAT_CONFIG.firmware_baseline_version),
                    _aviat_extract_version(AVIAT_CONFIG.firmware_final_version),
                }
                if target_version:
                    ready_versions.add(target_version)

                # If inactive firmware is ready, move to scheduled (activation) regardless of active version.
                if inactive_version and inactive_version in ready_versions:
                    _aviat_broadcast_log(
                        f"[{ip}] Firmware {inactive_version} loaded (inactive). Moving to scheduled queue.",
                        "success",
                    )
                    to_schedule.append(entry)
                    continue

                # If active firmware already meets target (or final), remove from loading queue.
                if target_version and active_version and _aviat_version_tuple(active_version) >= _aviat_version_tuple(target_version):
                    _aviat_broadcast_log(
                        f"[{ip}] Active firmware {active_version} already applied; removing from loading queue.",
                        "success",
                    )
                    _aviat_queue_upsert(ip, {
                        "status": "pending",
                        "firmwareStatus": "success",
                        "username": entry.get("username") or "aviat-tool",
                    })
                    continue
                if active_version and _aviat_version_tuple(active_version) >= _aviat_version_tuple(AVIAT_CONFIG.firmware_final_version):
                    _aviat_broadcast_log(
                        f"[{ip}] Active firmware {active_version} already final; removing from loading queue.",
                        "success",
                    )
                    _aviat_queue_upsert(ip, {
                        "status": "pending",
                        "firmwareStatus": "success",
                        "username": entry.get("username") or "aviat-tool",
                    })
                    continue
                if (
                    not inactive_version
                    or str(inactive_version).lower() in ("none", "unknown", "0.0.0", "0")
                ):
                    still_loading.append(entry)
                    _aviat_broadcast_log(
                        f"[{ip}] Inactive firmware version unknown; keeping in loading queue.",
                        "warning",
                    )
                    continue
                still_loading.append(entry)
                _aviat_broadcast_log(
                    f"[{ip}] Firmware still loading; next check in {AVIAT_LOADING_CHECK_INTERVAL // 60} min.",
                    "info",
                )

            if failed:
                for entry in failed:
                    ip = entry.get("ip")
                    if ip:
                        _aviat_queue_upsert(ip, {
                            "status": "error",
                            "firmwareStatus": "error",
                            "username": entry.get("username") or "aviat-tool",
                        })
                        _aviat_broadcast_log(
                            f"[{ip}] Firmware load timed out after {AVIAT_LOADING_MAX_WAIT // 60} min.",
                            "error",
                        )
                _aviat_save_shared_queue()

            if to_schedule:
                for entry in to_schedule:
                    remaining_tasks = _aviat_clean_remaining_tasks(entry.get("remaining_tasks", []))
                    aviat_scheduled_queue.append({
                        "ip": entry.get("ip"),
                        "remaining_tasks": remaining_tasks,
                        "maintenance_params": entry.get("maintenance_params", {}),
                        "activation_at": entry.get("activation_at"),
                        "username": entry.get("username") or "aviat-tool",
                    })
                    _aviat_queue_upsert(entry.get("ip"), {
                        "status": "scheduled",
                        "firmwareStatus": "scheduled",
                        "username": entry.get("username") or "aviat-tool",
                    })
                _aviat_save_scheduled_queue()
                _aviat_save_shared_queue()

            aviat_loading_queue[:] = still_loading
            _aviat_save_loading_queue()

        if aviat_loading_queue:
            next_check = get_cst_now() + timedelta(seconds=AVIAT_LOADING_CHECK_INTERVAL)
            _aviat_broadcast_log(
                f"[AVIAT] Next loading check at {next_check.strftime('%H:%M')} CST",
                "info",
            )

        time.sleep(AVIAT_LOADING_CHECK_INTERVAL)

def _aviat_activate_entries(task_id, to_activate, username=None):
    aviat_tasks[task_id]['status'] = 'running'

    def log_callback(message, level):
        _aviat_broadcast_log(message, level, task_id=task_id)
        if task_id in aviat_log_queues:
            aviat_log_queues[task_id].put({'message': message, 'level': level})

    activation_limit = int(os.environ.get("AVIAT_ACTIVATION_MAX", "20"))
    worker_count = max(1, min(len(to_activate), activation_limit))

    def run_activation(entry):
        ip = entry["ip"]
        # Activation should be isolated; do not re-run other steps.
        full_tasks = ["activate"]
        result = aviat_process_radio(
            ip,
            full_tasks,
            callback=log_callback,
            maintenance_params=entry.get("maintenance_params", {}),
        )
        return entry, result

    with ThreadPoolExecutor(max_workers=worker_count) as executor:
        futures = [executor.submit(run_activation, entry) for entry in to_activate]
        for future in as_completed(futures):
            entry, result = future.result()
            ip = entry["ip"]
            remaining_tasks = _aviat_clean_remaining_tasks(entry.get("remaining_tasks", []))
            aviat_tasks[task_id]['results'].append({
                'ip': ip,
                'username': entry.get("username") or username,
                'success': result.success,
                'status': result.status,
                'firmware_downloaded': result.firmware_downloaded,
                'firmware_scheduled': result.firmware_scheduled,
                'firmware_activated': result.firmware_activated,
                'password_changed': result.password_changed,
                'snmp_configured': result.snmp_configured,
                'buffer_configured': result.buffer_configured,
                'sop_checked': result.sop_checked,
                'sop_passed': result.sop_passed,
                'sop_results': result.sop_results,
                'firmware_version_before': result.firmware_version_before,
                'firmware_version_after': result.firmware_version_after,
                'error': result.error
            })
            _log_aviat_activity({
                'ip': ip,
                'username': entry.get("username") or username,
                'firmware_version_after': result.firmware_version_after,
                'firmware_version_before': result.firmware_version_before,
                'success': result.success,
                'status': result.status
            })
            _aviat_queue_update_from_result(
                _aviat_result_dict(result, username=entry.get("username") or username),
                username=entry.get("username") or username,
            )
            if result.status == "loading":
                aviat_loading_queue.append({
                    "ip": ip,
                    "remaining_tasks": remaining_tasks,
                    "maintenance_params": entry.get("maintenance_params", {}),
                    "activation_at": entry.get("activation_at"),
                    "username": entry.get("username") or username or "aviat-tool",
                    "target_version": result.firmware_downloaded_version,
                    "started_at": datetime.utcnow().isoformat() + "Z",
                })
                _aviat_save_loading_queue()
                _aviat_queue_upsert(ip, {
                    "status": "loading",
                    "firmwareStatus": "loading",
                    "username": entry.get("username") or username or "aviat-tool",
                })
                _aviat_save_shared_queue()
            elif result.firmware_scheduled and result.status == "scheduled":
                aviat_scheduled_queue.append({
                    "ip": ip,
                    "remaining_tasks": remaining_tasks,
                    "maintenance_params": entry.get("maintenance_params", {}),
                    "activation_at": entry.get("activation_at"),
                    "username": entry.get("username") or username or "aviat-tool",
                })
                _aviat_save_scheduled_queue()
                _aviat_queue_upsert(ip, {
                    "status": "scheduled",
                    "username": entry.get("username") or username or "aviat-tool",
                })
                _aviat_save_shared_queue()

    aviat_tasks[task_id]['status'] = 'completed'
    _aviat_save_shared_queue()
    if task_id in aviat_log_queues:
        aviat_log_queues[task_id].put(None)

def _aviat_auto_activate_loop():
    while True:
        time.sleep(AVIAT_AUTO_ACTIVATE_POLL)
        if not AVIAT_AUTO_ACTIVATE:
            continue
        if not HAS_AVIAT:
            continue
        now = datetime.now()
        if not (2 <= now.hour < 5):
            continue
        if not aviat_scheduled_queue:
            continue
        if not aviat_activation_lock.acquire(blocking=False):
            continue
        try:
            to_activate = []
            remaining = []
            for entry in aviat_scheduled_queue:
                activation_at = entry.get("activation_at")
                if activation_at:
                    try:
                        if datetime.fromisoformat(activation_at.replace("Z", "")) <= now:
                            to_activate.append(entry)
                        else:
                            remaining.append(entry)
                    except Exception:
                        to_activate.append(entry)
                else:
                    to_activate.append(entry)
            if not to_activate:
                continue
            aviat_scheduled_queue[:] = remaining
            _aviat_save_scheduled_queue()
            for entry in to_activate:
                _aviat_queue_upsert(entry["ip"], {
                    "status": "processing",
                    "username": entry.get("username") or "aviat-tool",
                })
            _aviat_save_shared_queue()
            task_id = f"auto-{uuid.uuid4()}"
            aviat_tasks[task_id] = {
                'status': 'pending',
                'abort': False,
                'ips': [x["ip"] for x in to_activate],
                'tasks': ['activate'],
                'results': []
            }
            aviat_log_queues[task_id] = queue.Queue()
            threading.Thread(
                target=_aviat_activate_entries,
                args=(task_id, to_activate),
                daemon=True,
            ).start()
        finally:
            aviat_activation_lock.release()

# ========================================
# ROUTERBOARD INTERFACE DATABASE
# ========================================
# Comprehensive database of all RouterBoard models with accurate interface layouts
# Updated: 2024-12-09 - Covers CCR, CRS, and RB series

ROUTERBOARD_INTERFACES = {
    # CCR1036 Series
    'CCR1036-12G-4S': {
        'model': 'CCR1036-12G-4S',
        'series': 'CCR1036',
        'cpu': 'Tilera Tile-Gx36',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6',
                           'ether7', 'ether8', 'ether9', 'ether10', 'ether11', 'ether12'],
            'sfp_1g': ['sfp1', 'sfp2', 'sfp3', 'sfp4']
        },
        'total_ports': 16,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-3': 'Uplinks',
            'ether4-12': 'Customer/Sector connections',
            'sfp1-4': 'Fiber uplinks or long-distance'
        }
    },
    
    # CCR2004 Series
    'CCR2004-1G-12S+2XS': {
        'model': 'CCR2004-1G-12S+2XS',
        'series': 'CCR2004',
        'cpu': 'Annapurna Labs Alpine v2',
        'ports': {
            'ethernet_1g': ['ether1'],
            'sfp_plus_10g': ['sfp-sfpplus1', 'sfp-sfpplus2', 'sfp-sfpplus3', 'sfp-sfpplus4',
                            'sfp-sfpplus5', 'sfp-sfpplus6', 'sfp-sfpplus7', 'sfp-sfpplus8',
                            'sfp-sfpplus9', 'sfp-sfpplus10', 'sfp-sfpplus11', 'sfp-sfpplus12'],
            'sfp28_25g': ['sfp28-1', 'sfp28-2']
        },
        'total_ports': 15,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'sfp-sfpplus1-2': 'Uplinks (10G)',
            'sfp-sfpplus3-12': 'Customer/Sector connections (10G)',
            'sfp28-1-2': 'High-speed uplinks (25G)'
        }
    },
    
    'CCR2004-16G-2S+': {
        'model': 'CCR2004-16G-2S+',
        'series': 'CCR2004',
        'cpu': 'Annapurna Labs Alpine v2',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6',
                           'ether7', 'ether8', 'ether9', 'ether10', 'ether11', 'ether12',
                           'ether13', 'ether14', 'ether15', 'ether16'],
            'sfp_plus_10g': ['sfp-sfpplus1', 'sfp-sfpplus2']
        },
        'total_ports': 18,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-16': 'Customer connections',
            'sfp-sfpplus1-2': 'Uplinks (10G)'
        }
    },
    
    # CCR2116 Series
    'CCR2116-12G-4S+': {
        'model': 'CCR2116-12G-4S+',
        'series': 'CCR2116',
        'cpu': 'Annapurna Labs Alpine v2',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6',
                           'ether7', 'ether8', 'ether9', 'ether10', 'ether11', 'ether12'],
            'sfp_plus_10g': ['sfp-sfpplus1', 'sfp-sfpplus2', 'sfp-sfpplus3', 'sfp-sfpplus4']
        },
        'total_ports': 16,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-12': 'Customer connections',
            'sfp-sfpplus1-4': 'Uplinks or high-speed connections (10G)'
        }
    },
    
    # CCR2216 Series
    'CCR2216-1G-12XS-2XQ': {
        'model': 'CCR2216-1G-12XS-2XQ',
        'series': 'CCR2216',
        'cpu': 'Annapurna Labs Alpine v3',
        'ports': {
            'ethernet_1g': ['ether1'],
            'sfp28_25g': ['sfp28-1', 'sfp28-2', 'sfp28-3', 'sfp28-4', 'sfp28-5', 'sfp28-6',
                         'sfp28-7', 'sfp28-8', 'sfp28-9', 'sfp28-10', 'sfp28-11', 'sfp28-12'],
            'qsfp28_100g': ['qsfpplus1-1', 'qsfpplus2-1']
        },
        'total_ports': 15,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'sfp28-1-12': 'High-speed customer/sector connections (25G)',
            'qsfpplus1-1, qsfpplus2-1': 'Ultra high-speed uplinks (100G)'
        }
    },

    # CCR1072 Series
    'CCR1072-12G-4S+': {
        'model': 'CCR1072-12G-4S+',
        'series': 'CCR1072',
        'cpu': 'Tilera Tile-Gx72',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6',
                           'ether7', 'ether8', 'ether9', 'ether10', 'ether11', 'ether12'],
            'sfp_1g': ['sfp1', 'sfp2', 'sfp3', 'sfp4']
        },
        'total_ports': 16,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-12': 'Customer/Sector connections',
            'sfp1-4': 'Fiber uplinks (1G)'
        }
    },

    # RB5009 Series
    'RB5009UG+S+': {
        'model': 'RB5009UG+S+',
        'series': 'RB5009',
        'cpu': 'Marvell ARMADA 7040',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6', 'ether7', 'ether8', 'ether9', 'ether10'],
            'sfp_plus_10g': ['sfp-sfpplus1']
        },
        'total_ports': 11,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-10': 'Customer/Sector connections',
            'sfp-sfpplus1': 'Fiber uplink'
        }
    },

    # RB2011 Series
    'RB2011UiAS': {
        'model': 'RB2011UiAS',
        'series': 'RB2011',
        'cpu': 'Atheros AR9344',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6', 'ether7', 'ether8', 'ether9', 'ether10'],
            'sfp_1g': ['sfp1']
        },
        'total_ports': 11,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-10': 'Customer/Sector connections',
            'sfp1': 'Fiber uplink'
        }
    },

    # RB1009 Series
    'RB1009UG+S+': {
        'model': 'RB1009UG+S+',
        'series': 'RB1009',
        'cpu': 'Tilera Tile-Gx9',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6', 'ether7', 'ether8', 'ether9'],
            'sfp_plus_10g': ['sfp-sfpplus1']
        },
        'total_ports': 10,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-9': 'Customer/Sector connections',
            'sfp-sfpplus1': 'Fiber uplink'
        }
    },
    
    # CRS Series (Switches)
    'CRS326-24G-2S+': {
        'model': 'CRS326-24G-2S+',
        'series': 'CRS326',
        'cpu': 'Marvell-98DX3236',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6',
                           'ether7', 'ether8', 'ether9', 'ether10', 'ether11', 'ether12',
                           'ether13', 'ether14', 'ether15', 'ether16', 'ether17', 'ether18',
                           'ether19', 'ether20', 'ether21', 'ether22', 'ether23', 'ether24'],
            'sfp_plus_10g': ['sfp-sfpplus1', 'sfp-sfpplus2']
        },
        'total_ports': 26,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-24': 'Access ports',
            'sfp-sfpplus1-2': 'Uplinks (10G)'
        }
    },
    
    'CRS354-48G-4S+2Q+': {
        'model': 'CRS354-48G-4S+2Q+',
        'series': 'CRS354',
        'cpu': 'Marvell-98DX3257',
        'ports': {
            'ethernet_1g': ['ether1', 'ether2', 'ether3', 'ether4', 'ether5', 'ether6',
                           'ether7', 'ether8', 'ether9', 'ether10', 'ether11', 'ether12',
                           'ether13', 'ether14', 'ether15', 'ether16', 'ether17', 'ether18',
                           'ether19', 'ether20', 'ether21', 'ether22', 'ether23', 'ether24',
                           'ether25', 'ether26', 'ether27', 'ether28', 'ether29', 'ether30',
                           'ether31', 'ether32', 'ether33', 'ether34', 'ether35', 'ether36',
                           'ether37', 'ether38', 'ether39', 'ether40', 'ether41', 'ether42',
                           'ether43', 'ether44', 'ether45', 'ether46', 'ether47', 'ether48'],
            'sfp_plus_10g': ['sfp-sfpplus1', 'sfp-sfpplus2', 'sfp-sfpplus3', 'sfp-sfpplus4'],
            'qsfp28_40g': ['qsfpplus1-1', 'qsfpplus2-1']
        },
        'total_ports': 54,
        'management_port': 'ether1',
        'typical_use': {
            'ether1': 'Management',
            'ether2-48': 'Access ports',
            'sfp-sfpplus1-4': 'Uplinks (10G)',
            'qsfpplus1-1, qsfpplus2-1': 'High-speed uplinks (40G)'
        }
    }
}

# ========================================
# INTELLIGENT INTERFACE MIGRATION LOGIC
# ========================================

def get_interface_type(interface_name):
    """Determine the type of interface from its name"""
    if interface_name.startswith('ether'):
        return 'ethernet_1g'
    elif interface_name.startswith('combo'):
        return 'sfp_plus_10g'
    elif interface_name.startswith('sfp-sfpplus'):
        return 'sfp_plus_10g'
    elif interface_name.startswith('sfp28'):
        return 'sfp28_25g'
    elif interface_name.startswith('qsfp'):
        return 'qsfp28_100g'
    elif interface_name.startswith('sfp'):
        return 'sfp_1g'
    else:
        return 'unknown'

def build_interface_migration_map(source_device, target_device):
    """
    Intelligently build interface migration map between two devices
    
    Rules:
    1. ether1 ALWAYS stays ether1 (management port)
    2. Map interfaces by type preference (SFP→SFP+, Ethernet→Ethernet)
    3. Use first available ports of matching type
    4. If no matching type, use next best alternative
    5. Preserve logical groupings (uplinks together, sectors together)
    """
    if source_device not in ROUTERBOARD_INTERFACES or target_device not in ROUTERBOARD_INTERFACES:
        return None
    
    source = ROUTERBOARD_INTERFACES[source_device]
    target = ROUTERBOARD_INTERFACES[target_device]
    
    migration_map = {}
    target_used_ports = set()
    
    # Rule 1: Management port always stays ether1
    migration_map['ether1'] = 'ether1'
    target_used_ports.add('ether1')
    
    # Get all source interfaces (excluding ether1)
    source_interfaces = []
    for port_type, ports in source['ports'].items():
        for port in ports:
            if port != 'ether1':
                source_interfaces.append((port, port_type))
    
    # Sort source interfaces to maintain logical order
    # Priority: uplinks first (ether2-3, sfp1-2), then others
    def interface_priority(item):
        port, port_type = item
        # Extract number from interface name
        import re
        match = re.search(r'(\d+)$', port)
        num = int(match.group(1)) if match else 999
        
        # Uplinks (ether2-3, sfp1-2) get highest priority
        if 'ether' in port and num in [2, 3]:
            return (0, num)
        elif 'sfp' in port and num in [1, 2]:
            return (1, num)
        else:
            return (2, num)
    
    source_interfaces.sort(key=interface_priority)
    
    # Map each source interface to best available target
    for source_port, source_type in source_interfaces:
        target_port = find_best_target_port(
            source_port, source_type, target, target_used_ports
        )
        if target_port:
            migration_map[source_port] = target_port
            target_used_ports.add(target_port)
    
    return migration_map

def find_best_target_port(source_port, source_type, target_device, used_ports):
    """
    Find the best available target port for a source port
    
    Preference order:
    1. Same type (ethernet→ethernet, sfp→sfp+)
    2. Upgrade type (sfp_1g→sfp_plus_10g, ethernet_1g→sfp_plus_10g)
    3. Any available port
    """
    target_ports = target_device['ports']
    
    # Type preference mapping (in order of preference)
    type_preferences = {
        'ethernet_1g': ['ethernet_1g', 'sfp_plus_10g', 'sfp28_25g'],
        'sfp_1g': ['sfp_plus_10g', 'sfp28_25g', 'ethernet_1g'],
        'sfp_plus_10g': ['sfp_plus_10g', 'sfp28_25g', 'qsfp28_100g'],
        'sfp28_25g': ['sfp28_25g', 'qsfp28_100g', 'sfp_plus_10g']
    }
    
    preferences = type_preferences.get(source_type, [source_type])
    
    # Try each preference in order
    for preferred_type in preferences:
        if preferred_type in target_ports:
            for port in target_ports[preferred_type]:
                if port not in used_ports:
                    return port
    
    # Fallback: use any available port
    for port_type, ports in target_ports.items():
        for port in ports:
            if port not in used_ports:
                return port
    
    return None

def migrate_interface_config(config_text, interface_map):
    """
    Migrate all interface-related configuration
    
    This includes:
    - Interface names in all contexts
    - IP addresses
    - OSPF interface assignments
    - BGP peer bindings
    - Firewall rules
    - Bridge ports
    - VLAN assignments
    - Any other interface references
    """
    migrated_config = config_text

    # Normalize legacy combo port naming (RB1009 exports often use combo1)
    if re.search(r'\bcombo\d+\b', migrated_config):
        migrated_config = re.sub(r'\bcombo1\b', 'sfp-sfpplus1', migrated_config)
    
    # Sort by length (longest first) to avoid partial replacements
    # e.g., replace "ether10" before "ether1"
    sorted_interfaces = sorted(interface_map.items(), key=lambda x: len(x[0]), reverse=True)
    
    for old_interface, new_interface in sorted_interfaces:
        # Use word boundaries to avoid partial matches
        import re
        
        # Pattern matches interface name with word boundaries
        # This ensures we don't replace "ether1" in "ether10"
        pattern = r'\b' + re.escape(old_interface) + r'\b'
        migrated_config = re.sub(pattern, new_interface, migrated_config)
    
    return migrated_config

def detect_device_from_config(config_text):
    """
    Detect device model from configuration content
    
    Looks for:
    1. System identity comments
    2. Interface names and patterns
    3. Board name in comments
    """
    import re
    
    # Try to find explicit device model mentions
    for model in ROUTERBOARD_INTERFACES.keys():
        if model in config_text:
            return model
    
    # Detect by interface pattern
    # CCR2004-1G-12S+2XS has sfp-sfpplus1-12 and sfp28-1-2
    if 'sfp-sfpplus12' in config_text and 'sfp28-' in config_text:
        return 'CCR2004-1G-12S+2XS'
    
    # CCR2004-16G-2S+ has ether1-16 and sfp-sfpplus1-2
    if 'ether16' in config_text and 'sfp-sfpplus2' in config_text and 'sfp-sfpplus3' not in config_text:
        return 'CCR2004-16G-2S+'
    
    # CCR1036-12G-4S has ether1-12 and sfp1-4
    if 'ether12' in config_text and 'sfp4' in config_text and 'sfp-sfpplus' not in config_text:
        return 'CCR1036-12G-4S'
    
    # CCR2116-12G-4S+ has ether1-12 and sfp-sfpplus1-4
    if 'ether12' in config_text and 'sfp-sfpplus4' in config_text and 'sfp-sfpplus5' not in config_text:
        return 'CCR2116-12G-4S+'

    # RB1009UG+S+ has ether1-9 and a single SFP+/combo port
    if ('RB1009' in config_text or 'MT1009' in config_text or
            ('ether9' in config_text and 'sfp-sfpplus1' in config_text and 'ether10' not in config_text)):
        return 'RB1009UG+S+'
    
    # CCR2216 has sfp28-1 through sfp28-12
    if 'sfp28-12' in config_text:
        return 'CCR2216-1G-12XS-2XQ'

    # RB5009UG+S+ has ether1-10 and sfp-sfpplus1
    if 'RB5009' in config_text or ('ether10' in config_text and 'sfp-sfpplus1' in config_text and 'ether11' not in config_text):
        return 'RB5009UG+S+'

    # RB2011UiAS has ether1-10 and sfp1
    if 'RB2011' in config_text or 'MT2011' in config_text or ('ether10' in config_text and 'sfp1' in config_text and 'sfp2' not in config_text and 'sfp-sfpplus' not in config_text):
        return 'RB2011UiAS'

    # RB1009UG+S+ has ether1-9 and sfp-sfpplus1 (combo)
    if 'RB1009' in config_text or 'MT1009' in config_text or ('ether9' in config_text and 'sfp-sfpplus1' in config_text and 'ether10' not in config_text):
        return 'RB1009UG+S+'
    
    return None

def detect_routeros_version(config_text):
    """
    Detect RouterOS version from configuration syntax
    
    ROS7 indicators:
    - Slash-separated commands (/interface/bridge)
    - sfp-sfpplus naming
    - New speed syntax (10G-baseSR-LR)
    
    ROS6 indicators:
    - Space-separated commands (/interface bridge)
    - Old speed syntax (10G-baseSR)
    - ether1-master syntax
    """
    ros7_indicators = [
        '/interface/bridge',
        '/ip/address',
        '/routing/ospf',
        'sfp-sfpplus',
        '10G-baseSR-LR',
        'sfp28-'
    ]
    
    ros6_indicators = [
        '/interface bridge',
        '/ip address',
        '/routing ospf',
        '10G-baseSR',
        'ether1-master'
    ]
    
    ros7_count = sum(1 for indicator in ros7_indicators if indicator in config_text)
    ros6_count = sum(1 for indicator in ros6_indicators if indicator in config_text)
    
    if ros7_count > ros6_count:
        return 7
    elif ros6_count > 0:
        return 6
    else:
        return None

def apply_ros6_to_ros7_syntax(config_text):
    """
    Convert RouterOS 6 syntax to RouterOS 7
    
    Changes:
    - Command paths (space → slash)
    - Speed syntax
    - Interface naming conventions
    """
    import re
    
    # Command structure changes
    syntax_changes = [
        (r'/interface bridge', '/interface/bridge'),
        (r'/interface ethernet', '/interface/ethernet'),
        (r'/interface vlan', '/interface/vlan'),
        (r'/interface bonding', '/interface/bonding'),
        (r'/ip address', '/ip/address'),
        (r'/ip route', '/ip/route'),
        (r'/ip firewall', '/ip/firewall'),
        (r'/ip service', '/ip/service'),
        (r'/routing ospf', '/routing/ospf'),
        (r'/routing bgp', '/routing/bgp'),
        (r'/routing filter', '/routing/filter'),
        (r'/system identity', '/system/identity'),
        (r'/system clock', '/system/clock'),
        (r'/system ntp', '/system/ntp'),
    ]
    
    migrated = config_text
    for old_syntax, new_syntax in syntax_changes:
        migrated = migrated.replace(old_syntax, new_syntax)
    
    # Speed syntax changes
    speed_changes = [
        ('10G-baseSR', '10G-baseSR-LR'),
        ('1G-baseT', '1000M-baseTX'),
        ('100M-baseT', '100M-baseTX'),
    ]
    
    for old_speed, new_speed in speed_changes:
        migrated = migrated.replace(old_speed, new_speed)
    
    return migrated


# ========================================
# AI PROVIDER CONFIGURATION (Early definition for model selection)
# ========================================
# Supports: 'ollama' (free, local) or 'openai' (paid, cloud)
AI_PROVIDER = os.getenv('AI_PROVIDER', 'ollama').lower()
OLLAMA_API_URL = os.getenv('OLLAMA_API_URL', 'http://localhost:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'phi3:mini')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', '')

# ========================================
# SMART MODEL SELECTION WITH AUTO-DETECTION
# ========================================

# Cache for available models (refresh every 5 minutes)
_available_models_cache = None
_models_cache_time = 0
_MODELS_CACHE_TTL = 300  # 5 minutes

def get_available_ollama_models():
    """
    Get list of available Ollama models with automatic refresh
    Returns list of model names, or empty list if Ollama is unavailable
    """
    global _available_models_cache, _models_cache_time
    
    # Return cached models if still valid
    import time
    current_time = time.time()
    if _available_models_cache is not None and (current_time - _models_cache_time) < _MODELS_CACHE_TTL:
        return _available_models_cache
    
    # Try to fetch available models from Ollama
    try:
        response = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            _available_models_cache = models
            _models_cache_time = current_time
            safe_print(f"[MODEL SELECTION] Found {len(models)} available Ollama models: {', '.join(models[:5])}{'...' if len(models) > 5 else ''}")
            return models
    except Exception as e:
        safe_print(f"[MODEL SELECTION] Could not fetch Ollama models: {e} - using defaults")
    
    # Fallback to default models if Ollama is unavailable
    return ['phi3:mini', 'llama3.2:3b', 'qwen2.5-coder:7b', 'llama3.2', 'qwen2.5-coder']

def select_best_model(task_type: str, config_size: int = 0, available_models: list = None) -> str:
    """
    Auto-select the best model based on task type, complexity, and available models
    Prioritizes models that are actually installed in Ollama
    
    Args:
        task_type: Type of task (chat, validation, translation, etc.)
        config_size: Size of config in characters (0 = unknown)
        available_models: List of available Ollama models (auto-detected if None)
    
    Returns:
        Model name that should work best for the task
    """
    if available_models is None:
        available_models = get_available_ollama_models()
    
    # Model performance profiles with priority order
    # Models are ordered by preference (fastest/smallest first, then larger)
    model_profiles = [
        {
            'name': 'phi3:mini',
            'speed': 'very_fast',
            'accuracy': 'good',
            'context': 4000,
            'max_config_size': 10000,  # ~10KB configs
            'best_for': ['chat', 'validation', 'suggestion', 'small_configs'],
            'timeout': 30
        },
        {
            'name': 'llama3.2:3b',
            'speed': 'fast',
            'accuracy': 'good',
            'context': 8000,
            'max_config_size': 20000,  # ~20KB configs
            'best_for': ['quick_tasks', 'simple_chat', 'medium_configs'],
            'timeout': 45
        },
        {
            'name': 'llama3.2',
            'speed': 'medium',
            'accuracy': 'very_good',
            'context': 8000,
            'max_config_size': 30000,  # ~30KB configs
            'best_for': ['translation', 'medium_configs', 'analysis'],
            'timeout': 60
        },
        {
            'name': 'qwen2.5-coder:7b',
            'speed': 'medium', 
            'accuracy': 'excellent',
            'context': 32000,
            'max_config_size': 100000,  # ~100KB configs
            'best_for': ['translation', 'complex_configs', 'detailed_analysis', 'large_configs'],
            'timeout': 120
        },
        {
            'name': 'qwen2.5-coder',
            'speed': 'medium',
            'accuracy': 'excellent',
            'context': 32000,
            'max_config_size': 100000,
            'best_for': ['translation', 'complex_configs', 'large_configs'],
            'timeout': 120
        },
        {
            'name': 'llama3.1:8b',
            'speed': 'slow',
            'accuracy': 'excellent',
            'context': 8000,
            'max_config_size': 50000,
            'best_for': ['complex_configs', 'detailed_analysis'],
            'timeout': 90
        }
    ]
    
    # Filter to only available models
    available_profiles = [p for p in model_profiles if p['name'] in available_models]
    
    # If no models match, use first available or default
    if not available_profiles:
        if available_models:
            safe_print(f"[MODEL SELECTION] Using first available model: {available_models[0]}")
            return available_models[0]
        return 'phi3:mini'  # Ultimate fallback
    
    # Select based on config size and task type
    if task_type in ['chat', 'validation', 'suggestion']:
        # For quick tasks, prefer fastest available model
        for profile in available_profiles:
            if profile['speed'] in ['very_fast', 'fast']:
                return profile['name']
        return available_profiles[0]['name']  # Fallback to first available
    
    elif task_type in ['translation', 'upgrade']:
        # For translations, select based on config size
        # Estimate: ~50 chars per line average, so 1000 lines ≈ 50KB
        estimated_lines = config_size // 50 if config_size > 0 else 0
        
        if config_size == 0:
            # Unknown size - use medium model
            for profile in available_profiles:
                if profile['speed'] in ['fast', 'medium']:
                    return profile['name']
            return available_profiles[0]['name']
        
        # For very large configs (>1000 lines ≈ >50KB), prefer largest available model
        if estimated_lines > 1000 or config_size > 50000:
            safe_print(f"[MODEL SELECTION] Very large config detected (~{estimated_lines} lines, {config_size} chars) - selecting largest available model")
            # Prefer qwen2.5-coder for very large configs (best context window)
            for profile in reversed(available_profiles):  # Start from largest
                if profile['name'].startswith('qwen') or profile['context'] >= 32000:
                    return profile['name']
            return available_profiles[-1]['name']  # Fallback to largest available
        
        # Select model that can handle the config size
        for profile in available_profiles:
            if config_size <= profile['max_config_size']:
                return profile['name']
        
        # Config is larger than any model's max - use largest available
        return available_profiles[-1]['name']
    
    elif task_type in ['analysis', 'detailed_review']:
        # For analysis, prefer models with better accuracy
        for profile in available_profiles:
            if profile['accuracy'] in ['excellent', 'very_good']:
                return profile['name']
        return available_profiles[-1]['name']  # Use largest available
    
    else:
        # Default: use fastest available
        return available_profiles[0]['name']

# ========================================
# TRAINING DATA LOADER (External directory)
# ========================================
TRAINING_DIR = os.getenv('ROS_TRAINING_DIR', '').strip()
TRAINING_RULES = {}

def _read_text_file(p: Path) -> str:
    try:
        return p.read_text(encoding='utf-8', errors='ignore')
    except Exception:
        return ''

def _read_json_file(p: Path):
    try:
        import json as _json
        return _json.loads(p.read_text(encoding='utf-8', errors='ignore'))
    except Exception:
        return None

def load_training_rules(directory: str) -> dict:
    rules = {}
    if not directory:
        return rules
    base = Path(directory)
    if not base.exists():
        print(f"[TRAINING] Directory not found: {directory}")
        return rules
    print(f"[TRAINING] Loading RouterOS training from: {directory}")
    for p in base.iterdir():
        if p.suffix.lower() in ['.json']:
            obj = _read_json_file(p)
            if obj is not None:
                rules[p.stem] = obj
        elif p.suffix.lower() in ['.md', '.txt']:
            rules[p.stem] = _read_text_file(p)
    print(f"[TRAINING] Loaded {len(rules)} items: {', '.join(sorted(rules.keys()))}")
    return rules

# Lazy load training rules - only when first accessed
TRAINING_RULES = {}
_training_rules_loaded = False

def get_training_rules():
    """Get training rules, loading them if not already loaded"""
    global TRAINING_RULES, _training_rules_loaded
    if not _training_rules_loaded:
        try:
            TRAINING_RULES = load_training_rules(TRAINING_DIR)
            _training_rules_loaded = True
        except Exception as e:
            print(f"[TRAINING] Error loading training rules: {e}")
            TRAINING_RULES = {}
            _training_rules_loaded = True
    return TRAINING_RULES

# ========================================
# CONFIG POLICY LOADER
# ========================================
# Get config_policies directory - handle both development and PyInstaller environments
def get_config_policies_dir():
    """Get the config_policies directory path, handling PyInstaller frozen executables"""
    if getattr(sys, 'frozen', False):
        # Running as compiled executable - files are in sys._MEIPASS
        base_path = Path(sys._MEIPASS)
        policies_dir = base_path / "config_policies"
        if policies_dir.exists():
            return policies_dir
        # Also check next to executable
        exe_dir = Path(sys.executable).parent
        policies_dir = exe_dir / "config_policies"
        if policies_dir.exists():
            return policies_dir
    # Development mode or fallback
    return Path("config_policies")

CONFIG_POLICIES_DIR = get_config_policies_dir()
CONFIG_POLICIES = {}
_policies_loaded = False

def load_config_policies(directory: Path = None) -> dict:
    """
    Load ALL configuration policies from directory structure recursively.
    Finds all .md policy files in config_policies/ and organizes them by category.
    
    Structure:
    - config_policies/{category}/{policy-name}.md
    - config_policies/{category}/{policy-name}-policy.md
    - config_policies/{category}/{policy-name}-config-policy.md
    
    Also loads compliance references from Python modules.
    """
    policies = {}
    if directory is None:
        directory = CONFIG_POLICIES_DIR
    
    # Ensure directory is a Path object
    if not isinstance(directory, Path):
        directory = Path(directory)
    
    if not directory.exists():
        print(f"[POLICIES] Directory not found: {directory}")
        print(f"[POLICIES] Current working directory: {Path.cwd()}")
        if getattr(sys, 'frozen', False):
            print(f"[POLICIES] PyInstaller base path: {sys._MEIPASS}")
        return policies
    
    print(f"[POLICIES] Loading ALL configuration policies from: {directory}")
    
    # Recursively find all .md files (excluding README.md and USAGE.md in root)
    exclude_names = {'README.md', 'USAGE.md', 'readme.md', 'usage.md'}
    exclude_dirs = {'examples', '__pycache__', '.git'}
    
    try:
        md_files = list(directory.rglob("*.md"))
    except Exception as e:
        print(f"[POLICIES] Error scanning directory: {e}")
        return policies
    
    for md_file in md_files:
        # Skip excluded files
        if md_file.name in exclude_names and md_file.parent == directory:
            continue
        
        # Skip excluded directories
        if any(excluded in md_file.parts for excluded in exclude_dirs):
            continue
        
        try:
            # Create policy key from path: category-policy-name
            # e.g., "nextlink/nextlink-internet-policy.md" -> "nextlink-internet-policy"
            try:
                relative_path = md_file.relative_to(directory)
            except ValueError:
                # If paths don't match, use filename
                relative_path = Path(md_file.name)
            
            category = relative_path.parts[0] if len(relative_path.parts) > 1 else 'root'
            policy_name_no_ext = md_file.stem
            
            # Create unique key: category-policy-name
            policy_key = f"{category}-{policy_name_no_ext}" if category != 'root' else policy_name_no_ext
            
            # Read policy content with proper error handling
            try:
                with open(md_file, 'r', encoding='utf-8', errors='ignore') as f:
                    policy_content = f.read()
            except Exception as e:
                print(f"[POLICIES] Error reading {md_file}: {e}")
                continue
            
            policies[policy_key] = {
                'name': policy_key,
                'category': category,
                'filename': md_file.name,
                'content': policy_content,
                'path': str(md_file),
                'relative_path': str(relative_path)
            }
            
            print(f"[POLICIES] Loaded: {policy_key} from {relative_path}")
            
        except Exception as e:
            print(f"[POLICIES] Error loading {md_file}: {e}")
    
    # Also load compliance references from Python modules
    try:
        if HAS_COMPLIANCE:
            from nextlink_compliance_reference import get_all_compliance_blocks
            compliance_blocks = get_all_compliance_blocks()
            if compliance_blocks:
                policies['compliance-reference'] = {
                    'name': 'compliance-reference',
                    'category': 'compliance',
                    'filename': 'nextlink_compliance_reference.py',
                    'content': f"# NextLink Compliance Reference (RFC-09-10-25)\n\nThis is the compliance reference module content.\n\n```python\n# Compliance blocks are available via get_all_compliance_blocks()\n```\n\n**Note:** This reference is loaded from the Python module `nextlink_compliance_reference.py`.",
                    'path': 'nextlink_compliance_reference.py',
                    'relative_path': 'nextlink_compliance_reference.py',
                    'type': 'python_module'
                }
                print(f"[POLICIES] Loaded compliance reference from Python module")
    except Exception as e:
        print(f"[POLICIES] Could not load compliance reference: {e}")
    
    # Load enterprise reference if available
    try:
        if HAS_REFERENCE:
            from nextlink_enterprise_reference import get_all_standard_blocks
            policies['enterprise-reference'] = {
                'name': 'enterprise-reference',
                'category': 'reference',
                'filename': 'nextlink_enterprise_reference.py',
                'content': "# NextLink Enterprise Reference\n\nThis is the enterprise reference module containing standard configuration blocks.\n\n**Note:** This reference is loaded from the Python module `nextlink_enterprise_reference.py`.",
                'path': 'nextlink_enterprise_reference.py',
                'relative_path': 'nextlink_enterprise_reference.py',
                'type': 'python_module'
            }
            print(f"[POLICIES] Loaded enterprise reference from Python module")
    except Exception as e:
        print(f"[POLICIES] Could not load enterprise reference: {e}")
    
    print(f"[POLICIES] Loaded {len(policies)} total policies/references")
    print(f"[POLICIES] Categories: {', '.join(sorted(set(p.get('category', 'unknown') for p in policies.values())))}")
    return policies

# Lazy load policies - only load when first accessed
# This prevents errors during import in PyInstaller environment
def get_config_policies():
    """Get config policies, loading them if not already loaded"""
    global CONFIG_POLICIES
    global _policies_loaded
    
    if not _policies_loaded:
        try:
            CONFIG_POLICIES = load_config_policies()
            _policies_loaded = True
        except Exception as e:
            print(f"[POLICIES] Error loading policies on startup: {e}")
            CONFIG_POLICIES = {}
            _policies_loaded = True  # Mark as loaded to prevent retry loops
    
    return CONFIG_POLICIES

# Initialize as empty - will be loaded on first access
CONFIG_POLICIES = {}

# ========================================
# CHAT HISTORY & MEMORY SYSTEM
# ========================================
# SECURITY: Database files in secure directory (not accessible via HTTP)
# Note: os and shutil already imported at top of file

# Lazy directory creation - only create when needed
def ensure_secure_data_dir():
    """Ensure secure_data directory exists (lazy)"""
    try:
        SECURE_DATA_DIR = Path("secure_data")
        SECURE_DATA_DIR.mkdir(exist_ok=True)
        # Set restrictive permissions (Unix-like systems)
        try:
            os.chmod(SECURE_DATA_DIR, 0o700)  # Only owner can access
        except:
            pass  # Windows doesn't support chmod
        return SECURE_DATA_DIR
    except Exception as e:
        print(f"[SECURE_DATA] Error creating directory: {e}")
        return Path("secure_data")  # Return path anyway

SECURE_DATA_DIR = Path("secure_data")  # Default path, will be created lazily
CHAT_DB_PATH = None  # Will be set lazily when ensure_chat_db() is called
CONFIGS_DB_PATH = None
FEEDBACK_DB_PATH = None

# Migration: Move existing databases from root to secure directory
def migrate_databases():
    """Migrate existing database files from root to secure_data directory"""
    global SECURE_DATA_DIR, CHAT_DB_PATH, CONFIGS_DB_PATH
    
    SECURE_DATA_DIR = ensure_secure_data_dir()  # Ensure directory exists first
    CHAT_DB_PATH = SECURE_DATA_DIR / "chat_history.db"
    CONFIGS_DB_PATH = SECURE_DATA_DIR / "completed_configs.db"
    
    old_chat_db = Path("chat_history.db")
    old_configs_db = Path("completed_configs.db")
    
    if old_chat_db.exists() and (CHAT_DB_PATH is None or not CHAT_DB_PATH.exists()):
        try:
            print(f"[MIGRATION] Moving {old_chat_db} to {CHAT_DB_PATH}")
            shutil.move(str(old_chat_db), str(CHAT_DB_PATH))
            print(f"[MIGRATION] ✓ Chat history database migrated")
        except Exception as e:
            print(f"[MIGRATION] Error moving chat DB: {e}")
    
    if old_configs_db.exists() and (CONFIGS_DB_PATH is None or not CONFIGS_DB_PATH.exists()):
        try:
            print(f"[MIGRATION] Moving {old_configs_db} to {CONFIGS_DB_PATH}")
            shutil.move(str(old_configs_db), str(CONFIGS_DB_PATH))
            print(f"[MIGRATION] ✓ Configs database migrated")
        except Exception as e:
            print(f"[MIGRATION] Error moving configs DB: {e}")

# Lazy migration - only run when first database is accessed
_migration_done = False
def ensure_migration():
    """Ensure database migration is done (lazy)"""
    global _migration_done
    if not _migration_done:
        try:
            migrate_databases()
            _migration_done = True
        except Exception as e:
            print(f"[MIGRATION] Error during migration: {e}")
            _migration_done = True  # Mark as done to prevent loops

def init_chat_db():
    """Initialize chat history database in secure location"""
    global SECURE_DATA_DIR, CHAT_DB_PATH
    SECURE_DATA_DIR = ensure_secure_data_dir()  # Ensure directory exists
    CHAT_DB_PATH = SECURE_DATA_DIR / "chat_history.db"
    conn = sqlite3.connect(str(CHAT_DB_PATH))
    cursor = conn.cursor()
    
    # Create conversations table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS conversations (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT NOT NULL,
            user_message TEXT NOT NULL,
            ai_response TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            model_used TEXT,
            task_type TEXT
        )
    ''')
    
    # Create user preferences table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS user_preferences (
            session_id TEXT PRIMARY KEY,
            preferred_model TEXT,
            context_memory TEXT,
            last_activity DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    conn.close()
    print(f"[CHAT] Database initialized: {CHAT_DB_PATH}")

def save_chat_message(session_id, user_message, ai_response, model_used, task_type):
    """Save chat message to database"""
    ensure_chat_db()  # Lazy init - this sets CHAT_DB_PATH
    global CHAT_DB_PATH
    if CHAT_DB_PATH is None:
        CHAT_DB_PATH = ensure_secure_data_dir() / "chat_history.db"
    conn = sqlite3.connect(str(CHAT_DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT INTO conversations (session_id, user_message, ai_response, model_used, task_type)
        VALUES (?, ?, ?, ?, ?)
    ''', (session_id, user_message, ai_response, model_used, task_type))
    
    conn.commit()
    conn.close()

def get_chat_history(session_id, limit=10):
    """Get recent chat history for context"""
    ensure_chat_db()  # Lazy init - this sets CHAT_DB_PATH
    global CHAT_DB_PATH
    if CHAT_DB_PATH is None:
        CHAT_DB_PATH = ensure_secure_data_dir() / "chat_history.db"
    conn = sqlite3.connect(str(CHAT_DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT user_message, ai_response, timestamp, model_used, task_type
        FROM conversations 
        WHERE session_id = ? 
        ORDER BY timestamp DESC 
        LIMIT ?
    ''', (session_id, limit))
    
    history = cursor.fetchall()
    conn.close()
    
    return history

def get_user_context(session_id):
    """Get user's context and preferences"""
    ensure_chat_db()  # Lazy init - this sets CHAT_DB_PATH
    global CHAT_DB_PATH
    if CHAT_DB_PATH is None:
        CHAT_DB_PATH = ensure_secure_data_dir() / "chat_history.db"
    conn = sqlite3.connect(str(CHAT_DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('''
        SELECT preferred_model, context_memory, last_activity
        FROM user_preferences 
        WHERE session_id = ?
    ''', (session_id,))
    
    result = cursor.fetchone()
    conn.close()
    
    if result:
        return {
            'preferred_model': result[0],
            'context_memory': result[1],
            'last_activity': result[2]
        }
    return None

def update_user_context(session_id, preferred_model=None, context_memory=None):
    """Update user preferences and context"""
    ensure_chat_db()  # Lazy init - this sets CHAT_DB_PATH
    global CHAT_DB_PATH
    if CHAT_DB_PATH is None:
        CHAT_DB_PATH = ensure_secure_data_dir() / "chat_history.db"
    conn = sqlite3.connect(str(CHAT_DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('''
        INSERT OR REPLACE INTO user_preferences (session_id, preferred_model, context_memory, last_activity)
        VALUES (?, ?, ?, CURRENT_TIMESTAMP)
    ''', (session_id, preferred_model, context_memory))
    
    conn.commit()
    conn.close()

# Lazy initialize chat database - only when first accessed
_chat_db_initialized = False
def ensure_chat_db():
    """Ensure chat database is initialized (lazy)"""
    global _chat_db_initialized
    if not _chat_db_initialized:
        try:
            ensure_migration()  # Run migration first
            init_chat_db()
            _chat_db_initialized = True
        except Exception as e:
            print(f"[CHAT] Error initializing database: {e}")
            _chat_db_initialized = True  # Mark as attempted to prevent loops

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size
CORS(app)  # Enable CORS for local HTML file access


# Serve the main UI directly from the deployment directory
@app.route("/", methods=["GET"])
@app.route("/app", methods=["GET"])
@app.route("/tool", methods=["GET"])
@app.route("/NOC-configMaker.html", methods=["GET"])
def serve_ui():
    # Try to find HTML file in multiple locations
    possible_paths = [
        Path.cwd() / "NOC-configMaker.html",
        Path.cwd() / "vm_deployment" / "NOC-configMaker.html",
        Path(__file__).parent / "NOC-configMaker.html",
    ]
    
    for html_path in possible_paths:
        if html_path.exists():
            return send_file(str(html_path), mimetype='text/html')
    
    # Fallback: return error if file not found
    return jsonify({'error': 'NOC-configMaker.html not found'}), 404

# Serve login page
@app.route("/login", methods=["GET"])
@app.route("/login.html", methods=["GET"])
def serve_login():
    # Try to find login.html in multiple locations
    possible_paths = [
        Path.cwd() / "login.html",
        Path.cwd() / "vm_deployment" / "login.html",
        Path(__file__).parent / "login.html",
    ]
    
    for login_path in possible_paths:
        if login_path.exists():
            return send_file(str(login_path), mimetype='text/html')
    
    # Fallback: redirect to main app if login not found
    return serve_ui()

# Serve change password page
@app.route("/change-password", methods=["GET"])
@app.route("/change-password.html", methods=["GET"])
def serve_change_password():
    # Try to find change-password.html in multiple locations
    possible_paths = [
        Path.cwd() / "change-password.html",
        Path.cwd() / "vm_deployment" / "change-password.html",
        Path(__file__).parent / "change-password.html",
    ]
    
    for pwd_path in possible_paths:
        if pwd_path.exists():
            return send_file(str(pwd_path), mimetype='text/html')
    
    # Fallback: return error if file not found
    return jsonify({'error': 'change-password.html not found'}), 404

# Serve static assets if present
@app.route("/static/<path:filename>")
def serve_static(filename):
    # Try multiple locations for static files
    possible_dirs = [
        Path.cwd() / "static",
        Path.cwd() / "vm_deployment" / "static",
        Path(__file__).parent / "static",
    ]
    
    for static_dir in possible_dirs:
        static_file = static_dir / filename
        if static_file.exists() and static_file.is_file():
            return send_file(str(static_file))
    
    return jsonify({'error': 'File not found'}), 404

# Suppress noisy Flask logs from network scanners
import logging
log = logging.getLogger('werkzeug')
log.setLevel(logging.WARNING)  # Only show warnings and errors, not info/debug

# Custom request handler to filter out scanner noise
@app.before_request
def filter_scanner_requests():
    """Filter out noisy scanner requests from logs"""
    client_ip = request.remote_addr
    
    # Check for broadcast addresses or suspicious patterns
    if client_ip and ('.255' in client_ip or client_ip.startswith('192.168.225')):
        # These are likely network scanners - silently handle
        pass
    
    # Continue processing the request normally
    return None

@app.after_request
def log_request(response):
    """Custom logging that filters scanner noise"""
    client_ip = request.remote_addr
    
    # Suppress logging for scanner IPs
    if client_ip and ('.255' in client_ip or client_ip.startswith('192.168.225')):
        # Don't log scanner requests
        return response
    
    # Only log legitimate requests (optional - can be removed for cleaner logs)
    # if response.status_code < 400:
    #     app.logger.info(f'{client_ip} - {request.method} {request.path} - {response.status_code}')
    
    return response

# Hot-reload endpoint (now that app exists)
@app.route('/api/reload-training', methods=['POST'])
def reload_training():
    global TRAINING_RULES
    global TRAINING_DIR
    try:
        d = request.json.get('dir') if request.is_json else None
        if d:
            os.environ['ROS_TRAINING_DIR'] = d
            TRAINING_DIR = d
        directory = d or TRAINING_DIR
        global TRAINING_RULES, _training_rules_loaded
        TRAINING_RULES = load_training_rules(directory)
        _training_rules_loaded = True
        return jsonify({'success': True, 'loaded': list(TRAINING_RULES.keys()), 'dir': directory})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

# ========================================
# ENDPOINT: Simple AI Chat
# ========================================

@app.route('/api/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json(force=True)
        msgs = data.get('messages')
        message = data.get('message')
        session_id = data.get('session_id', 'default')

        # Get user context and chat history
        user_context = get_user_context(session_id)
        chat_history = get_chat_history(session_id, limit=5)
        
        training_context = build_training_context()
        system_prompt = "You are a RouterOS and network engineering assistant. Be accurate, concise, and prefer RouterOS CLI examples. If asked about migrations, apply ROS6→ROS7 rules."
        if training_context:
            system_prompt += "\n\n" + training_context
        
        # Add user context to system prompt
        if user_context and user_context.get('context_memory'):
            system_prompt += f"\n\nUser Context: {user_context['context_memory']}"

        messages = [{"role": "system", "content": system_prompt}]
        
        # Add recent chat history for context
        for hist in reversed(chat_history):
            messages.append({"role": "user", "content": hist[0]})
            messages.append({"role": "assistant", "content": hist[1]})
        
        # Add current message
        if isinstance(msgs, list) and msgs:
            for m in msgs:
                if isinstance(m, dict) and m.get('role') in ('user', 'assistant') and m.get('content'):
                    messages.append({"role": m['role'], "content": str(m['content'])})
        elif message:
            messages.append({"role": "user", "content": str(message)})
        else:
            return jsonify({"success": False, "error": "No message(s) provided"}), 400

        # Smart model selection for chat (use user's preferred model if available)
        preferred_model = user_context.get('preferred_model') if user_context else None
        reply = call_ai(messages, max_tokens=2000, task_type='chat', model=preferred_model)
        
        # Save chat message to database
        current_message = message if message else (msgs[-1]['content'] if msgs else '')
        save_chat_message(session_id, current_message, reply, preferred_model or 'auto', 'chat')
        
        return jsonify({"success": True, "reply": reply, "session_id": session_id})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

# ========================================
# CHAT HISTORY & MEMORY ENDPOINTS
# ========================================

@app.route('/api/chat/history/<session_id>', methods=['GET'])
def get_chat_history_endpoint(session_id):
    """Get chat history for a session"""
    try:
        limit = request.args.get('limit', 20, type=int)
        history = get_chat_history(session_id, limit)
        
        formatted_history = []
        for hist in history:
            formatted_history.append({
                'user_message': hist[0],
                'ai_response': hist[1],
                'timestamp': hist[2],
                'model_used': hist[3],
                'task_type': hist[4]
            })
        
        return jsonify({"success": True, "history": formatted_history})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/chat/context/<session_id>', methods=['GET'])
def get_user_context_endpoint(session_id):
    """Get user context and preferences"""
    try:
        context = get_user_context(session_id)
        return jsonify({"success": True, "context": context})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/chat/context/<session_id>', methods=['POST'])
def update_user_context_endpoint(session_id):
    """Update user context and preferences"""
    try:
        data = request.get_json(force=True)
        preferred_model = data.get('preferred_model')
        context_memory = data.get('context_memory')
        
        update_user_context(session_id, preferred_model, context_memory)
        return jsonify({"success": True, "message": "Context updated"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/chat/export/<session_id>', methods=['GET'])
def export_chat_history(session_id):
    """Export chat history as JSON"""
    try:
        history = get_chat_history(session_id, limit=1000)
        
        export_data = {
            'session_id': session_id,
            'export_timestamp': get_cst_timestamp(),
            'total_messages': len(history),
            'conversations': []
        }
        
        for hist in history:
            export_data['conversations'].append({
                'user_message': hist[0],
                'ai_response': hist[1],
                'timestamp': hist[2],
                'model_used': hist[3],
                'task_type': hist[4]
            })
        
        return jsonify(export_data)
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/chat', methods=['GET'])
def web_chat():
    # Minimal standalone web chat for backend interaction
    popup = request.args.get('popup') in ('1','true','yes')
    wrap_extra = 'position:fixed; right:16px; bottom:16px; width:420px;' if popup else 'max-width:900px;margin:0 auto;'
    bg = '#0f1115cc' if popup else '#0f1115'
    radius = '12px' if popup else '0'
    shadow = '0 8px 24px rgba(0,0,0,.45)' if popup else 'none'
    log_h = '360px' if popup else '60vh'
    html = """
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NOC Config Maker - AI Chat</title>
  <style>
    body{font-family:Segoe UI,Arial,sans-serif;background:#0f1115;color:#eee;margin:0;padding:0}
    .wrap{ %(wrap_extra)s padding:20px; background:%(bg)s; border-radius:%(radius)s; box-shadow:%(shadow)s; z-index:99999; position:relative }
    h1{font-size:18px;margin:0 0 12px}
    #log{height:%(log_h)s;overflow:auto;border:1px solid #333;border-radius:8px;padding:12px;background:#161a22;white-space:pre-wrap}
    .row{display:flex;gap:8px;margin-top:10px}
    #q{flex:1;padding:10px;border-radius:6px;border:1px solid #333;background:#0f1115;color:#eee}
    button{background:#4CAF50;border:none;color:#fff;border-radius:6px;padding:10px 14px;cursor:pointer}
    .note{font-size:12px;color:#9aa}
  </style>
  <script>
    function bindChat(){
      const log=document.getElementById('log');
      const input=document.getElementById('q');
      const btn=document.getElementById('sendBtn');
      const status=document.getElementById('status');
      if(!log||!input||!btn) return;
      function append(prefix, msg){
        log.innerText += (prefix? prefix+' ':'') + msg + '\n';
        log.scrollTop = log.scrollHeight;
      }
      async function health(){
        try{ const r=await fetch('/api/health'); const j=await r.json(); status.textContent = (j.status==='online'?'Online':'Offline'); status.style.color = (j.status==='online'?'#7ee2a8':'#ff7676'); }
        catch{ status.textContent='Offline'; status.style.color='#ff7676'; }
      }
      async function sendMsg(){
        const t=input.value.trim();
        if(!t) return;
        append('[user]', t);
        input.value='';
        try{
          const ctrl = new AbortController(); const to = setTimeout(()=>ctrl.abort(), 60000);
          const r = await fetch('/api/chat',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({message:t}) , signal: ctrl.signal});
          clearTimeout(to);
          if(!r.ok){ throw new Error('HTTP '+r.status); }
          const j = await r.json();
          if(!j.success) throw new Error(j.error||'Chat failed');
          append('[ai]', String(j.reply||''));
        }catch(e){ append('[error]', e.message||String(e)); }
      }
      const form=document.getElementById('frm');
      form.addEventListener('submit', function(e){ e.preventDefault(); sendMsg(); });
      btn.addEventListener('click', function(e){ e.preventDefault(); sendMsg(); });
      input.addEventListener('keydown', function(e){ if(e.key==='Enter'){ e.preventDefault(); sendMsg(); }});
      window.sendMsg = sendMsg; // optional
      health();
    }
    window.addEventListener('DOMContentLoaded', bindChat);
  </script>
</head>
<body>
  <div class="wrap">
    <h1>AI Assistant Chat</h1>
    <div id="status" class="note" style="margin:4px 0 8px 0;">Checking...</div>
    <div id="log"></div>
    <form class="row" id="frm">
      <input id="q" type="text" placeholder="Ask about RouterOS, ROS6->ROS7, OSPF/BGP, etc..." autocomplete="off" />
      <button id="sendBtn" type="submit">Send</button>
    </form>
    <p class="note">Tip: set ROS_TRAINING_DIR before start, or POST /api/reload-training to apply your standards.</p>
  </div>
</body>
</html>
""" % { 'wrap_extra': wrap_extra, 'bg': bg, 'radius': radius, 'shadow': shadow, 'log_h': log_h }
    return html

# ========================================
# OpenAI-compatible endpoints for external UIs (e.g., Open WebUI)
# Base URL: http://localhost:5000/v1
# ========================================

@app.route('/v1/models', methods=['GET'])
def v1_models():
    # Minimal model list so Open WebUI recognizes the server
    model_name = os.getenv('OPENAI_COMPAT_MODEL', 'noc-local')
    return jsonify({
        "object": "list",
        "data": [
            {"id": model_name, "object": "model", "created": int(datetime.utcnow().timestamp()), "owned_by": "noc-configmaker"}
        ]
    })

@app.route('/v1/chat/completions', methods=['POST'])
def v1_chat_completions():
    try:
        payload = request.get_json(force=True) or {}
        msgs = payload.get('messages', [])
        temperature = float(payload.get('temperature', 0.1))
        max_tokens = int(payload.get('max_tokens', 2000))

        # Merge OpenAI-style messages with our training context
        training_context = build_training_context()
        sys_prompt = "You are a RouterOS assistant. Prefer MikroTik CLI. Enforce ROS6→ROS7 standards and Nextlink rules."
        if training_context:
            sys_prompt += "\n\n" + training_context

        messages = [{"role": "system", "content": sys_prompt}]
        for m in msgs:
            role = m.get('role')
            content = m.get('content')
            if role in ("system", "user", "assistant") and isinstance(content, str):
                messages.append({"role": role, "content": content})

        answer = call_ai(messages, temperature=temperature, max_tokens=max_tokens)

        resp = {
            "id": f"chatcmpl_{int(datetime.utcnow().timestamp())}",
            "object": "chat.completion",
            "created": int(datetime.utcnow().timestamp()),
            "model": payload.get('model') or os.getenv('OPENAI_COMPAT_MODEL', 'noc-local'),
            "choices": [
                {
                    "index": 0,
                    "message": {"role": "assistant", "content": answer},
                    "finish_reason": "stop"
                }
            ],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
        return jsonify(resp)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ========================================
# AI PROVIDER INITIALIZATION
# ========================================
# Initialize based on provider (config already defined above)
if AI_PROVIDER == 'openai':
    try:
        from openai import OpenAI, OpenAIError, RateLimitError, AuthenticationError
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        safe_print(f"Using OpenAI (API Key: {'configured' if OPENAI_API_KEY else 'MISSING'})")
    except ImportError:
        safe_print("WARNING: OpenAI library not installed. Install with: pip install openai")
        AI_PROVIDER = 'ollama'
else:
    safe_print(f"Using Ollama (Local AI)")
    safe_print(f"Default Model: {OLLAMA_MODEL} (will auto-select best model based on config size)")
    safe_print(f"API URL: {OLLAMA_API_URL}")
    safe_print(f"[INFO] Model selection is automatic - will choose best model for each request")

# ========================================
# AI-POWERED CONFIG HELPERS
# ========================================

def call_ollama(messages, model=None, temperature=0.1, max_tokens=4000, timeout=None, task_type='chat', config_size=0):
    """
    Call Ollama local LLM API with automatic timeout and model selection
    
    Args:
        messages: List of message dicts with 'role' and 'content'
        model: Model name (auto-selected if None)
        temperature: Sampling temperature
        max_tokens: Maximum tokens to generate
        timeout: Request timeout in seconds (auto-calculated if None)
        task_type: Task type for timeout calculation
        config_size: Config size in characters for timeout calculation
    """
    if model is None:
        available_models = get_available_ollama_models()
        model = select_best_model(task_type, config_size, available_models)
        estimated_lines = config_size // 50 if config_size > 0 else 0
        safe_print(f"[AUTO MODEL SELECTION] Selected: {model} for {task_type} (size: {config_size} chars, ~{estimated_lines} lines)")
    
    # Calculate timeout based on config size and model
    if timeout is None:
        # Estimate lines: ~50 chars per line average
        estimated_lines = config_size // 50 if config_size > 0 else 0
        
        # Validation tasks use shorter timeouts (frontend has 30s timeout)
        if task_type == 'validation':
            if config_size > 30000 or estimated_lines > 600:
                timeout = 25  # 25 seconds max for validation (frontend timeout is 30s)
            elif config_size > 10000 or estimated_lines > 200:
                timeout = 20  # 20 seconds for medium configs
            else:
                timeout = 15  # 15 seconds for small configs
        # Base timeout: scale with config size and lines
        elif config_size > 50000 or estimated_lines > 1000:
            timeout = 240  # 4 minutes for very large configs (>1000 lines)
        elif config_size > 20000 or estimated_lines > 500:
            timeout = 180  # 3 minutes for large configs (500-1000 lines)
        elif config_size > 10000 or estimated_lines > 200:
            timeout = 120  # 2 minutes for medium-large configs (200-500 lines)
        elif config_size > 5000 or estimated_lines > 100:
            timeout = 90   # 90 seconds for medium configs (100-200 lines)
        elif task_type in ['translation', 'upgrade']:
            timeout = 60   # 1 minute for translations
        else:
            timeout = 30   # 30 seconds for quick tasks
    
    try:
        # Convert messages to Ollama format
        prompt = "\n\n".join([f"{msg['role'].upper()}: {msg['content']}" for msg in messages])
        
        estimated_lines = len(prompt) // 50
        safe_print(f"[OLLAMA] Calling {model} with timeout {timeout}s (prompt: {len(prompt)} chars, ~{estimated_lines} lines)")
        
        response = requests.post(
            f"{OLLAMA_API_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            },
            timeout=timeout
        )
        
        if response.status_code != 200:
            raise Exception(f"Ollama API returned {response.status_code}: {response.text}")
        
        result = response.json()
        response_text = result.get('response', '')
        safe_print(f"[OLLAMA] Successfully got response ({len(response_text)} chars)")
        return response_text
        
    except requests.exceptions.ConnectionError:
        raise Exception(
            "Cannot connect to Ollama. Make sure Ollama is running and reachable. "
            "If you're running via Docker Compose, start the Ollama service with: "
            "`docker compose up -d --build ollama`. "
            "If you're running Ollama on the host, install from: https://ollama.com/download "
            "and set OLLAMA_API_URL (default: http://localhost:11434)."
        )
    except requests.exceptions.Timeout:
        # Automatic fallback to smaller/faster model on timeout
        available_models = get_available_ollama_models()
        current_index = available_models.index(model) if model in available_models else -1
        
        # Try next smaller/faster model
        fallback_models = ['phi3:mini', 'llama3.2:3b', 'llama3.2']
        for fallback in fallback_models:
            if fallback in available_models and fallback != model:
                safe_print(f"[OLLAMA TIMEOUT] Model {model} timed out. Trying faster model: {fallback}")
                try:
                    # Retry with smaller model and shorter timeout
                    return call_ollama(messages, model=fallback, temperature=temperature, 
                                     max_tokens=min(max_tokens, 2000), timeout=30, 
                                     task_type=task_type, config_size=config_size)
                except Exception:
                    continue  # Try next fallback
        
        raise Exception(f"Ollama request timed out with model '{model}'. Tried fallback models but all failed. The config might be too large or your system is under heavy load.")
    except Exception as e:
        raise Exception(f"Ollama Error: {str(e)}")


def call_openai_chat(messages, model="gpt-4o", temperature=0.1, max_tokens=4000):
    """
    Call OpenAI API
    """
    try:
        response = openai_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    except RateLimitError:
        raise Exception("OpenAI API quota exceeded. Please check your billing settings.")
    except AuthenticationError:
        raise Exception("Invalid OpenAI API key. Please check server configuration.")
    except OpenAIError as e:
        raise Exception(f"OpenAI API Error: {str(e)}")
    except Exception as e:
        raise Exception(f"Unexpected error: {str(e)}")


def call_ai(messages, model=None, temperature=0.1, max_tokens=16000, task_type='chat', config_size=0):
    """
    Universal AI caller with smart model selection and automatic fallback
    Auto-selects best model based on task type, complexity, and available models
    Automatically falls back to smaller/faster models on timeout
    
    Args:
        messages: List of message dicts with 'role' and 'content'
        model: Model name (auto-selected if None based on task and config size)
        temperature: Sampling temperature
        max_tokens: Maximum tokens to generate
        task_type: Task type (chat, validation, translation, etc.)
        config_size: Config size in characters (for model selection)
    """
    # Calculate config size from messages if not provided
    if config_size == 0:
        total_content = sum(len(str(msg.get('content', ''))) for msg in messages)
        config_size = total_content
    
    # Auto-select model if not specified
    if not model:
        available_models = get_available_ollama_models()
        model = select_best_model(task_type, config_size, available_models)
        estimated_lines = config_size // 50 if config_size > 0 else 0
        safe_print(f"[AUTO MODEL SELECTION] Selected: {model} for {task_type} (size: {config_size} chars, ~{estimated_lines} lines)")
    
    if AI_PROVIDER == 'ollama':
        return call_ollama(messages, model, temperature, max_tokens, 
                          timeout=None, task_type=task_type, config_size=config_size)
    
    # Prefer OpenAI if configured, but fall back to Ollama on any error
    try:
        return call_openai_chat(messages, model or "gpt-4o", temperature, max_tokens)
    except Exception as e:
        safe_print(f"[AI FALLBACK] OpenAI failed: {e}. Falling back to Ollama...")
        available_models = get_available_ollama_models()
        fallback_model = select_best_model(task_type, config_size, available_models)
        return call_ollama(messages, fallback_model, temperature, max_tokens,
                          timeout=None, task_type=task_type, config_size=config_size)


def build_training_context() -> str:
    rules = get_training_rules()  # Lazy load
    if not rules:
        return ''
    parts = ["TRAINING DATA (ROS6→ROS7 standards):"]
    # Compact include for important sections
    for key in [
        'ai-consistency-rules', 'routing-ospf', 'routing-bgp', 'firewall',
        'ip-addresses', 'interfaces', 'mpls-vpls', 'snmp', 'users', 'queue',
        'nextlink-styleguide', 'system-prompt'
    ]:
        if key in rules:
            val = rules[key]
            if isinstance(val, dict):
                parts.append(f"[{key}]\n{json.dumps(val, indent=2)}")
            else:
                # Trim long docs to keep prompt size manageable
                txt = str(val)
                if len(txt) > 2000:
                    txt = txt[:2000] + "\n... (truncated)"
                parts.append(f"[{key}]\n{txt}")
    return "\n\n".join(parts)

# ========================================
# CONFIG NORMALIZATION / DEDUP
# ========================================

def normalize_line_breaks(config_text: str) -> str:
    """Remove RouterOS line continuation characters (backslash) and join broken lines."""
    if not isinstance(config_text, str):
        return config_text

    def _toggle_in_quote(in_quote: bool, text: str) -> bool:
        escape = False
        for ch in text:
            if escape:
                escape = False
                continue
            if ch == '\\':
                escape = True
                continue
            if ch == '"':
                in_quote = not in_quote
        return in_quote

    def _join_continuation(left: str, right: str, in_quote: bool) -> str:
        # RouterOS wraps long lines with a trailing "\" and indents the continuation.
        # We must avoid inserting spaces when the wrap happens mid-token (e.g., "sensitiv\" + "e").
        r = right.lstrip()
        if not left:
            return r
        if not r:
            return left

        last = left[-1]
        first = r[0]

        # If we are inside a quoted string, do NOT force-add whitespace; preserve what's already in `left`.
        if in_quote:
            return left + r

        # If left already ends with whitespace, just concat (continuation indentation removed already).
        if last.isspace():
            return left + r

        # Join without space when it looks like a token was split.
        token_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789._-")
        if (last in token_chars) and (first in token_chars):
            return left + r

        # Join without space when the next segment starts with punctuation that should immediately follow.
        if first in ",.;:)]}":
            return left + r

        # Join without space when left ends with '=' (value continuation).
        if last == '=':
            return left + r

        return left + ' ' + r

    lines = config_text.replace('\r\n', '\n').replace('\r', '\n').split('\n')
    normalized = []
    i = 0
    in_quote = False

    while i < len(lines):
        # Preserve spaces in the middle of a line; only strip trailing newline artifacts.
        line = lines[i]
        # Join one or more continuation lines ending in "\".
        while line.endswith('\\') and i + 1 < len(lines):
            prefix = line[:-1]  # keep any spaces before the backslash
            next_line = lines[i + 1]
            line = _join_continuation(prefix, next_line, in_quote)
            i += 1
        normalized.append(line.rstrip())
        in_quote = _toggle_in_quote(in_quote, line)
        i += 1

    return '\n'.join(normalized)

def normalize_config(config_text: str) -> str:
    """Normalize RouterOS config: strip markdown fences, deduplicate lines per section,
    and output sections in a stable order for consistency."""
    if not isinstance(config_text, str):
        return config_text
    
    # First normalize line breaks (remove \ continuations)
    txt = normalize_line_breaks(config_text)
    
    # Then proceed with existing normalization
    txt = txt.replace('```routeros', '').replace('```', '').replace('\r', '\n')
    lines = [ln.strip() for ln in txt.split('\n') if ln.strip()]

    # Section ordering for readability
    order = [
        '/system identity',
        '/queue type',
        '/interface bridge',
        '/interface ethernet',
        '/interface vlan',
        '/interface bridge port',
        '/ip address',
        '/ip pool',
        '/ip dhcp-server',
        '/ip dhcp-server network',
        '/ip dhcp-server option',
        '/ip dhcp-server option sets',
        '/routing ospf',
        '/routing bgp',
        '/mpls',
        '/interface vpls',
        '/snmp',
        '/ip service',
        '/ip firewall address-list',
        '/ip firewall filter',
        '/ip firewall nat',
        '/ip firewall mangle',
        '/system logging',
        '/system ntp',
    ]

    # Bucket lines by nearest section header
    buckets = {}
    current = None
    for ln in lines:
        if ln.startswith('/'):
            current = ln.split(' ', 2)[0] + ('' if ' ' not in ln else ' ' + ln.split(' ', 2)[1])
            # normalize known multi-word headers
            if ln.startswith('/ip firewall address-list'):
                current = '/ip firewall address-list'
            elif ln.startswith('/ip firewall filter'):
                current = '/ip firewall filter'
            elif ln.startswith('/ip firewall nat'):
                current = '/ip firewall nat'
            elif ln.startswith('/ip firewall mangle'):
                current = '/ip firewall mangle'
            elif ln.startswith('/routing ospf'):
                current = '/routing ospf'
            elif ln.startswith('/routing bgp'):
                current = '/routing bgp'
            elif ln.startswith('/interface bridge port'):
                current = '/interface bridge port'
            elif ln.startswith('/interface bridge'):
                current = '/interface bridge'
            elif ln.startswith('/interface ethernet'):
                current = '/interface ethernet'
            elif ln.startswith('/interface vlan'):
                current = '/interface vlan'
            elif ln.startswith('/interface vpls'):
                current = '/interface vpls'
            elif ln.startswith('/system identity'):
                current = '/system identity'
            elif ln.startswith('/system logging'):
                current = '/system logging'
            elif ln.startswith('/system ntp'):
                current = '/system ntp'
            elif ln.startswith('/ip address'):
                current = '/ip address'
            elif ln.startswith('/ip pool'):
                current = '/ip pool'
            elif ln.startswith('/ip dhcp-server network'):
                current = '/ip dhcp-server network'
            elif ln.startswith('/ip dhcp-server option sets'):
                current = '/ip dhcp-server option sets'
            elif ln.startswith('/ip dhcp-server option'):
                current = '/ip dhcp-server option'
            elif ln.startswith('/ip dhcp-server'):
                current = '/ip dhcp-server'
            elif ln.startswith('/ip service'):
                current = '/ip service'
            elif ln.startswith('/snmp'):
                current = '/snmp'
            buckets.setdefault(current, [])
            buckets[current].append(ln)
        else:
            if current is None:
                current = '_preamble'
            buckets.setdefault(current, [])
            buckets[current].append(ln)

    # Deduplicate while preserving order within each bucket
    def dedup(seq):
        seen = set()
        out = []
        for s in seq:
            key = s
            if key in seen:
                continue
            seen.add(key)
            out.append(s)
        return out

    parts = []
    for sect in order:
        if sect in buckets:
            parts.extend(dedup(buckets[sect]))
            parts.append('')
    # Append any remaining buckets not in order list
    for sect, content in buckets.items():
        if sect not in order:
            parts.extend(dedup(content))
            parts.append('')
    return '\n'.join([p for p in parts]).strip() + '\n'

def remove_duplicate_entries(config_text: str) -> str:
    """
    Comprehensive deduplication: Remove duplicate entries within each section.
    Preserves order and ensures no duplicate commands exist.
    """
    if not isinstance(config_text, str):
        return config_text
    
    lines = config_text.split('\n')
    result = []
    current_section = None
    section_lines = []
    
    for line in lines:
        stripped = line.strip()
        
        # Check if this is a section header
        if stripped.startswith('/'):
            # Process previous section (deduplicate)
            if current_section is not None:
                # Deduplicate section content
                deduped = []
                seen_content = set()
                for sl in section_lines:
                    # Normalize line for comparison (remove extra whitespace)
                    normalized = re.sub(r'\s+', ' ', sl.strip())
                    # Create a key that ignores order of parameters
                    if normalized.startswith('add ') or normalized.startswith('set '):
                        # Extract key parameters for comparison
                        key_parts = []
                        # Extract address=, interface=, name=, target=, etc. for comparison
                        addr_match = re.search(r'address=([^\s]+)', normalized)
                        iface_match = re.search(r'interface=([^\s]+)', normalized)
                        name_match = re.search(r'name=([^\s]+)', normalized)
                        target_match = re.search(r'target=([^\s]+)', normalized)
                        chain_match = re.search(r'chain=([^\s]+)', normalized)
                        
                        # Build comparison key
                        if addr_match:
                            key_parts.append(f"addr={addr_match.group(1)}")
                        if iface_match:
                            key_parts.append(f"iface={iface_match.group(1)}")
                        if name_match:
                            key_parts.append(f"name={name_match.group(1)}")
                        if target_match:
                            key_parts.append(f"target={target_match.group(1)}")
                        if chain_match:
                            key_parts.append(f"chain={chain_match.group(1)}")
                        
                        # If we have key parts, use them for deduplication
                        if key_parts:
                            content_key = '|'.join(sorted(key_parts))
                        else:
                            content_key = normalized
                    else:
                        content_key = normalized
                    
                    if content_key not in seen_content:
                        seen_content.add(content_key)
                        deduped.append(sl)
                
                # Add deduplicated section
                result.extend(deduped)
                section_lines = []
            
            # Start new section
            current_section = stripped.split()[0] if stripped.split() else stripped
            result.append(line)
        else:
            # Content line - add to current section
            if current_section is None:
                # Preamble lines
                result.append(line)
            else:
                section_lines.append(line)
    
    # Process final section
    if current_section is not None and section_lines:
        deduped = []
        seen_content = set()
        for sl in section_lines:
            normalized = re.sub(r'\s+', ' ', sl.strip())
            if normalized.startswith('add ') or normalized.startswith('set '):
                # Extract key parameters
                key_parts = []
                addr_match = re.search(r'address=([^\s]+)', normalized)
                iface_match = re.search(r'interface=([^\s]+)', normalized)
                name_match = re.search(r'name=([^\s]+)', normalized)
                target_match = re.search(r'target=([^\s]+)', normalized)
                chain_match = re.search(r'chain=([^\s]+)', normalized)
                
                if addr_match:
                    key_parts.append(f"addr={addr_match.group(1)}")
                if iface_match:
                    key_parts.append(f"iface={iface_match.group(1)}")
                if name_match:
                    key_parts.append(f"name={name_match.group(1)}")
                if target_match:
                    key_parts.append(f"target={target_match.group(1)}")
                if chain_match:
                    key_parts.append(f"chain={chain_match.group(1)}")
                
                if key_parts:
                    content_key = '|'.join(sorted(key_parts))
                else:
                    content_key = normalized
            else:
                content_key = normalized
            
            if content_key not in seen_content:
                seen_content.add(content_key)
                deduped.append(sl)
        
        result.extend(deduped)
    
    return '\n'.join(result)

# ========================================
# ENDPOINT 1: AI Config Validation
# ========================================

def validate_enterprise_feeding_config(config_text):
    """
    Validates Enterprise Feeding Side configuration for accuracy and format.
    Returns (is_valid, errors, warnings)
    """
    errors = []
    warnings = []
    
    if not config_text or len(config_text.strip()) < 50:
        errors.append("Configuration is empty or too short")
        return False, errors, warnings
    
    # Validate required sections exist
    required_sections = [
        '/interface ethernet',
        '/ip address',
        '/routing ospf interface-template'
    ]
    missing_sections = []
    for section in required_sections:
        if section not in config_text:
            missing_sections.append(section)
    
    if missing_sections:
        errors.append(f"Missing required sections: {', '.join(missing_sections)}")
    
    # Validate IP address line format: add address=X.X.X.X/29 comment="..." interface=... network=...
    ip_address_match = re.search(r'add address=(\d+\.\d+\.\d+\.\d+)/(\d+)\s+comment=([^\s]+)\s+interface=([^\s]+)\s+network=(\d+\.\d+\.\d+\.\d+)', config_text)
    if not ip_address_match:
        errors.append("Missing or malformed IP address line")
    else:
        gateway_ip = ip_address_match.group(1)
        prefix = ip_address_match.group(2)
        network_addr = ip_address_match.group(5)
        
        # Validate IP address format
        try:
            import ipaddress
            ipaddress.IPv4Address(gateway_ip)
            ipaddress.IPv4Address(network_addr)
            prefix_int = int(prefix)
            if prefix_int < 8 or prefix_int > 30:
                errors.append(f"Invalid prefix length: /{prefix} (must be between /8 and /30)")
        except ValueError as e:
            errors.append(f"Invalid IP address format: {str(e)}")
    
    # Validate OSPF interface-template format
    ospf_match = re.search(r'/routing ospf interface-template.*?networks=(\d+\.\d+\.\d+\.\d+/\d+)', config_text, re.DOTALL)
    if not ospf_match:
        warnings.append("OSPF interface-template not found or malformed")
    else:
        ospf_network = ospf_match.group(1)
        # Verify OSPF uses network address (should match IP address network)
        if ip_address_match:
            expected_network = ip_address_match.group(5)
            if expected_network not in ospf_network:
                warnings.append("OSPF network parameter should match IP address network parameter")
    
    # Validate routes format if present
    route_matches = re.findall(r'add comment=([^\s]+)\s+disabled=no\s+distance=1\s+dst-address=([^\s]+)\s+gateway=([^\s]+)', config_text)
    if route_matches:
        for route in route_matches:
            dst = route[1]
            gateway = route[2]
            # Validate route format
            try:
                import ipaddress
                # Handle CIDR in dst-address
                dst_ip = dst.split('/')[0]
                ipaddress.IPv4Address(dst_ip)
                ipaddress.IPv4Address(gateway)
            except ValueError:
                warnings.append(f"Invalid route format: dst-address={dst}, gateway={gateway}")
    
    is_valid = len(errors) == 0
    return is_valid, errors, warnings

@app.route('/api/validate-config', methods=['POST'])
def validate_config():
    """
    Validates a RouterOS config for syntax errors, missing fields, RFC compliance
    """
    try:
        print(f"[VALIDATE] Validation request received for type: {request.json.get('type', 'unknown')}")
        data = request.json
        config = data.get('config', '')
        config_type = data.get('type', 'tower')  # tower, enterprise, mpls, enterprise-feeding
        
        if not config:
            return jsonify({'error': 'No configuration provided'}), 400
        
        print(f"[VALIDATE] Config size: {len(config)} characters, type: {config_type}")
        
        # For Enterprise Feeding, use specific validation first (like Tarana)
        if config_type == 'enterprise-feeding':
            is_valid, validation_errors, validation_warnings = validate_enterprise_feeding_config(config)
            if not is_valid:
                print(f"[VALIDATE] Enterprise Feeding validation failed: {validation_errors}")
                return jsonify({
                    'success': True,
                    'validation': {
                        'valid': False,
                        'issues': [{'severity': 'error', 'message': err} for err in validation_errors] + 
                                 [{'severity': 'warning', 'message': warn} for warn in validation_warnings],
                        'summary': f"Validation found {len(validation_errors)} error(s) and {len(validation_warnings)} warning(s)"
                    }
                })
            elif validation_warnings:
                print(f"[VALIDATE] Enterprise Feeding validation passed with warnings: {validation_warnings}")
                return jsonify({
                    'success': True,
                    'validation': {
                        'valid': True,
                        'issues': [{'severity': 'warning', 'message': warn} for warn in validation_warnings],
                        'summary': f"Validation passed with {len(validation_warnings)} warning(s)"
                    }
                })
            else:
                print(f"[VALIDATE] Enterprise Feeding validation passed - no issues")
                return jsonify({
                    'success': True,
                    'validation': {
                        'valid': True,
                        'issues': [],
                        'summary': 'Validation passed - no issues found'
                    }
                })

        print(f"[VALIDATE] Calling AI for validation...")

        # Build Nextlink-specific context
        nextlink_context = f"""
NEXTLINK CONFIGURATION STANDARDS:

Device Roles:
{json.dumps(NEXTLINK_DEVICE_ROLES, indent=2)}

Naming Conventions:
- Devices: {NEXTLINK_NAMING['device_patterns']['tower']} or {NEXTLINK_NAMING['device_patterns']['core']}
- Bridges: {', '.join(NEXTLINK_NAMING['bridge_patterns'].values())}
- VLANs: {NEXTLINK_NAMING['vlan_patterns']['format']}

IP Addressing:
- Loopbacks: {NEXTLINK_IP_RANGES['loopback']['format']}
- Uplinks: {', '.join(NEXTLINK_IP_RANGES['uplink']['formats'])}
- Management VLANs: {json.dumps(NEXTLINK_IP_RANGES['management_vlans'], indent=2)}
- Customer VLANs: Range {NEXTLINK_IP_RANGES['customer_vlans']['range']}

Common NOC Errors to Check:
{chr(10).join([f"- {err['error']} (severity: {err['severity']})" for err in NEXTLINK_COMMON_ERRORS])}

Auto-detectable Errors:
{chr(10).join([f"- {err}" for err in NEXTLINK_AUTO_DETECTABLE_ERRORS])}
"""

        training_context = build_training_context()
        system_prompt = f"""You are a Nextlink NOC MikroTik RouterOS configuration validator. 
Analyze the provided configuration and identify:
1. Syntax errors
2. Missing required fields
3. RFC compliance issues (OSPF RFC 2328, BGP RFC 4271, MPLS RFC 3031, IPv4 RFC 791)
4. Security issues
5. Best practice violations
6. Nextlink-specific standard violations

{nextlink_context}

Return JSON format:
{{
  "valid": true/false,
  "issues": [{{"severity": "error|warning|info", "message": "description", "line": number}}],
  "summary": "brief summary"
}}
"""
        if training_context:
            system_prompt += "\n\n" + training_context

        user_prompt = f"""Validate this RouterOS configuration ({config_type}):

```
{config}
```

Provide validation results in JSON format."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        config_size = len(config)
        
        # For validation, use a shorter timeout and smaller model for faster response
        # Validation doesn't need to process the entire config in detail - just check for common issues
        safe_print(f"[VALIDATE] Calling AI for validation (config size: {config_size} chars, using fast validation mode)...")
        
        # Truncate very large configs for validation (keep first 80% and last 20% to catch common issues)
        # This speeds up validation significantly without losing important context
        validation_config = config
        if config_size > 30000:  # For configs > 30KB
            truncate_point = int(config_size * 0.8)
            validation_config = config[:truncate_point] + "\n\n[... truncated for validation ...]\n\n" + config[-int(config_size * 0.2):]
            safe_print(f"[VALIDATE] Config truncated for faster validation: {len(validation_config)} chars (from {config_size})")
            # Update user prompt with truncated config
            user_prompt = f"""Validate this RouterOS configuration ({config_type}):

```
{validation_config}
```

Provide validation results in JSON format. Note: Large config was truncated for faster validation."""
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        
        # Use smaller max_tokens and faster model for validation
        try:
            result = call_ai(messages, max_tokens=1500, task_type='validation', config_size=min(config_size, 30000))
            safe_print(f"[VALIDATE] AI response received, parsing...")
        except Exception as ai_error:
            # If AI validation fails/times out, return a basic validation result
            safe_print(f"[VALIDATE] AI validation failed or timed out: {ai_error}")
            safe_print(f"[VALIDATE] Returning basic validation result (config generated, manual review recommended)")
            validation_result = {
                "valid": True,
                "issues": [
                    {
                        "severity": "info",
                        "message": "AI validation unavailable or timed out. Configuration generated successfully but manual review recommended.",
                        "line": 0
                    }
                ],
                "summary": "Configuration generated. AI validation skipped due to timeout or unavailability."
            }
            return jsonify({
                'success': True,
                'validation': validation_result
            })
        
        # Parse JSON response
        try:
            validation_result = json.loads(result)
        except:
            # Fallback if AI doesn't return pure JSON
            print(f"[VALIDATE] AI response not in JSON format, using fallback")
            validation_result = {
                "valid": True,
                "issues": [],
                "summary": result
            }

        print(f"[VALIDATE] Validation complete, returning results")
        return jsonify({
            'success': True,
            'validation': validation_result
        })

    except Exception as e:
        print(f"[VALIDATE ERROR] {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# ========================================
# ENDPOINT 2: AI Config Suggestions
# ========================================

@app.route('/api/suggest-config', methods=['POST'])
def suggest_config():
    """
    AI suggests config values based on partial input (autocomplete assistant)
    """
    try:
        data = request.json
        
        # Handle both old format (partial_config) and new format (device-based)
        if 'device' in data:
            # New format from frontend
            device = data.get('device', '')
            target_version = data.get('target_version', '')
            loopback_ip = data.get('loopback_ip', '')
            public_cidr = data.get('public_cidr', '')
            bh_cidr = data.get('bh_cidr', '')
            
            # Generate suggestions based on device type
            suggestions = {}
            
            if device == 'ccr2004':
                suggestions = {
                    'public_port': 'sfp-sfpplus7',
                    'nat_port': 'sfp-sfpplus8',
                    'uplink_interface': 'sfp-sfpplus1',
                    'public_pool': f"{public_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(public_cidr.split('/')[0].split('.')[-1]) + 1}-{public_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(public_cidr.split('/')[0].split('.')[-1]) + 2}",
                    'gateway': f"{bh_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(bh_cidr.split('/')[0].split('.')[-1]) - 1}"
                }
            elif device == 'rb5009':
                suggestions = {
                    'public_port': 'ether7',
                    'nat_port': 'ether8',
                    'uplink_interface': 'sfp-sfpplus1',
                    'public_pool': f"{public_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(public_cidr.split('/')[0].split('.')[-1]) + 1}-{public_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(public_cidr.split('/')[0].split('.')[-1]) + 2}",
                    'gateway': f"{bh_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(bh_cidr.split('/')[0].split('.')[-1]) - 1}"
                }
            elif device == 'ccr1036':
                suggestions = {
                    'public_port': 'sfp-sfpplus7',
                    'nat_port': 'sfp-sfpplus8',
                    'uplink_interface': 'sfp-sfpplus1',
                    'public_pool': f"{public_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(public_cidr.split('/')[0].split('.')[-1]) + 1}-{public_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(public_cidr.split('/')[0].split('.')[-1]) + 2}",
                    'gateway': f"{bh_cidr.split('/')[0].rsplit('.', 1)[0]}.{int(bh_cidr.split('/')[0].split('.')[-1]) - 1}"
                }
            
            return jsonify({
                'success': True,
                'public_port': suggestions.get('public_port', ''),
                'nat_port': suggestions.get('nat_port', ''),
                'uplink_interface': suggestions.get('uplink_interface', ''),
                'public_pool': suggestions.get('public_pool', ''),
                'gateway': suggestions.get('gateway', '')
            })
        
        # Old format (legacy)
        partial_config = data.get('partial_config', '')
        config_type = data.get('type', 'tower')
        context = data.get('context', {})  # Customer info, site details, etc.

        training_context = build_training_context()
        system_prompt = """You are a MikroTik RouterOS configuration assistant for NOC operations.
Given partial configuration and context, suggest appropriate values for:
- IP addressing schemes
- OSPF/BGP parameters
- MPLS labels
- Firewall rules
- Interface configurations

Follow these rules:
1. Use RFC-compliant values
2. Maintain consistency with existing network design
3. Suggest private IPs (RFC 1918) for internal networks
4. Use logical OSPF areas and BGP AS numbers
5. Provide explanations for suggestions

Return JSON format:
{
  "suggestions": [{"field": "name", "value": "suggested value", "reason": "why"}],
  "warnings": ["potential issues"]
}
"""
        if training_context:
            system_prompt += "\n\n" + training_context

        user_prompt = f"""Configuration Type: {config_type}
Context: {json.dumps(context, indent=2)}

Partial Configuration:
```
{partial_config}
```

Suggest appropriate values for missing or incomplete fields."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        result = call_ai(messages, max_tokens=2000, task_type='suggestion')

        try:
            suggestions = json.loads(result)
        except:
            suggestions = {"suggestions": [], "warnings": [], "raw": result}

        return jsonify({
            'success': True,
            'suggestions': suggestions
        })

    except Exception as e:
        safe_print(f"[TRANSLATE CONFIG ERROR] {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

# ========================================
# CONFIG FORMATTING HELPER
# ========================================

def format_config_spacing(config_text):
    """
    Format RouterOS config for readability without changing semantics.

    Goals:
    - Deterministic output (same input -> same output)
    - Add a single blank line before each top-level section line (lines starting with '/')
    - Normalize obvious key/value spacing (e.g., `foo= bar` -> `foo=bar`) but ONLY outside quoted strings
    - Trim trailing whitespace
    """
    if not config_text:
        return config_text

    def _normalize_kv_spacing_outside_quotes(line: str) -> str:
        if not line or '"' not in line:
            # Only remove whitespace adjacent to '=' (do not touch '==' since it has no whitespace)
            line = re.sub(r'\s+=', '=', line)
            line = re.sub(r'=\s+', '=', line)
            return line

        out_parts = []
        buf = []
        in_quote = False
        escape = False

        def flush_buf(is_quoted: bool):
            if not buf:
                return
            segment = ''.join(buf)
            if not is_quoted:
                segment = re.sub(r'\s+=', '=', segment)
                segment = re.sub(r'=\s+', '=', segment)
            out_parts.append(segment)
            buf.clear()

        for ch in line:
            if escape:
                buf.append(ch)
                escape = False
                continue
            if ch == '\\':
                buf.append(ch)
                escape = True
                continue
            if ch == '"':
                flush_buf(in_quote)
                out_parts.append('"')
                in_quote = not in_quote
                continue
            buf.append(ch)

        flush_buf(in_quote)
        return ''.join(out_parts)

    normalized = config_text.replace('\r\n', '\n').replace('\r', '\n')
    lines = normalized.split('\n')
    formatted_lines = []

    # Drop exact duplicate lines (common after merges), but never touch embedded script sources.
    seen_exact = set()

    for raw_line in lines:
        line = raw_line.rstrip()

        # Drop known export-only warnings that shouldn't be re-applied.
        if line.strip().lower() == '# unsupported speed':
            continue

        # Drop RouterOS visual separators / "BREAK" rules.
        # These are commonly inserted as visual dividers and are not intended to be applied.
        # Examples:
        #   add chain=break comment="--------- BREAK --------- ..."
        #   add action=break comment="--------- BREAK --------- ..." disabled=yes
        if re.search(r'(?i)\bchain\s*=\s*break\b', line) and re.search(r'(?i)\bBREAK\b', line):
            continue
        if re.search(r'(?i)\baction\s*=\s*break\b', line) and re.search(r'(?i)\bBREAK\b', line) and re.search(r'-{5,}', line):
            continue
        if re.search(r'(?i)\bBREAK\b', line) and re.search(r'-{5,}', line):
            continue

        # Fix user group policy line-wrapping (RouterOS export sometimes breaks tokens like "sensitiv e").
        # Safe rule: policy is a comma-separated token list; whitespace inside the value is never meaningful.
        if re.search(r'(?i)\bpolicy\s*=', line):
            def _fix_policy_quoted(m):
                inner = m.group(1)
                inner = re.sub(r'\s+', '', inner)
                return f'policy="{inner}"'

            # Quoted form: policy="a,b,c"
            line = re.sub(r'(?i)\bpolicy\s*=\s*"([^"]*)"', _fix_policy_quoted, line)

            # Unquoted form (common on /user group set read): policy=a,b,c
            # Capture value until next " key=" token or end-of-line, then strip whitespace inside.
            def _fix_policy_unquoted(m):
                head = m.group(1)
                inner = m.group(2)
                tail = m.group(3) or ''
                inner = re.sub(r'\s+', '', inner)
                return f"{head}{inner}{tail}"

            line = re.sub(r'(?i)(\bpolicy\s*=\s*)([^"\r\n]+?)(\s+\w+\s*=\s*.*)?$', _fix_policy_unquoted, line)

        # Fix common RouterOS rule token splits caused by line-wrapping in exports (do NOT remove normal spaces).
        # Only apply to routing filter rule strings (rule="...").
        if re.search(r'\brule="', line):
            def _fix_rule(m):
                inner = m.group(1)
                inner = re.sub(r'(?i)dst-le\s+n', 'dst-len', inner)
                inner = re.sub(r'(?i)blackh\s+ole', 'blackhole', inner)
                inner = re.sub(r'(?i)bgp-communit\s+ies', 'bgp-communities', inner)
                inner = re.sub(r'(?i)bgp-local-\s*pref', 'bgp-local-pref', inner)
                return f'rule="{inner}"'
            line = re.sub(r'rule="([^"]+)"', _fix_rule, line)

        line = _normalize_kv_spacing_outside_quotes(line)

        # Drop exact duplicate lines (outside script sources).
        if line and not line.lstrip().startswith('/') and not re.search(r'(?i)\bsource\s*=\s*"', line):
            key = line.strip()
            if key in seen_exact:
                continue
            seen_exact.add(key)

        stripped = line.strip()
        is_section_line = stripped.startswith('/') and stripped != '/'

        if is_section_line and formatted_lines and formatted_lines[-1].strip() != '':
            formatted_lines.append('')
        formatted_lines.append(line)

    # Remove leading/trailing blank lines and limit to a single blank line between blocks.
    while formatted_lines and formatted_lines[0].strip() == '':
        formatted_lines.pop(0)
    while formatted_lines and formatted_lines[-1].strip() == '':
        formatted_lines.pop()

    result = '\n'.join(formatted_lines)
    # At most one blank line between content blocks (i.e., no "\n\n\n").
    result = re.sub(r'\n{3,}', '\n\n', result)
    return result + '\n'

# ========================================
# ENDPOINT 3: AI Config Translation
# ========================================

@app.route('/api/translate-config', methods=['POST'])
def translate_config():
    """
    Translates RouterOS config between firmware versions
    """
    try:
        data = request.json
        source_config = data.get('source_config', '')
        target_device = data.get('target_device', '')
        target_version = data.get('target_version', '')
        # Behavior flags:
        # - strict_preserve: preserve source structure/lines; only apply syntax + interface mapping (recommended for Upgrade Existing)
        # - apply_compliance: optionally append RFC-09-10-25 compliance blocks (additive; may intentionally differ from live configs)
        strict_preserve = bool(data.get('strict_preserve', True))
        apply_compliance = bool(data.get('apply_compliance', False))

        if not all([source_config, target_device, target_version]):
            return jsonify({'error': 'Missing required fields'}), 400
        
        # Normalize line breaks FIRST (remove RouterOS \ continuation characters)
        source_config = normalize_line_breaks(source_config)
        
        # Define helper functions FIRST before using them
        def detect_routeros_syntax(config):
            """Intelligently detect RouterOS version and syntax patterns from config"""
            syntax_info = {
                'version': 'unknown',
                'bgp_syntax': 'unknown',
                'ospf_syntax': 'unknown',
                'parameter_style': 'unknown'
            }
            
            # Detect version from multiple patterns
            if 'by RouterOS 6.' in config or 'RouterOS 6.' in config:
                syntax_info['version'] = '6.x'
            elif 'by RouterOS 7.' in config or 'RouterOS 7.' in config or 'interface-template' in config:
                syntax_info['version'] = '7.x'
            
            # Detect BGP syntax
            if '/routing bgp peer' in config:
                syntax_info['bgp_syntax'] = 'peer'
            elif '/routing bgp connection' in config:
                syntax_info['bgp_syntax'] = 'connection'
            
            # Detect OSPF syntax  
            if '/routing ospf interface add' in config:
                syntax_info['ospf_syntax'] = 'interface'
            elif '/routing ospf interface-template add' in config:
                syntax_info['ospf_syntax'] = 'interface-template'
            
            # Detect parameter style
            if 'remote-address=' in config:
                syntax_info['parameter_style'] = 'dash'
            elif 'remote.address=' in config:
                syntax_info['parameter_style'] = 'dot'

            # Fallback: infer version from syntax if header is missing
            if syntax_info['version'] == 'unknown':
                if '/routing ospf interface-template' in config or '/routing bgp connection' in config or 'remote.address=' in config:
                    syntax_info['version'] = '7.x'
                elif '/routing ospf interface' in config or '/routing bgp peer' in config or '/interface ethernet' in config:
                    syntax_info['version'] = '6.x'
            
            return syntax_info
        
        def get_target_syntax(target_version):
            """Determine target syntax based on RouterOS version"""
            if target_version.startswith('7.'):
                return {
                    'bgp_peer': '/routing bgp connection',
                    'bgp_params': {
                        'remote-address': 'remote.address',
                        'remote-as': 'remote.as',
                        'tcp-md5-key': 'tcp.md5.key',
                        'update-source': 'update.source'
                    },
                    'ospf_interface': '/routing ospf interface-template',
                    'ospf_params': {
                        'interface': 'interfaces'
                    },
                    'bridge_vlan': True
                }
            else:
                return {
                    'bgp_peer': '/routing bgp peer',
                    'bgp_params': {
                        'remote-address': 'remote-address',
                        'remote-as': 'remote-as',
                        'tcp-md5-key': 'tcp-md5-key',
                        'update-source': 'update-source'
                    },
                    'ospf_interface': '/routing ospf interface',
                    'ospf_params': {
                        'interface': 'interface'
                    },
                    'bridge_vlan': False
                }

        def detect_source_device(config):
            """Intelligently detect source device from config patterns (deterministic)."""
            device_info = {
                'model': 'unknown',
                'type': 'unknown',
                'ports': [],
                'management': 'ether1'
            }

            text = config or ''

            def _ports_list(specs):
                ports = []
                for group in specs.get('ports', {}).values():
                    ports.extend(group)
                return ports

            def _normalize(s: str) -> str:
                return re.sub(r'[^a-z0-9]+', '', (s or '').lower())

            aliases = {
                'ccr1036': 'CCR1036-12G-4S',
                'ccr1072': 'CCR1072-12G-4S+',
                'ccr2004': 'CCR2004-1G-12S+2XS',
                'ccr2004-1g-12s+2xs': 'CCR2004-1G-12S+2XS',
                'ccr2004-16g-2s+': 'CCR2004-16G-2S+',
                'ccr2116': 'CCR2116-12G-4S+',
                'ccr2216': 'CCR2216-1G-12XS-2XQ',
                'rb5009': 'RB5009UG+S+',
                'rb2011': 'RB2011UiAS',
                'rb1009': 'RB1009UG+S+',
                'crs326': 'CRS326-24G-2S+',
                'crs354': 'CRS354-48G-4S+2Q+'
            }

            # Prefer explicit RouterOS export header model.
            header_model = None
            hm = re.search(r'(?m)^\s*#\s*model\s*=\s*(.+?)\s*$', text)
            if hm:
                header_model = hm.group(1).strip().strip('"').strip("'")

            # Try direct model match
            if header_model:
                if header_model in ROUTERBOARD_INTERFACES:
                    specs = ROUTERBOARD_INTERFACES[header_model]
                    return {
                        'model': specs['model'],
                        'type': specs['series'].lower(),
                        'ports': _ports_list(specs),
                        'management': specs.get('management_port', 'ether1')
                    }

                # Try normalized match
                norm_header = _normalize(header_model)
                for model_key, specs in ROUTERBOARD_INTERFACES.items():
                    if _normalize(model_key) == norm_header:
                        return {
                            'model': specs['model'],
                            'type': specs['series'].lower(),
                            'ports': _ports_list(specs),
                            'management': specs.get('management_port', 'ether1')
                        }

                # Try alias match (including CCR2004-16G variant)
                alias_key = aliases.get(_normalize(header_model))
                if not alias_key and '2004' in norm_header and '16g' in norm_header:
                    alias_key = aliases.get('ccr2004-16g-2s+')
                if alias_key and alias_key in ROUTERBOARD_INTERFACES:
                    specs = ROUTERBOARD_INTERFACES[alias_key]
                    return {
                        'model': specs['model'],
                        'type': specs['series'].lower(),
                        'ports': _ports_list(specs),
                        'management': specs.get('management_port', 'ether1')
                    }

            # Prefer identity-based detection (e.g., RTR-MT2004-..., RTR-MTCCR2216-...).
            ident_digits = None
            im = re.search(r'(?i)\bMT(?:CCR)?(\d{3,4})\b', text)
            if im:
                ident_digits = im.group(1)

            # Resolve a canonical type/model if we can.
            model_hint = (header_model or '')
            digits_hint = None
            dm = re.search(r'(?i)\b(?:CCR|RB|CRS)\s*(\d{3,4})\b', model_hint.replace('-', ' '))
            if dm:
                digits_hint = dm.group(1)
            if not digits_hint:
                digits_hint = ident_digits

            alias_key = None
            if digits_hint == '2216':
                alias_key = aliases['ccr2216']
            elif digits_hint == '2116':
                alias_key = aliases['ccr2116']
            elif digits_hint == '2004':
                if '16g' in (model_hint or '').lower() or '16g' in text.lower():
                    alias_key = aliases['ccr2004-16g-2s+']
                else:
                    alias_key = aliases['ccr2004']
            elif digits_hint == '1072':
                alias_key = aliases['ccr1072']
            elif digits_hint == '1036':
                alias_key = aliases['ccr1036']
            elif digits_hint == '5009':
                alias_key = aliases['rb5009']
            elif digits_hint == '2011':
                alias_key = aliases['rb2011']
            elif digits_hint == '1009':
                alias_key = aliases['rb1009']
            elif digits_hint == '326':
                alias_key = aliases['crs326']
            elif digits_hint == '354':
                alias_key = aliases['crs354']

            if alias_key and alias_key in ROUTERBOARD_INTERFACES:
                specs = ROUTERBOARD_INTERFACES[alias_key]
                return {
                    'model': specs['model'],
                    'type': specs['series'].lower(),
                    'ports': _ports_list(specs),
                    'management': specs.get('management_port', 'ether1')
                }

            # Fallback: interface-pattern scoring across all known models.
            iface_pattern = r"\b(ether\d+|sfp\d+(?:-\d+)?|sfp-sfpplus\d+|sfp28-\d+|qsfp28-\d+-\d+|qsfpplus\d+-\d+|qsfp\d+(?:-\d+)?|combo\d+)\b"
            found_ifaces = set(re.findall(iface_pattern, text))
            if found_ifaces:
                best = None
                best_score = -1
                best_ports = -1
                for model_key, specs in ROUTERBOARD_INTERFACES.items():
                    ports = set(_ports_list(specs))
                    score = len(found_ifaces & ports)
                    if score > best_score or (score == best_score and specs.get('total_ports', 0) > best_ports):
                        best_score = score
                        best_ports = specs.get('total_ports', 0)
                        best = specs
                if best and best_score > 0:
                    return {
                        'model': best['model'],
                        'type': best['series'].lower(),
                        'ports': _ports_list(best),
                        'management': best.get('management_port', 'ether1')
                    }

            return device_info
        
        def get_target_device_info(target_device):
            """Get target device information dynamically"""
            def ports_list(specs):
                ports = []
                for group in specs.get('ports', {}).values():
                    ports.extend(group)
                return ports

            aliases = {
                'ccr1036': 'CCR1036-12G-4S',
                'ccr1072': 'CCR1072-12G-4S+',
                'ccr2004': 'CCR2004-1G-12S+2XS',
                'ccr2004-1g-12s+2xs': 'CCR2004-1G-12S+2XS',
                'ccr2004-16g-2s+': 'CCR2004-16G-2S+',
                'ccr2116': 'CCR2116-12G-4S+',
                'ccr2216': 'CCR2216-1G-12XS-2XQ',
                'rb5009': 'RB5009UG+S+',
                'rb2011': 'RB2011UiAS',
                'rb1009': 'RB1009UG+S+',
                'crs326': 'CRS326-24G-2S+',
                'crs354': 'CRS354-48G-4S+2Q+'
            }

            key = (target_device or '').strip()
            key_lower = key.lower()

            # Direct match against ROUTERBOARD_INTERFACES
            if key in ROUTERBOARD_INTERFACES:
                specs = ROUTERBOARD_INTERFACES[key]
                return {
                    'model': specs['model'],
                    'type': specs['series'].lower(),
                    'ports': ports_list(specs),
                    'management': specs.get('management_port', 'ether1'),
                    'description': specs.get('series', 'Unknown')
                }

            # Alias match
            alias_key = aliases.get(key_lower)
            if alias_key and alias_key in ROUTERBOARD_INTERFACES:
                specs = ROUTERBOARD_INTERFACES[alias_key]
                return {
                    'model': specs['model'],
                    'type': specs['series'].lower(),
                    'ports': ports_list(specs),
                    'management': specs.get('management_port', 'ether1'),
                    'description': specs.get('series', 'Unknown')
                }

            # Fallback by digits in key
            if '1072' in key_lower:
                alias_key = aliases['ccr1072']
            elif '2216' in key_lower:
                alias_key = aliases['ccr2216']
            elif '2116' in key_lower:
                alias_key = aliases['ccr2116']
            elif '2004' in key_lower and '16g' in key_lower:
                alias_key = aliases['ccr2004-16g-2s+']
            elif '2004' in key_lower:
                alias_key = aliases['ccr2004']
            elif '1036' in key_lower:
                alias_key = aliases['ccr1036']
            elif '5009' in key_lower:
                alias_key = aliases['rb5009']
            elif '2011' in key_lower:
                alias_key = aliases['rb2011']
            elif '1009' in key_lower:
                alias_key = aliases['rb1009']
            elif 'crs326' in key_lower:
                alias_key = aliases['crs326']
            elif 'crs354' in key_lower:
                alias_key = aliases['crs354']
            else:
                alias_key = aliases.get('ccr2004')

            specs = ROUTERBOARD_INTERFACES.get(alias_key)
            if specs:
                return {
                    'model': specs['model'],
                    'type': specs['series'].lower(),
                    'ports': ports_list(specs),
                    'management': specs.get('management_port', 'ether1'),
                    'description': specs.get('series', 'Unknown')
                }

            return {
                'model': 'unknown',
                'type': 'unknown',
                'ports': ['ether1'],
                'management': 'ether1',
                'description': 'Unknown device'
            }
        
        # SMART DETECT: Only skip AI if SAME EXACT device model AND same major version
        # CRITICAL: Device changes (e.g., CCR2004 → CCR2216) require AI translation for proper port mapping
        is_source_v7 = ('interface-template' in source_config or 
                       'default-v2' in source_config or
                       'ros7' in source_config.lower())
        is_source_v6 = ('routing ospf interface' in source_config and 
                       'routing ospf instance' not in source_config)
        is_target_v7 = target_version.startswith('7.')
        
        # Detect source device model more precisely
        source_device_info = detect_source_device(source_config)
        target_device_info = get_target_device_info(target_device)
        
        # Only skip AI if SAME EXACT device model (not just same family)
        same_exact_device = (source_device_info['model'] != 'unknown' and 
                             source_device_info['model'] == target_device_info['model'])
        
        if is_source_v7 and is_target_v7 and same_exact_device:
            print(f"[FAST MODE] Same exact device ({source_device_info['model']}), same major version - minimal processing")
            # Just return source with device name update if needed
            translated = source_config
            validation = validate_translation(source_config, translated)
            return jsonify({
                'success': True,
                'translated_config': translated,
                'validation': validation,
                'fast_mode': True,
                'source_info': source_info,
                'target_info': target_info,
                'message': 'Config already compatible - no changes needed'
            })
        elif is_source_v7 and is_target_v7 and not same_exact_device:
            print(f"[AI MODE] Device change detected: {source_device_info['model']} → {target_device_info['model']} (both ROS7, but hardware differs - need port mapping)")
        elif is_source_v6 and is_target_v7:
            print(f"[AI MODE] v6 to v7 conversion needed - using AI")
        else:
            print(f"[AI MODE] Device change detected: {source_device_info['model']} → {target_device_info['model']}, using AI")

        # Build Nextlink migration context
        migration_notes = ""
        if '7.' in target_version:
            migration_notes = f"""
NEXTLINK-SPECIFIC MIGRATION NOTES (6.x → 7.x):

OSPF Changes:
- Old: {NEXTLINK_MIGRATION_6X_TO_7X['ospf']['old']}
- New: {NEXTLINK_MIGRATION_6X_TO_7X['ospf']['new']}
- Note: {NEXTLINK_MIGRATION_6X_TO_7X['ospf']['change']}

BGP Changes:
- Old: {NEXTLINK_MIGRATION_6X_TO_7X['bgp']['old']}
- New: {NEXTLINK_MIGRATION_6X_TO_7X['bgp']['new']}
- Note: {NEXTLINK_MIGRATION_6X_TO_7X['bgp']['change']}

Bridge VLAN:
- {NEXTLINK_MIGRATION_6X_TO_7X['bridge_vlan']['change']}
- MANDATORY: {NEXTLINK_MIGRATION_6X_TO_7X['bridge_vlan']['mandatory']}

Interface Naming:
- {NEXTLINK_MIGRATION_6X_TO_7X['interface_naming']['change']}
- {NEXTLINK_MIGRATION_6X_TO_7X['interface_naming']['note']}

Port Roles:
- {NEXTLINK_MIGRATION_6X_TO_7X['port_roles']['change']}
- {NEXTLINK_MIGRATION_6X_TO_7X['port_roles']['note']}
"""

        # INTELLIGENT ROUTEROS SYNTAX DETECTION AND LEARNING
        # (Functions already defined above, no need to redefine)

        def extract_loopback_ip(config_text):
            """Extract loopback IP (interface=loop0) without mask if present; fallback to router-id."""
            # Try interface=loop0 address first
            m = re.search(r"/ip address add[^\n]*address=(\d+\.\d+\.\d+\.\d+)(?:/\d+)?[^\n]*interface=loop0", config_text)
            if m:
                return m.group(1)
            # Fallback to router-id in BGP/OSPF instance
            m = re.search(r"router-id=(\d+\.\d+\.\d+\.\d+)", config_text)
            if m:
                return m.group(1)
            return None

        def validate_translation_completeness(source_text, translated_text):
            """
            Validate that critical sections aren't lost during translation.
            Returns (is_valid, missing_sections, warnings)
            """
            missing = []
            warnings = []
            
            # Critical sections that MUST be preserved
            critical_sections = [
                ('/interface bridge', 'Interface bridges'),
                ('/interface ethernet', 'Ethernet interfaces'),
                ('/interface bonding', 'Bonding configuration'),
                ('/ip address', 'IP addresses'),
                ('/ip firewall', 'Firewall rules'),
                ('/routing ospf', 'OSPF routing'),
                ('/mpls ldp', 'MPLS LDP'),
                ('/snmp', 'SNMP configuration'),
                ('/system identity', 'System identity'),
                ('/user aaa', 'User AAA'),
            ]
            
            for section_pattern, section_name in critical_sections:
                if section_pattern in source_text and section_pattern not in translated_text:
                    missing.append(section_name)
            
            # Count IP addresses in both
            source_ips = set(re.findall(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/\d{1,2}\b', source_text))
            translated_ips = set(re.findall(r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/\d{1,2}\b', translated_text))
            missing_ips = source_ips - translated_ips
            if missing_ips:
                warnings.append(f"Missing {len(missing_ips)} IP addresses: {list(missing_ips)[:5]}")
            
            # Check firewall rules count
            source_fw_rules = len(re.findall(r'(?m)^add\s+action=', source_text))
            translated_fw_rules = len(re.findall(r'(?m)^add\s+action=', translated_text))
            if translated_fw_rules < source_fw_rules * 0.8:  # Allow 20% variance for optimization
                warnings.append(f"Firewall rules dropped from {source_fw_rules} to {translated_fw_rules}")
            
            is_valid = len(missing) == 0
            return is_valid, missing, warnings
        
        def postprocess_to_v7(translated_text, target_version):
            """
            MINIMAL postprocessing - ONLY fix critical RouterOS 7.x syntax.
            DO NOT try to fix AI mistakes - let validation catch them instead.
            Focus: syntax correctness, preserve ALL content.
            """
            if not target_version.startswith('7.'):
                return translated_text

            text = translated_text
            safe_print("[POSTPROCESS] Starting MINIMAL syntax normalization (preserving all content)")

            # Unwrap RouterOS line continuations and clean spacing
            # Prefer joining WITHOUT a space when the break is in the middle of a token
            # 1) mid-token joins: non-space before and after the break
            text = re.sub(r"(?<=\S)\\\r?\n\s*(?=\S)", "", text)
            # 2) remaining continuations join with a single space
            text = re.sub(r"\\\r?\n\s*", " ", text)
            # Remove stray trailing backslashes
            text = re.sub(r"(?m)\\\s*$", "", text)
            # Remove stray trailing backslashes
            text = re.sub(r"(?m)\\\s*$", "", text)
            # ENHANCED SPACING: Ensure proper separation between major sections
            # Each major section (/interface, /ip, /routing, /mpls, /system, etc.) should have clear separation
            # Collapse excessive blank lines but ensure at least one blank line between major sections
            text = re.sub(r"\n{4,}", "\n\n\n", text)  # Max 3 blank lines between sections
            # Ensure major sections have at least one blank line before them (if not already present)
            major_sections = [
                r'^/interface ',
                r'^/ip address',
                r'^/ip firewall',
                r'^/ip route',
                r'^/ip dns',
                r'^/ip service',
                r'^/ip pool',
                r'^/routing ospf',
                r'^/routing bgp',
                r'^/routing bfd',
                r'^/mpls ',
                r'^/system ',
                r'^/user ',
                r'^/snmp ',
                r'^/radius ',
                r'^/tool '
            ]
            for section_pattern in major_sections:
                # Add blank line before section if not already present
                text = re.sub(
                    rf'(?m)([^\n])\n({section_pattern})',
                    r'\1\n\n\2',
                    text
                )

            # Remove placeholder BREAK marker lines and comments
            text = re.sub(r"(?m)^.*\bchain=break\b.*$", "", text)
            text = re.sub(r"(?m)^.*\bcomment(?:s)?=([\"'])?[^\n]*?BREAK[^\n]*\1.*$", "", text)

            # Normalize spacing around '=' and collapse excessive spaces per line
            norm_lines = []
            for raw_line in text.splitlines():
                line = re.sub(r"=\s+", "=", raw_line)
                line = re.sub(r"\s{2,}", " ", line)
                norm_lines.append(line.rstrip())
            text = "\n".join(norm_lines)

            # --- BGP normalizations ---
            # instance -> template
            if re.search(r"(?m)^/routing bgp ", text):
                text = re.sub(r"(?m)^/routing bgp instance", "/routing bgp template", text)
            # ensure 'set default' lines use template
            text = re.sub(r"(?m)^/routing bgp template\s+set\s+default(.*)$", lambda m: f"/routing bgp template set default{m.group(1)}", text)

            # parameter key migrations (RouterOS 7.x uses dot notation)
            param_map = {
                'remote-address=': 'remote.address=',
                'remote-as=': 'remote.as=',
                'tcp-md5-key=': 'tcp-md5-key=',  # CRITICAL: Keep as tcp-md5-key (not tcp.md5.key) for RouterOS 7.x
                'update-source=': 'update.source=',
                'local-address=': 'local.address='
            }
            for old, new in param_map.items():
                text = text.replace(old, new)
            
            # CRITICAL: Fix incorrect BGP parameter conversions that AI might generate
            # RouterOS 7.x uses tcp-md5-key (with hyphens), NOT tcp.md5.key
            text = re.sub(r'\btcp\.md5\.key=', 'tcp-md5-key=', text)
            # RouterOS 7.x uses output.network (singular), NOT output.networks
            text = re.sub(r'\boutput\.networks=', 'output.network=', text)
            
            # CRITICAL: Remove any duplicate BGP parameters that might have been added
            # If both tcp.md5.key and tcp-md5-key exist on same line, keep only tcp-md5-key
            text = re.sub(r'\btcp\.md5\.key=\S+\s+(?=.*tcp-md5-key=)', '', text)
            # If both output.networks and output.network exist on same line, keep only output.network
            text = re.sub(r'\boutput\.networks=\S+\s+(?=.*output\.network=)', '', text)
            
            # FIRMWARE-SPECIFIC: Handle speed format based on RouterOS version
            # RouterOS 7.11.2 and earlier use speed=10Gbps, speed=1Gbps etc
            # RouterOS 7.16+ use speed=10G-baseSR-LR, speed=1G-baseT-full etc
            version_parts = target_version.split('.')
            major = int(version_parts[0]) if len(version_parts) > 0 and version_parts[0].isdigit() else 7
            minor = int(version_parts[1]) if len(version_parts) > 1 and version_parts[1].isdigit() else 16
            
            if major == 7 and minor < 16:
                # For 7.11.2 and earlier: Convert new format to old format
                text = re.sub(r'\bspeed=10G-baseSR-LR\b', 'speed=10Gbps', text)
                text = re.sub(r'\bspeed=10G-baseCR\b', 'speed=10Gbps', text)
                text = re.sub(r'\bspeed=1G-baseT-full\b', 'speed=1Gbps', text)
                text = re.sub(r'\bspeed=100M-baseT-full\b', 'speed=100Mbps', text)
                print(f"[FIRMWARE] RouterOS {target_version} uses legacy speed format (XGbps)")
            elif major == 7 and minor >= 16:
                # For 7.16+: Convert old format to new format
                text = re.sub(r'\bspeed=10Gbps\b', 'speed=10G-baseSR-LR', text)
                text = re.sub(r'\bspeed=1Gbps\b', 'speed=1G-baseT-full', text)
                text = re.sub(r'\bspeed=100Mbps\b', 'speed=100M-baseT-full', text)
                print(f"[FIRMWARE] RouterOS {target_version} uses new speed format (XG-baseX)")


            # peer -> connection
            text = text.replace('/routing bgp peer', '/routing bgp connection')
            
            # CRITICAL: Consolidate BGP sections and remove duplicates
            lines = text.splitlines()
            cleaned_lines = []
            current_block = None
            bgp_connections_seen = set()  # Track unique BGP connections by name
            bgp_template_seen = False
            skip_next_empty_bgp_header = False
            
            for i, line in enumerate(lines):
                # Detect section headers
                if re.match(r'^/(routing|interface|ip|mpls|system|user|tool|snmp)', line):
                    block_match = re.match(r'^/([^\s]+(?:\s+[^\s]+)*)', line)
                    current_block = block_match.group(0) if block_match else (line.split()[0] if line.split() else None)
                    
                    # Skip duplicate empty BGP template headers
                    if line.strip() == '/routing bgp template' and bgp_template_seen:
                        skip_next_empty_bgp_header = True
                        print(f"[BGP CLEANUP] Skipping duplicate BGP template header at line {i}")
                        continue
                    elif line.strip() == '/routing bgp template':
                        bgp_template_seen = True
                    
                    cleaned_lines.append(line)
                    continue
                
                # Check if this is a BGP connection line
                bgp_indicators = [
                    r'\bas=\d+', r'\bremote\.address=', r'\bremote\.as=', r'\btcp-md5-key=',
                    r'\blocal\.address=', r'\brouter-id=', r'\btemplates=', r'\boutput\.network=',
                    r'\bcisco-vpls', r'\.role=ibgp', r'\.role=ebgp'
                ]
                bgp_match_count = sum(1 for pattern in bgp_indicators if re.search(pattern, line, re.IGNORECASE))
                is_bgp_connection = bgp_match_count >= 3
                
                if is_bgp_connection and current_block:
                    # Extract connection name if present
                    name_match = re.search(r'\bname=([^\s]+)', line)
                    conn_name = name_match.group(1) if name_match else None
                    
                    block_lower = current_block.lower()
                    
                    # If BGP connection is in NON-BGP section OR is a duplicate, REMOVE it
                    if 'bgp' not in block_lower:
                        print(f"[BGP CLEANUP] Removing BGP connection from non-BGP section '{current_block}': {line[:80]}...")
                        continue
                    elif conn_name and conn_name in bgp_connections_seen and 'bgp connection' in block_lower:
                        print(f"[BGP CLEANUP] Removing duplicate BGP connection '{conn_name}': {line[:80]}...")
                        continue
                    elif conn_name and 'bgp connection' in block_lower:
                        bgp_connections_seen.add(conn_name)
                
                cleaned_lines.append(line)
            
            text = '\n'.join(cleaned_lines)
            print(f"[BGP CLEANUP] Removed duplicates and consolidated BGP sections (kept {len(bgp_connections_seen)} unique connections)")

            # Filters: v6 'in-filter/out-filter' -> v7 'input.filter/output.filter'
            # Per requirement: DO NOT set any BGP input/output filter on ROS7 (avoid loops)
            # Remove any existing v6 or v7 filter parameters from bgp connection lines
            def strip_bgp_filters(m):
                line = m.group(0)
                line = re.sub(r"\s(?:in-filter|out-filter|input\.filter|output\.filter)=[^\s]+", "", line)
                line = re.sub(r"\s{2,}", " ", line).rstrip()
                return line
            if re.search(r"(?m)^/routing bgp ", text):
                text = re.sub(r"(?m)^/routing bgp connection\s+(?:add|set)\b[^\n]*$", strip_bgp_filters, text)

            # update.source should use IP, not interface name
            lb_ip = extract_loopback_ip(text)
            if lb_ip:
                text = re.sub(r"update\.source=([A-Za-z0-9_.-]+)", f"update.source={lb_ip}", text)

            # Ensure there is exactly one '/routing bgp template set default' with key IP fields
            def ensure_template_set_default(t):
                tpl_line = None
                out_lines = []
                for l in t.splitlines():
                    if re.match(r"^/routing bgp template\s+set\s+default\b", l):
                        tpl_line = l
                        continue
                    out_lines.append(l)
                # Build normalized default line
                base = "/routing bgp template set default disabled=no multihop=yes output.network=bgp-networks routing-table=main"
                if lb_ip:
                    base += f" local.address={lb_ip} router-id={lb_ip} update.source={lb_ip}"
                # Re-add at top of routing bgp template section
                out_text = "\n".join(out_lines)
                return base + "\n" + out_text if base not in out_text else out_text

            # Only ensure defaults if BGP exists in the source
            if re.search(r"(?m)^/routing bgp ", text):
                text = ensure_template_set_default(text)
                # Move template 'set default' to immediately after the last connection add
                def _reorder_bgp(t: str) -> str:
                    lines = t.splitlines()
                    tpl_positions = [i for i,l in enumerate(lines) if re.match(r"^/routing bgp template\s+set\s+default\b", l)]
                    conn_positions = [i for i,l in enumerate(lines) if re.match(r"^/routing bgp connection\s+add\b", l)]
                    if not tpl_positions or not conn_positions:
                        return t
                    tpl_line = lines[tpl_positions[0]]
                    del lines[tpl_positions[0]]
                    insert_at = conn_positions[-1] + 1
                    lines.insert(insert_at, tpl_line)
                    return "\n".join(lines)
                text = _reorder_bgp(text)

            # Ensure BGP template defaults exist and include ROS7 recommended fields
            def normalize_bgp_template(line: str, lb_ip_val: str, asn_val: str) -> str:
                if asn_val and ' as=' not in line:
                    line += f' as={asn_val}'
                if ' disabled=' not in line:
                    line += ' disabled=no'
                if ' multihop=' not in line:
                    line += ' multihop=yes'
                if ' output.network=' not in line:
                    line += ' output.network=bgp-networks'
                if lb_ip_val and ' router-id=' not in line:
                    line += f' router-id={lb_ip_val}'
                if ' routing-table=' not in line:
                    line += ' routing-table=main'
                return line

            # derive ASN from existing config if possible
            asn_match = re.search(r"(?m)\bas=(\d+)\b", text)
            asn_val = asn_match.group(1) if asn_match else ''

            if re.search(r"(?m)^/routing bgp template\s+set\s+default", text):
                text = re.sub(r"(?m)^(/routing bgp template\s+set\s+default[^\n]*)$",
                              lambda m: normalize_bgp_template(m.group(1), lb_ip, asn_val),
                              text)
            else:
                # Do not add a BGP template line if there is no BGP section at all
                pass

            # Normalize each BGP connection add line
            def normalize_bgp_connection(m):
                line = m.group(0)
                if ' output.network=' not in line:
                    line += ' output.network=bgp-networks'
                if lb_ip and ' local.address=' not in line:
                    line += f' local.address={lb_ip}'
                if lb_ip and ' router-id=' not in line:
                    line += f' router-id={lb_ip}'
                if ' routing-table=' not in line:
                    line += ' routing-table=main'
                if ' templates=' not in line:
                    line += ' templates=default'
                if ' multihop=' not in line:
                    line += ' multihop=yes'
                if ' connect=' not in line:
                    line += ' connect=yes'
                if ' listen=' not in line:
                    line += ' listen=yes'
                # Enforce /32 remote.address if missing mask
                line = re.sub(r"remote\.address=(\d+\.\d+\.\d+\.\d+)(?!/\d+)", r"remote.address=\1/32", line)
                return line

            if re.search(r"(?m)^/routing bgp ", text):
                text = re.sub(r"(?m)^/routing bgp connection\s+add\b[^\n]*$", normalize_bgp_connection, text)

            # Convert '/routing bgp network' entries into address-list 'bgp-networks' and remove them
            if re.search(r"(?m)^/routing bgp ", text):
                nets = set(re.findall(r"(?m)^/routing bgp network\s+add\s+[^\n]*?network=([^\s]+)", text))
                if nets:
                    # Remove header and add lines
                    text = re.sub(r"(?m)^/routing bgp network\s*$\n?", "", text)
                    text = re.sub(r"(?m)^/routing bgp network\s+add\b[^\n]*\n?", "", text)
                    # Ensure address-list exists for each network
                    for net in nets:
                        if not re.search(rf"(?m)^/ip firewall address-list\s+add\s+address={re.escape(net)}\s+list=bgp-networks\b", text):
                            text += f"\n/ip firewall address-list add address={net} list=bgp-networks"

            # --- OSPF normalizations ---
            # Fix accidental slash-separated hierarchy to spaced hierarchy
            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?m)^/routing/ospf/interface-template\b", "/routing ospf interface-template", text)
                text = re.sub(r"(?m)^/routing/ospf/interface\b", "/routing ospf interface-template", text)
                text = re.sub(r"(?m)^/routing ospf interface\b", "/routing ospf interface-template", text)

            # instance default to default-v2 add
            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?ms)^/routing ospf instance\s+set\s+\[\s*find\s+default=yes\s*\]\s+router-id=(\d+\.\d+\.\d+\.\d+).*?$",
                              r"/routing ospf instance add disabled=no name=default-v2 router-id=\1", text)

            # interface -> interface-template (OSPF only)
            if re.search(r"(?m)^/routing ospf ", text):
                text = text.replace('/routing ospf interface add', '/routing ospf interface-template add')

            # Prefix orphan OSPF interface-template 'add' lines without header
            # e.g., lines that start with 'add area=... interfaces=...' or 'add area=... networks=...'
            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?m)^(add\s+[^\n]*\barea=\S+[^\n]*(?:\binterfaces?=|\bnetworks=)[^\n]*)$",
                              r"/routing ospf interface-template \1", text)

            # Replace interface= to interfaces= ONLY on OSPF interface-template lines
            def ospf_iface_params(m):
                line = m.group(0)
                line = re.sub(r'\binterface=', 'interfaces=', line)
                return line
            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?m)^/routing ospf interface-template\s+add\b[^\n]*$", ospf_iface_params, text)

            # Force-convert any leftover v6-style OSPF interface lines to v7 interface-template
            if re.search(r"(?m)^/routing ospf interface\s+add\b", text):
                def v6_to_v7_iface(m):
                    ln = m.group(0).replace('/routing ospf interface add', '/routing ospf interface-template add')
                    ln = re.sub(r'\bauthentication=', 'auth=', ln)
                    ln = re.sub(r'\bauthentication-key=', 'auth-key=', ln)
                    ln = re.sub(r'\bnetwork-type=point-to-point', 'type=ptp', ln)
                    ln = re.sub(r'\binterface=', 'interfaces=', ln)
                    if ' cost=' not in ln and ' type=ptp' in ln:
                        ln += ' cost=10'
                    if ' disabled=' not in ln:
                        ln += ' disabled=no'
                    return ln
                text = re.sub(r"(?m)^/routing ospf interface\s+add\b[^\n]*$", v6_to_v7_iface, text)

            # Fix unintended pluralization outside OSPF context
            # in-interfaces/out-interfaces -> singular
            text = re.sub(r"\bin-interfaces=", "in-interface=", text)
            text = re.sub(r"\bout-interfaces=", "out-interface=", text)
            # generic interfaces= -> interface= on non-OSPF lines
            def depluralize_non_ospf(m):
                line = m.group(0)
                if line.startswith('/routing ospf interface-template'):
                    return line
                return line.replace('interfaces=', 'interface=')
            text = re.sub(r"(?m)^.*\binterfaces=[^\n]*$", depluralize_non_ospf, text)
            # parameter key migrations
            text = text.replace('authentication=', 'auth=')
            text = text.replace('authentication-key=', 'auth-key=')
            text = text.replace('network-type=point-to-point', 'type=ptp')

            # Convert old network statements to interface-template networks= form (preserve source area)
            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?m)^/routing ospf network\s+add\s+area=([^\s]+)\s+network=([^\s]+)$",
                              r"/routing ospf interface-template add area=\1 networks=\2", text)

            # Detect declared OSPF area names (use first as primary when we need a default)
            declared_areas = re.findall(r"(?m)^/routing ospf area\s+add\s+[^\n]*\bname=([^\s]+)", text)
            primary_area = declared_areas[0] if declared_areas else None

            # Normalize all interface-template lines (area, disabled, networks for loopback)
            def normalize_ospf_iface_tmpl(m):
                line = m.group(0)
                # Preserve existing area; if missing and we detected one, set it
                if ' area=' not in line and primary_area:
                    line = line.replace('add ', f'add area={primary_area} ', 1)
                if ' disabled=' not in line:
                    line += ' disabled=no'
                if 'interfaces=loop0' in line and 'networks=' not in line:
                    lb = extract_loopback_ip(text)
                    if lb:
                        line += f' networks={lb}/32 passive priority=1'
                if ' auth=md5' in line and ' auth-id=' not in line:
                    line += ' auth-id=1'
                return line

            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?m)^/routing ospf interface-template\s+add\b[^\n]*$", normalize_ospf_iface_tmpl, text)

            # Remove legacy '/routing ospf network' block remnants
            if re.search(r"(?m)^/routing ospf ", text):
                text = re.sub(r"(?m)^/routing ospf network\s*$\n?", "", text)
                text = re.sub(r"(?m)^/routing ospf network\s+add\b[^\n]*\n?", "", text)

            # Consolidate all OSPF interface-template lines under a single header block
            ospf_iface_lines = re.findall(r"(?m)^/routing ospf interface-template\s+add[^\n]*$", text)
            # Also capture orphan lines like 'add area=...' that were emitted under wrong headers (e.g. after /radius)
            orphan_iface_adds = re.findall(r"(?m)^(add\s+[^\n]*\barea=\S+[^\n]*)$", text)
            # Capture additional orphan OSPF interface lines that contain OSPF-specific tokens ONLY
            # Require at least one of: auth=, area=, networks=, passive, priority
            # Explicitly exclude VLAN/bridge/VPLS/DHCP style tokens to avoid misclassification
            orphan_iface_auth_adds = []
            for l in re.findall(r"(?m)^(add\s+[^\n]+)$", text):
                if re.search(r"\b(auth=|area=|networks=|passive\b|priority=)", l):
                    if re.search(r"\b(vlan\-id=|bridge=|horizon=|name=|add\-default\-route=|use\-peer\-(dns|ntp)=|remote\-peer=|peer=)\b", l):
                        continue
                    orphan_iface_auth_adds.append(l)
            if orphan_iface_adds:
                # Remove orphans from original locations and normalize params
                text = re.sub(r"(?m)^add\s+[^\n]*\barea=\S+[^\n]*\n?", "", text)
                def normalize_orphan_ospf(l: str) -> str:
                    ln = re.sub(r"\binterface=", "interfaces=", l)
                    # If area missing and primary_area known, set it
                    if ' area=' not in ln and primary_area:
                        ln = ln.replace('add ', f'add area={primary_area} ', 1)
                    # disabled=no if missing
                    if ' disabled=' not in ln:
                        ln += ' disabled=no'
                    # md5 -> auth-id=1 if missing
                    if ' auth=md5' in ln and ' auth-id=' not in ln:
                        ln += ' auth-id=1'
                    return f"/routing ospf interface-template {ln}"
                for l in orphan_iface_adds:
                    ospf_iface_lines.append(normalize_orphan_ospf(l))
            if orphan_iface_auth_adds:
                # Exclude lines that are clearly not OSPF (e.g., contain radius-specific tokens)
                filtered = []
                for l in orphan_iface_auth_adds:
                    if re.search(r"\b(secret=|service=)\b", l):
                        continue
                    filtered.append(l)
                if filtered:
                    # Remove them from original positions
                    for l in filtered:
                        # Escape for regex removal
                        esc = re.escape(l)
                        text = re.sub(rf"(?m)^{esc}\n?", "", text)
                    # Normalize to OSPF interface-template add lines
                    for l in filtered:
                        ln = re.sub(r"\binterface=", "interfaces=", l)
                        if ' disabled=' not in ln:
                            ln += ' disabled=no'
                        if ' interfaces=' in ln and 'interfaces=loop0' not in ln and ' type=' not in ln:
                            ln += ' type=ptp'
                        if ' type=ptp' in ln and ' cost=' not in ln:
                            ln += ' cost=10'
                        if ' auth=md5' in ln and ' auth-id=' not in ln:
                            ln += ' auth-id=1'
                        ospf_iface_lines.append(f"/routing ospf interface-template {ln}")
            if ospf_iface_lines:
                # Remove any existing headers (with or without trailing text) and scattered add lines
                text = re.sub(r"(?m)^/routing ospf interface-template\b[^\n]*\n?", "", text)
                text = re.sub(r"(?m)^/routing ospf interface-template\s+add[^\n]*\n?", "", text)

                # Clean header prefixes and normalize token order for stable de-duplication
                def normalize_ospf_add_line(line: str) -> str:
                    line = re.sub(r"^/routing ospf interface-template\s+", "", line)
                    if not line.startswith('add '):
                        line = 'add ' + line
                    # Ensure interfaces/networks normalizations already applied
                    # Normalize token ordering for consistent de-dup
                    tokens = dict(re.findall(r"(\w[\w\.-]*)=([^\s]+)", line))
                    order = ['area','interfaces','networks','auth','auth-key','auth-id','type','cost','passive','priority','disabled','comment','address']
                    parts = ['add']
                    for k in order:
                        if k in tokens:
                            parts.append(f"{k}={tokens[k]}")
                    # Append any remaining tokens deterministically
                    for k in sorted(tokens.keys()):
                        if k not in order:
                            parts.append(f"{k}={tokens[k]}")
                    return ' '.join(parts)

                cleaned_lines = [normalize_ospf_add_line(l) for l in ospf_iface_lines]

                # De-duplicate while preserving order (by normalized content)
                seen = set()
                unique_lines = []
                for l in cleaned_lines:
                    if l not in seen:
                        seen.add(l)
                        unique_lines.append(l)

                consolidated = "/routing ospf interface-template\n" + "\n".join(unique_lines) + "\n"

                # Find best insertion point: after the last OSPF area/instance line
                insert_pos = 0
                last_matches = [m.end() for m in re.finditer(r"(?m)^/routing ospf (area|instance)\b[^\n]*$", text)]
                if last_matches:
                    insert_pos = last_matches[-1]
                else:
                    # If no area/instance found, append at end
                    insert_pos = len(text)

                text = text[:insert_pos] + ("\n" if insert_pos and text[insert_pos-1] != "\n" else "") + consolidated + text[insert_pos:]
                # Collapse accidental duplicate headers
                text = re.sub(r"(?m)^(?:/routing ospf interface-template\s*\n){2,}", "/routing ospf interface-template\n", text)

                # Enhance OSPF block lines using /ip address mappings for interfaces
                # Build network -> interface map from '/ip address add' lines
                net_to_iface = {}
                for ip_line in re.findall(r"(?m)^/ip address\s+add\b[^\n]*$", text):
                    m_net = re.search(r"\bnetwork=(\d+\.\d+\.\d+\.\d+/(?:\d+))", ip_line)
                    m_if = re.search(r"\binterface=([^\s]+)", ip_line)
                    if m_net and m_if:
                        net_to_iface[m_net.group(1)] = m_if.group(1)

                def process_ospf_block(match):
                    body = match.group(1)
                    seen_lines: set[str] = set()
                    out: list[str] = []

                    # Helper: final whitelist of OSPF params
                    def strip_non_ospf_params(s: str) -> str:
                        allowed = [
                            'area', 'interfaces', 'networks', 'auth', 'auth-key', 'auth-id',
                            'type', 'cost', 'passive', 'priority', 'disabled', 'comment', 'address'
                        ]
                        def repl(m):
                            key = m.group(1)
                            return '' if key not in allowed else m.group(0)
                        s = re.sub(r"\s(\w[\w\.-]*)=\S+", repl, s)
                        s = re.sub(r"\s{2,}", " ", s).rstrip()
                        return s

                    for ln in body.splitlines():
                        if not ln.strip():
                            continue
                        # Hard exclude non-OSPF content that may have slipped in
                        if re.search(r"\b(vlan\-id=|\bname=vlan|\bbridge=|\bhorizon=|\badd\-default\-route=|\buse\-peer\-(dns|ntp)=|\b(remote\.)?peer=|\bvpls|\bmac\-address=|\bmtu=|\bpw\-)", ln):
                            continue
                        # force interfaces= using network mapping if missing
                        if (' networks=' in ln or ' network=' in ln) and ' interfaces=' not in ln:
                            mnet = re.search(r"networks=(\d+\.\d+\.\d+\.\d+/(?:\d+))|network=(\d+\.\d+\.\d+\.\d+/(?:\d+))", ln)
                            if mnet:
                                net_val = mnet.group(1) or mnet.group(2)
                                iface = net_to_iface.get(net_val)
                                if iface:
                                    ln += f" interfaces={iface}"
                        # singular -> plural on OSPF lines
                        ln = re.sub(r"\binterface=", "interfaces=", ln)
                        # add type/cost defaults
                        if ' interfaces=' in ln and 'interfaces=loop0' not in ln and ' type=' not in ln:
                            ln += ' type=ptp'
                        if ' type=ptp' in ln and ' cost=' not in ln:
                            ln += ' cost=10'
                        if ' disabled=' not in ln:
                            ln += ' disabled=no'
                        # Whitelist
                        ln = strip_non_ospf_params(ln)
                        # de-dup exact lines
                        if ln and ln not in seen_lines:
                            seen_lines.add(ln)
                            out.append(ln)
                    return "/routing ospf interface-template\n" + "\n".join(out) + "\n"

                # Normalize ' network=' to ' networks=' within OSPF block lines
                text = re.sub(r"(?m)^/routing ospf interface-template\s+add\b[^\n]*\bnetwork=", lambda m: m.group(0).replace(' network=', ' networks='), text)

                text = re.sub(r"(?ms)^/routing ospf interface-template\s*\n((?:add[^\n]*\n)+)", process_ospf_block, text)

            # DNS vs address-lists: move any '/ip dns add address=' lines into firewall address-lists
            # Example wrong: /ip dns add address=1.2.3.4 list=SNMP
            # Correct:       /ip firewall address-list add address=1.2.3.4 list=SNMP
            def move_dns_address_adds(m):
                body = m.group(1)
                # keep only address and list keys
                addr = re.search(r"address=([^\s]+)", body)
                lst = re.search(r"list=([^\s]+)", body)
                if not addr or not lst:
                    return ''
                return f"/ip firewall address-list add address={addr.group(1)} list={lst.group(1)}"
            text = re.sub(r"(?m)^/ip dns\s+add\s+([^\n]+)$", move_dns_address_adds, text)

            # Consolidate RADIUS lines to live only under '/radius'
            # Capture original positions before removal for stable reinsertion
            radius_positions = [m.start() for m in re.finditer(r"(?m)^/radius\b.*$", text)]
            radius_adds = re.findall(r"(?m)^/radius\s+add\b[^\n]*$", text)
            orphan_radius_adds = re.findall(r"(?m)^(add\s+[^\n]*\baddress=\d+\.\d+\.\d+\.\d+[^\n]*(?:\bsecret=|\bservice=)[^\n]*)$", text)
            if radius_adds or orphan_radius_adds:
                # Remove existing header-only and add lines, plus orphan radius-style adds
                text = re.sub(r"(?m)^/radius\s*$\n?", "", text)
                text = re.sub(r"(?m)^/radius\s+add\b[^\n]*\n?", "", text)
                text = re.sub(r"(?m)^(add\s+[^\n]*\baddress=\d+\.\d+\.\d+\.\d+[^\n]*(?:\bsecret=|\bservice=)[^\n]*)\n?", "", text)
                # Remove any leftover header-only lines created by the removals
                text = re.sub(r"(?m)^/radius\s*$\n?", "", text)

                # De-duplicate and build radius block
                seen_r = set()
                merged = []
                for l in radius_adds + [f"/radius {l}" if not l.startswith('/radius') else l for l in orphan_radius_adds]:
                    # Normalize orphan to '/radius add ...'
                    if l.startswith('/radius add'):
                        norm = l
                    else:
                        norm = l.replace('/radius ', '/radius add ', 1) if l.startswith('/radius ') else f"/radius {l}"
                    norm = norm.replace('/radius add add ', '/radius add ')
                    # Strictly keep only valid RADIUS fields; exclude any OSPF-like tokens accidentally captured
                    if not re.search(r"\baddress=\d+\.\d+\.\d+\.\d+", norm):
                        continue
                    if not re.search(r"\b(secret=|service=)", norm):
                        continue
                    if re.search(r"\b(auth=|auth-key=|interfaces?=|area=|networks?=|type=ptp|cost=)", norm):
                        continue
                    if norm not in seen_r:
                        seen_r.add(norm)
                        merged.append(norm)
                if merged:
                    # Emit only 'add ...' lines under a single header
                    lines_only = [re.sub(r"^/radius\s+", "", x) for x in merged]
                    radius_block = "/radius\n" + "\n".join(lines_only) + "\n"
                    # Reinsertion point: earliest prior '/radius' position if any, else before first '/user ' or at top
                    if radius_positions:
                        ins = radius_positions[0]
                    else:
                        u = re.search(r"(?m)^/user\b", text)
                        ins = u.start() if u else 0
                    text = text[:ins] + radius_block + text[ins:]
                    # Collapse duplicate consecutive '/radius' headers
                    text = re.sub(r"(?m)^(?:/radius\s*\n){2,}", "/radius\n", text)
                    # De-duplicate identical radius add lines
                    def dedup_radius_block(m):
                        body = m.group(1)
                        seen = set()
                        out = []
                        for ln in body.splitlines():
                            if not ln.strip():
                                continue
                            if ln not in seen:
                                seen.add(ln)
                                out.append(ln)
                        return "/radius\n" + "\n".join(out) + "\n"
                    text = re.sub(r"(?ms)^/radius\s*\n((?:add[^\n]*\n)+)", dedup_radius_block, text)

            # --- VPLS normalizations (dynamic) ---
            # Rehome orphan VPLS 'add' lines missing the header
            text = re.sub(r"(?m)^(add\s+[^\n]*(?:cisco\-static\-id|cisco\-style\-id|remote\-peer|peer)=[^\n]*)$",
                          r"/interface vpls \1", text)
            def normalize_vpls_line(m):
                src = m.group(0)
                body = src.split(' ', 3)[-1]  # after '/interface vpls add'
                kv = dict(re.findall(r"([A-Za-z0-9_.\-]+)=([^\s]+)", body))
                # Derive identifiers
                peer = kv.get('peer') or kv.get('remote-peer') or ''
                name = kv.get('name', '')
                mac = kv.get('mac-address', '')
                disabled = kv.get('disabled', 'no')
                # static id from cisco-style-id or cisco-static-id or from name
                static_id_val = kv.get('cisco-style-id') or kv.get('cisco-static-id')
                if static_id_val is None:
                    m_name = re.search(r"vpls(\d+)", name or '')
                    static_id_val = m_name.group(1) if m_name else ''
                # bridge thousand grouping
                bridge_part = ''
                if static_id_val and static_id_val.isdigit():
                    base = (int(static_id_val) // 1000) * 1000
                    if base > 0:
                        bridge_part = f"bridge=bridge{base} "
                # pw-l2mtu from existing values
                pw_l2mtu = kv.get('pw-l2mtu')
                if not pw_l2mtu:
                    m_l2 = re.search(r"\b(advertised-)?l2mtu=(\d+)", body)
                    pw_l2mtu = m_l2.group(2) if m_l2 else '1580'
                mtu = kv.get('mtu', '1500')
                # Build canonical line
                parts = [
                    '/interface vpls add',
                    'arp=enabled',
                    bridge_part + 'bridge-horizon=1' if bridge_part else 'bridge-horizon=1',
                ]
                if static_id_val:
                    parts.append(f'cisco-static-id={static_id_val}')
                parts.append(f'disabled={disabled}')
                if mac:
                    parts.append(f'mac-address={mac}')
                parts.append(f'mtu={mtu}')
                if name:
                    parts.append(f'name={name}')
                if peer:
                    parts.append(f'peer={peer}')
                parts.append('pw-control-word=disabled')
                parts.append(f'pw-l2mtu={pw_l2mtu}')
                parts.append('pw-type=raw-ethernet')
                return ' '.join(parts).strip()

            # Remove ROS6-style tokens before rebuilding
            text = re.sub(r"\s\badvertised-l2mtu=\S+", "", text)
            text = re.sub(r"\s\bl2mtu=\S+", "", text)
            text = re.sub(r"\s\bcisco-style=yes\b", "", text)
            text = re.sub(r"\bcisco-style-id=(\d+)", r"cisco-static-id=\1", text)
            text = re.sub(r"(?m)^/interface vpls\s+add\b[^\n]*$", normalize_vpls_line, text)

            # --- LDP instance (for MPLS/VPLS) ---
            # If MPLS/VPLS is present, ensure an LDP instance uses the loopback/router-id
            if re.search(r"(?m)^/interface vpls\b|^/mpls\b", text) and lb_ip:
                def normalize_ldp_instance(m):
                    l = m.group(0)
                    if ' lsr-id=' not in l:
                        l += f' lsr-id={lb_ip}'
                    else:
                        l = re.sub(r"lsr-id=\S+", f"lsr-id={lb_ip}", l)
                    if ' transport-addresses=' not in l:
                        l += f' transport-addresses={lb_ip}'
                    else:
                        l = re.sub(r"transport-addresses=\S+", f"transport-addresses={lb_ip}", l)
                    if ' vrf=' not in l:
                        l += ' vrf=main'
                    if ' afi=' not in l:
                        l += ' afi=ip'
                    return l

                if re.search(r"(?m)^/mpls ldp instance\s+(?:add|set)\b", text):
                    text = re.sub(r"(?m)^/mpls ldp instance\s+(?:add|set)\b[^\n]*$", normalize_ldp_instance, text)
                else:
                    text += f"\n/mpls ldp instance add lsr-id={lb_ip} transport-addresses={lb_ip} vrf=main afi=ip"

            # --- Generic block consolidation & formatting ---
            # Goal: avoid scattered one-liners; group add-lines under a single header for safe sections
            # Normalize terminal-prompt artifacts from pasted CLI transcripts
            # 1) Strip leading router prompts like: [admin@RTR-XYZ] 
            text = re.sub(r"(?m)^\s*\[[^\]]+\]\s*", "", text)
            # 2) Convert '/path/subpath> add ...' → '/path subpath add ...'
            def fix_cli_path(m):
                p1 = m.group(1)
                p2 = m.group(2)
                rest = m.group(3)
                return f"/{p1} {p2} add {rest}".strip()
            text = re.sub(r"(?mi)^\s*/([a-z0-9\-]+)/([a-z0-9\-]+)>\s*add\s+(.*)$", fix_cli_path, text)
            # 3) Remove stray '>' characters that sometimes trail the header
            text = re.sub(r"(?m)^(/{1}[A-Za-z0-9\-]+(?:\s+[A-Za-z0-9\-]+)?)>\s*$", r"\1", text)

            # 4) Harden RADIUS: keep only real RADIUS server lines; re-home everything else
            def fix_radius_line(m):
                line = m.group(0)
                body = line.split(' ', 2)[-1]
                has_addr = 'address=' in body
                has_secret = 'secret=' in body
                has_service = 'service=' in body
                # Valid RADIUS server definition requires address and secret, service optional
                if has_addr and has_secret:
                    return line
                # Otherwise strip the '/radius ' prefix so orphan routing can re-home it
                return re.sub(r"^/radius\s+", "", line)

            text = re.sub(r"(?m)^/radius\s+add\b[^\n]*$", fix_radius_line, text)
            # Normalize BGP remote.address to strip CIDR if present (ROS7 expects pure IP)
            text = re.sub(r"(?m)(remote\.address=)(\d+\.\d+\.\d+\.\d+)/(?:3[0-2]|[12]?\d)", r"\1\2", text)
            # Re-home orphan lines missing their headers so we always have proper block headers
            # Fix bare 'add' commands without section headers
            def fix_bare_add_commands(text):
                """Fix bare 'add' commands that lost their section headers"""
                lines = text.split('\n')
                result = []
                current_section = None
                
                for i, line in enumerate(lines):
                    stripped = line.strip()
                    
                    # Check if this is a section header
                    if stripped.startswith('/'):
                        current_section = stripped.split()[0] if stripped.split() else stripped
                        result.append(line)
                    # Check if this is a bare 'add' command without a section
                    elif stripped.startswith('add ') and current_section is None:
                        # Try to infer section from the line content
                        if 'address=' in stripped and 'interface=' in stripped:
                            current_section = '/ip address'
                            result.append('/ip address')
                            result.append(line)
                        elif 'chain=' in stripped or 'action=' in stripped:
                            if 'dst-port=' in stripped or 'src-port=' in stripped or 'protocol=' in stripped:
                                current_section = '/ip firewall filter'
                                result.append('/ip firewall filter')
                                result.append(line)
                            elif 'port=' in stripped and 'protocol=udp' in stripped:
                                current_section = '/ip firewall raw'
                                result.append('/ip firewall raw')
                                result.append(line)
                            else:
                                current_section = '/ip firewall filter'
                                result.append('/ip firewall filter')
                                result.append(line)
                        elif 'list=' in stripped and 'address=' in stripped:
                            current_section = '/ip firewall address-list'
                            result.append('/ip firewall address-list')
                            result.append(line)
                        else:
                            # Unknown - keep as is but log warning
                            result.append(line)
                    else:
                        result.append(line)
                
                return '\n'.join(result)
            
            text = fix_bare_add_commands(text)
            # NAT orphans → '/ip firewall nat add ...'
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*\b(chain=srcnat|chain=dstnat|action=(src\-nat|dst\-nat))\b[^\n]*)$",
                          r"/ip firewall nat \1", text)
            # Address-list orphans → '/ip firewall address-list add ...' (address and list tokens only, ignore src-address/dst-address/dst-address-list)
            # Require standalone tokens preceded by start or whitespace to avoid matching 'src-address=' or 'dst-address='
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*(?:(?:(?:^|\s)list=\S+)\s+[^\n]*(?:(?:^|\s)address=\S+)|(?:(?:^|\s)address=\S+)\s+[^\n]*(?:(?:^|\s)list=\S+))[^\n]*)$",
                          r"/ip firewall address-list \1", text)
            # DHCP network orphans → '/ip dhcp-server network add ...'
            # Recognize typical network lines with address + dns-server(s) + gateway (+ optional netmask)
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*\baddress=\S+[^\n]*\b(dns-server|dns-servers)=\S+[^\n]*\bgateway=\S+[^\n]*(?:\bnetmask=\S+)?[^\n]*)$",
                          r"/ip dhcp-server network \1", text)
            # Filter-rule orphans → '/ip firewall filter add ...'
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*\bchain=(input|forward|output)\b[^\n]*\baction=\S+[^\n]*)$",
                          r"/ip firewall filter \1", text)
            # Mangle orphans → '/ip firewall mangle add ...' (only when clear mangle actions/marks are present)
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*((?=([^\n]*\baction=(mark\-connection|mark\-packet|change\-dscp|jump)\b))|([^\n]*\bnew\-connection\-mark=)|([^\n]*\bnew\-packet\-mark=))[^\n]*)$",
                          r"/ip firewall mangle \1", text)
            # Queue tree orphans → '/queue tree add ...' (name + parent, but not firewall 'chain=')
            text = re.sub(r"(?m)^\s*(add\s+(?=.*\bname=)(?=.*\bparent=)(?!.*\bchain=)[^\n]*)$",
                          r"/queue tree \1", text)
            # Bridge port orphans → '/interface bridge port add ...'
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*\bbridge=\S+\s+[^\n]*\binterface=\S+[^\n]*)$",
                          r"/interface bridge port \1", text)
            def consolidate_block(t: str, header: str) -> str:
                # Collect all '/header add ...' lines
                pattern_add = rf"(?m)^" + re.escape(header) + r"\s+add\b[^\n]*$"
                add_lines = re.findall(pattern_add, t)
                if not add_lines:
                    return t
                # Remove existing header-only lines and add-lines for this header
                t = re.sub(rf"(?m)^" + re.escape(header) + r"\s*$\n?", "", t)
                t = re.sub(pattern_add + "\n?", "", t)
                # Normalize collected lines to 'add ...' (strip header prefix)
                cleaned = [re.sub(rf"^" + re.escape(header) + r"\s+", "", ln) for ln in add_lines]
                # De-duplicate while preserving order
                seen = set()
                unique = []
                for ln in cleaned:
                    if ln not in seen:
                        seen.add(ln)
                        unique.append(ln)
                block = header + "\n" + "\n".join(unique) + "\n"
                # Insert block near the first occurrence of any sibling header, else append
                insert_pos = 0
                m = re.search(rf"(?m)^{re.escape(header.split()[0])}\\b", t)
                if m:
                    insert_pos = m.start()
                else:
                    insert_pos = len(t)
                # Ensure a blank line before block if needed
                prefix = "\n" if insert_pos and t[insert_pos-1] != "\n" else ""
                t = t[:insert_pos] + prefix + block + t[insert_pos:]
                return t

            # Safe sections to consolidate
            safe_headers = [
                '/interface vpls',
                '/routing bgp connection',
                '/routing filter',
                '/interface bridge port',
                '/ip firewall address-list',
                '/ip firewall filter',
                '/ip firewall nat',
                '/ip firewall mangle',
                '/ip firewall raw',
                '/ip dhcp-server network',
                '/queue tree'
            ]
            for hdr in safe_headers:
                text = consolidate_block(text, hdr)

            # Reassemble safe blocks in canonical order for consistent arrangement
            def extract_block(t: str, header: str) -> tuple[str, str]:
                m = re.search(rf"(?ms)^" + re.escape(header) + r"\s*\n(?:add[^\n]*\n)+", t)
                if not m:
                    return t, ''
                block = m.group(0)
                t = t.replace(block, '')
                return t, block.strip() + "\n"

            ordered_blocks = []
            remainder = text
            # Move '/routing bgp template set default ...' to live after the BGP connection block
            bgp_tmpl_lines = re.findall(r"(?m)^/routing bgp template\s+(?:set|add)\b[^\n]*$", remainder)
            if bgp_tmpl_lines:
                # strip from remainder to avoid appearing at top
                remainder = re.sub(r"(?m)^/routing bgp template\s+(?:set|add)\b[^\n]*\n?", "", remainder)
            for hdr in safe_headers:
                remainder, blk = extract_block(remainder, hdr)
                if blk:
                    ordered_blocks.append(blk.strip())

            # If we have bgp template lines, append them immediately after the BGP connection block
            if bgp_tmpl_lines:
                bgp_block_idx = next((i for i, b in enumerate(ordered_blocks) if b.startswith('/routing bgp connection\n')), None)
                tmpl_block = '/routing bgp template\n' + '\n'.join(sorted(set([re.sub(r"\\s+"," ",l).strip() for l in bgp_tmpl_lines]))) + '\n'
                if bgp_block_idx is not None:
                    ordered_blocks.insert(bgp_block_idx + 1, tmpl_block.strip())
                else:
                    # If no connection block exists, place near other routing blocks or at the end
                    ordered_blocks.append(tmpl_block.strip())

            # Ensure clean separation: single blank line between top-level headers for remainder
            lines = remainder.splitlines()
            out = []
            prev_was_header = False
            def is_header(l: str) -> bool:
                return l.startswith('/') and ' ' not in l.strip()
            for l in lines:
                if is_header(l):
                    if out and out[-1] != '':
                        out.append('')
                    out.append(l)
                    prev_was_header = True
                else:
                    out.append(l)
                    prev_was_header = False
            # Collapse multiple blank lines
            final = []
            blank = 0
            for l in out:
                if l.strip() == '':
                    blank += 1
                    if blank <= 2:
                        final.append('')
                else:
                    blank = 0
                    final.append(l)
            remainder_clean = "\n".join(final).strip()
            # Put remainder first, then ordered canonical blocks
            pieces = [p for p in [remainder_clean] + ordered_blocks if p]
            text = ("\n\n".join(pieces)).strip() + "\n"

            # Final safety: any remaining bare address-list lines must be fully-qualified
            # Convert stray 'add ... list=... address=...' or 'add ... address=... list=...' to '/ip firewall address-list add ...'
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*(?:(?:(?:^|\s)list=\S+)\s+[^\n]*(?:(?:^|\s)address=\S+)|(?:(?:^|\s)address=\S+)\s+[^\n]*(?:(?:^|\s)list=\S+))[^\n]*)$",
                          r"/ip firewall address-list \1", text)
            text = re.sub(r"(?m)^\s*(add\s+[^\n]*\baddress=\S+[^\n]*\blist=\S+[^\n]*)$",
                          r"/ip firewall address-list \1", text)

            return text

        # STANDALONE INTERFACE MAPPING FUNCTION (used by both AI and intelligent translation paths)
        def map_interfaces_dynamically(text: str, source_ports: list, target_ports: list, mgmt_port: str, target_type: str) -> str:
            """
            DYNAMIC interface mapping that handles device migrations (e.g., CCR1072 → CCR2216).
            
            Philosophy: PRESERVE source structure, DON'T enforce hardcoded port layouts.
            - Each site is unique (some have bonding, some don't; ports vary)
            - Source config IS the authority on structure
            - Tool updates syntax + interface names, NOT port assignments
            
            Key Behaviors:
            1. If source and target use same port format (both sfp28-) → PRESERVE interface numbers exactly
            2. If hardware changes (sfp-sfpplus → sfp28) → Map intelligently based on comments/purpose
            3. If bonding exists in source → Preserve bonding structure with updated port names
            4. If NO bonding in source → Do NOT add bonding
            
            CRITICAL: This is SYNTAX translation, not infrastructure redesign.
            """
            # STEP 0: Determine if mapping is needed based on device port differences
            source_has_sfp28 = any('sfp28-' in p for p in source_ports) or bool(re.search(r'\bsfp28-\d+', text))
            target_has_sfp28 = any('sfp28-' in p for p in target_ports)
            source_has_sfp_sfpplus = any('sfp-sfpplus' in p for p in source_ports) or bool(re.search(r'\bsfp-sfpplus\d+', text))
            target_has_sfp_sfpplus = any('sfp-sfpplus' in p for p in target_ports)
            source_has_ethernet = any(p.startswith('ether') for p in source_ports) or bool(re.search(r'\bether\d+', text))
            target_has_ethernet = any(p.startswith('ether') for p in target_ports)

            # Normalize legacy combo port naming (RB1009 exports often use combo1)
            if re.search(r'\bcombo\d+\b', text):
                if target_ports and any(p.startswith('sfp-sfpplus') for p in target_ports):
                    text = re.sub(r'\bcombo1\b', 'sfp-sfpplus1', text)
                elif target_ports and any(p.startswith('sfp28-') for p in target_ports):
                    text = re.sub(r'\bcombo1\b', 'sfp28-1', text)
                elif target_ports and any(p.startswith('sfp') and not p.startswith('sfp-sfpplus') for p in target_ports):
                    text = re.sub(r'\bcombo1\b', 'sfp1', text)
            
            source_ether_count = len([p for p in source_ports if p.startswith('ether')]) if source_ports else 0
            target_ether_count = len([p for p in target_ports if p.startswith('ether')]) if target_ports else 0
            
            # CRITICAL: Determine if mapping is REQUIRED
            mapping_required = False
            
            # Case 1: Source has ethernet, target has SFP (different port types) - MUST map
            if source_has_ethernet and target_has_sfp_sfpplus and not target_has_ethernet:
                print(f"[INTERFACE MAPPING] Source has ethernet, target has sfp-sfpplus only - mapping REQUIRED")
                mapping_required = True
            # Case 2: Source has more ethernet ports than target - MUST map excess to SFP
            elif source_has_ethernet and target_has_ethernet and source_ether_count > target_ether_count:
                print(f"[INTERFACE MAPPING] Source has {source_ether_count} ethernet ports, target has {target_ether_count} - mapping REQUIRED")
                mapping_required = True
            # Case 3: Both use same port format - no mapping needed (early exit)
            elif source_has_sfp28 and target_has_sfp28:
                # Only preserve sfp28 numbering if the target actually supports all referenced sfp28 indices.
                # Example: CCR2216 (sfp28-1..12) -> CCR2004 (sfp28-1..2 + sfp-sfpplus1..12) MUST map, even though both "have sfp28".
                source_sfp28_idxs = set(int(n) for n in re.findall(r'\bsfp28-(\d+)\b', text))
                target_sfp28_idxs = set(int(re.search(r'(\d+)', p).group(1)) for p in target_ports if p.startswith('sfp28-') and re.search(r'(\d+)', p))
                if source_sfp28_idxs and target_sfp28_idxs and source_sfp28_idxs.issubset(target_sfp28_idxs):
                    print(f"[INTERFACE MAPPING] Source and target both use sfp28- ports and indices match - preserving interface numbers")
                    if source_has_sfp_sfpplus and not target_has_sfp_sfpplus:
                        for i in range(1, 13):
                            text = re.sub(rf"\bsfp-sfpplus{i}\b", f"sfp28-{i}", text)
                    return text
                print(f"[INTERFACE MAPPING] Target sfp28 indices do not cover source usage - mapping REQUIRED")
                mapping_required = True
            elif source_has_sfp_sfpplus and target_has_sfp_sfpplus and not source_has_sfp28 and not target_has_sfp28:
                print(f"[INTERFACE MAPPING] Source and target both use sfp-sfpplus ports - preserving interface numbers")
                return text
            elif source_has_ethernet and target_has_ethernet and source_ether_count == target_ether_count:
                # Same ethernet count, both have ethernet - check if port types match
                if not source_has_sfp28 and not target_has_sfp28 and not source_has_sfp_sfpplus and not target_has_sfp_sfpplus:
                    print(f"[INTERFACE MAPPING] Source and target have same ethernet ports - no mapping needed")
                    return text
            
            # If mapping is not required and we haven't returned, continue to check if interfaces need updating
            if not mapping_required:
                # Check if any source interfaces don't exist in target
                source_interfaces_in_text = set(re.findall(r'\b(ether\d+|sfp\d+|sfp-sfpplus\d+|sfp28-\d+|qsfp28-\d+-\d+|qsfpplus\d+-\d+|combo\d+)\b', text))
                target_interface_set = set(target_ports) if target_ports else set()
                if source_interfaces_in_text and target_interface_set:
                    interfaces_need_mapping = source_interfaces_in_text - target_interface_set
                    if interfaces_need_mapping:
                        print(f"[INTERFACE MAPPING] Found {len(interfaces_need_mapping)} interfaces that need mapping: {list(interfaces_need_mapping)[:5]}")
                        mapping_required = True

            # Special-case: CCR2004 <-> CCR2216 (12x sfp-sfpplus1..12 <-> 12x sfp28-1..12).
            # For Upgrade Existing we prefer stable, index-based mapping (no reshuffling).
            if mapping_required:
                src_sfp_sfpplus = [p for p in (source_ports or []) if p.startswith('sfp-sfpplus')]
                tgt_sfp_sfpplus = [p for p in (target_ports or []) if p.startswith('sfp-sfpplus')]
                src_sfp28 = [p for p in (source_ports or []) if p.startswith('sfp28-')]
                tgt_sfp28 = [p for p in (target_ports or []) if p.startswith('sfp28-')]

                def _apply_mapping_everywhere(t: str, mapping: dict) -> str:
                    # 1) Replace standalone interface tokens (safe: avoids qsfp28-1-1 and sfp28-10 collisions)
                    for src in sorted(mapping.keys(), key=len, reverse=True):
                        dst = mapping[src]
                        t = re.sub(rf"\b{re.escape(src)}\b", dst, t)
                    # 2) Replace embedded port tokens in common vlan interface naming patterns (e.g., vlan1000sfp-sfpplus1)
                    for src in sorted(mapping.keys(), key=len, reverse=True):
                        dst = mapping[src]
                        t = re.sub(rf"(?m)\b(vlan\d+[-_]?)" + re.escape(src) + r"(?!\d)\b", rf"\1{dst}", t)
                    return t

                stable_mapping = None
                if len(src_sfp_sfpplus) >= 12 and len(tgt_sfp28) >= 12:
                    stable_mapping = {f"sfp-sfpplus{i}": f"sfp28-{i}" for i in range(1, 13)}
                    print("[INTERFACE MAPPING] Using stable CCR2004->CCR2216 index mapping (sfp-sfpplusN -> sfp28-N)")
                elif len(src_sfp28) >= 12 and len(tgt_sfp_sfpplus) >= 12:
                    stable_mapping = {f"sfp28-{i}": f"sfp-sfpplus{i}" for i in range(1, 13)}
                    print("[INTERFACE MAPPING] Using stable CCR2216->CCR2004 index mapping (sfp28-N -> sfp-sfpplusN)")

                if stable_mapping:
                    text = _apply_mapping_everywhere(text, stable_mapping)
                    # Strip qsfp* interface lines if target has no QSFP ports (prevents invalid ports after migration).
                    target_has_qsfp = any(p.startswith('qsfp') for p in (target_ports or []))
                    if not target_has_qsfp:
                        text = re.sub(r"(?m)^\s*set\s+\[\s*find\s+default-name=qsfp[^\]]+\][^\n]*\n?", "", text)
                    return text
             
            # STEP 1: Extract interface comments to detect purpose
            interface_info = {}  # {interface_name: {'comment': '', 'purpose': 'unknown'}}
            
            # Extract comments from /interface ethernet set lines
            ethernet_pattern = r'/interface ethernet\s+set\s+\[[^\]]*default-name=([^\]]+)\][^\n]*comment=([^\s\n"]+|"[^"]+")'
            for m in re.finditer(ethernet_pattern, text, re.MULTILINE | re.DOTALL):
                iface = m.group(1).strip()
                comment = m.group(2).strip().strip('"')
                if iface not in interface_info:
                    interface_info[iface] = {'comment': comment, 'purpose': 'unknown'}
            
            # Extract comments from /ip address lines (often have interface purpose in comment)
            ip_pattern = r'/ip address\s+add\s+[^\n]*interface=([^\s\n]+)[^\n]*comment=([^\s\n"]+|"[^"]+")'
            for m in re.finditer(ip_pattern, text, re.MULTILINE):
                iface = m.group(1).strip()
                comment = m.group(2).strip().strip('"')
                if iface not in interface_info:
                    interface_info[iface] = {'comment': comment, 'purpose': 'unknown'}
                elif not interface_info[iface]['comment']:
                    interface_info[iface]['comment'] = comment
            
            # STEP 2: Classify interface purpose from comments
            def classify_interface_purpose(comment, iface_name):
                """Classify interface purpose based on comment and context"""
                comment_upper = comment.upper()
                iface_upper = iface_name.upper()
                
                # Backhaul detection (TX-*, site names, BH, BACKHAUL)
                if any(keyword in comment_upper for keyword in ['BACKHAUL', 'BH', 'TX-', 'KS-', 'IL-', 'TOWER']):
                    # Check if it's a site name pattern (TX-SITE-NO-1, KS-SITE-CN-1, etc.)
                    if re.search(r'[A-Z]{2}-[A-Z]+', comment_upper):
                        return 'backhaul'
                    if 'BACKHAUL' in comment_upper or 'BH' in comment_upper:
                        return 'backhaul'
                
                # OLT detection
                if 'OLT' in comment_upper or 'NOKIA' in comment_upper:
                    return 'olt'
                
                # Switch detection
                if any(keyword in comment_upper for keyword in ['SWITCH', 'SW', 'NETONIX']):
                    return 'switch'

                # Power/UPS detection
                if any(keyword in comment_upper for keyword in ['UPS', 'ICT', 'POWER', 'WPS']):
                    return 'power'
                
                # LTE detection
                if 'LTE' in comment_upper:
                    return 'lte'
                
                # Tarana detection
                if 'TARANA' in comment_upper:
                    return 'tarana'
                
                # Management
                if 'MANAGEMENT' in comment_upper or 'MGMT' in comment_upper or iface_upper == 'ETHER1':
                    return 'management'
                
                # Default: unknown (will be mapped sequentially)
                return 'unknown'
            
            # Classify all interfaces
            for iface, info in interface_info.items():
                info['purpose'] = classify_interface_purpose(info['comment'], iface)
                print(f"[INTERFACE CLASSIFY] {iface}: '{info['comment']}' → {info['purpose']}")
            
            # Gather ALL used interface tokens in order of appearance (more comprehensive pattern)
            used = []
            # Match all interface patterns: etherN, sfpN, sfp-sfpplusN, sfp28-N, etc.
            interface_pattern = r"\b(ether\d+|sfp\d+(?:-\d+)?|sfp-sfpplus\d+|sfp28-\d+|qsfp28-\d+-\d+|qsfpplus\d+-\d+|qsfp\d+(?:-\d+)?|combo\d+)\b"
            for m in re.finditer(interface_pattern, text):
                name = m.group(1)
                if name not in used:
                    used.append(name)
            
            print(f"[INTERFACE MAPPING] Found {len(used)} unique interfaces in config: {', '.join(used[:10])}{'...' if len(used) > 10 else ''}")
            
            # Prepare target port sequence excluding management if present
            target_seq = [p for p in target_ports if p != mgmt_port]
            # Prefer SFP28 on CCR2216 (no qsfp28 usage)
            target_seq = [p for p in target_seq if not p.startswith('qsfp28')]
            
            # STEP 3: Build policy-compliant port assignments for CCR2216 (sfp28- ports)
            # Policy: sfp28-1-2=switches, sfp28-4+=backhauls, sfp28-6=LTE, sfp28-6-8=Tarana, sfp28-9+=additional backhauls
            policy_ports = {
                'switch': [],
                'olt': [],
                'backhaul': [],
                'lte': [],
                'tarana': [],
                'unknown': []
            }
            
            # Extract all SFP-type ports from target_seq (sfp28, sfp-sfpplus, sfp, etc.)
            sfp28_ports = sorted([p for p in target_seq if p.startswith('sfp28-')], 
                                key=lambda x: int(re.search(r'(\d+)', x).group(1)) if re.search(r'(\d+)', x) else 999)
            sfp_sfpplus_ports = sorted([p for p in target_seq if p.startswith('sfp-sfpplus')], 
                                       key=lambda x: int(re.search(r'(\d+)', x).group(1)) if re.search(r'(\d+)', x) else 999)
            sfp_ports = sorted([p for p in target_seq if p.startswith('sfp') and not p.startswith('sfp28-') and not p.startswith('sfp-sfpplus')], 
                              key=lambda x: int(re.search(r'(\d+)', x).group(1)) if re.search(r'(\d+)', x) else 999)
            
            # Get all available SFP-type ports (prioritize sfp-sfpplus for CCR2004, sfp28 for CCR2216, etc.)
            available_sfp_ports = []
            if sfp_sfpplus_ports:
                available_sfp_ports = sfp_sfpplus_ports  # CCR2004 uses sfp-sfpplus
            elif sfp28_ports:
                available_sfp_ports = sfp28_ports  # CCR2216 uses sfp28
            elif sfp_ports:
                available_sfp_ports = sfp_ports  # Legacy devices use sfp
            
            # DYNAMIC port assignment - DO NOT enforce hardcoded policies
            # Let the source config structure determine port usage
            # The tool should PRESERVE the source structure, not force a specific layout
            if available_sfp_ports:
                # Build available port pools by type, but DO NOT enforce specific assignments
                # These are just suggestions for NEW interfaces, not forced reassignments
                policy_ports['olt'] = available_sfp_ports[:12]  # Any available ports
                policy_ports['switch'] = available_sfp_ports[:12]  # Any available ports
                policy_ports['backhaul'] = available_sfp_ports[:12]  # Any available ports
                policy_ports['lte'] = available_sfp_ports[:12]  # Any available ports
                policy_ports['tarana'] = available_sfp_ports[:12]  # Any available ports
                policy_ports['unknown'] = available_sfp_ports[:12]  # Any available ports
                
                # NOTE: The actual mapping is driven by interface_info (comments/purpose)
                # NOT by hardcoded "OLT must be sfp28-8,9,10,11" rules
                # If source has OLT on sfp28-1,2,3 → preserve that structure
                # If source has bonding on certain ports → preserve those specific ports
                
            print(f"[POLICY PORTS] OLT: {policy_ports['olt']}, Backhaul: {policy_ports['backhaul']}, Switch: {policy_ports['switch']}")
            
            # STEP 4: Build policy-compliant mapping based on interface purpose
            mapping = {}
            purpose_counters = {
                'switch': 0,
                'olt': 0,
                'backhaul': 0,
                'lte': 0,
                'tarana': 0,
                'unknown': 0
            }
            
            # Sort used interfaces to ensure consistent mapping (ether ports first, then SFP ports)
            def interface_sort_key(iface):
                if iface.startswith('ether'):
                    num = int(re.search(r'(\d+)', iface).group(1)) if re.search(r'(\d+)', iface) else 999
                    return (0, num)  # ether ports first
                elif iface.startswith('sfp28-'):
                    num = int(re.search(r'(\d+)', iface).group(1)) if re.search(r'(\d+)', iface) else 999
                    return (2, num)
                elif iface.startswith('sfp-sfpplus'):
                    num = int(re.search(r'(\d+)', iface).group(1)) if re.search(r'(\d+)', iface) else 999
                    return (1, num)
                elif iface.startswith('sfp'):
                    num = int(re.search(r'(\d+)', iface).group(1)) if re.search(r'(\d+)', iface) else 999
                    return (1, num)
                return (3, 999)
            
            sorted_used = sorted(used, key=interface_sort_key)
            
            # CRITICAL: Process interfaces in priority order (OLT first, then backhauls, then others)
            # This ensures OLT gets sfp28-1-3 before backhauls take them
            priority_order = []
            for src in sorted_used:
                iface_info = interface_info.get(src, {})
                purpose = iface_info.get('purpose', 'unknown')
                # Priority: OLT > Switch > Backhaul > LTE > Tarana > Unknown
                if purpose == 'olt':
                    priority_order.append((0, src))  # Highest priority
                elif purpose == 'switch':
                    priority_order.append((1, src))
                elif purpose == 'backhaul':
                    priority_order.append((2, src))
                elif purpose == 'lte':
                    priority_order.append((3, src))
                elif purpose == 'tarana':
                    priority_order.append((4, src))
                else:
                    priority_order.append((5, src))  # Lowest priority
            
            # Sort by priority, then by original sort key
            priority_order.sort(key=lambda x: (x[0], interface_sort_key(x[1])))
            sorted_used = [src for _, src in priority_order]

            target_has_qsfp = any(p.startswith('qsfp') for p in (target_ports or []))
            
            for src in sorted_used:
                # Skip if interface is already in target format (already correct)
                if src in target_ports:
                    print(f"[INTERFACE MAPPING] Skipping {src} (already in target format)")
                    continue
                
                # Skip management port mapping unless target device has only 1 ethernet port (like CCR2004, CCR2216)
                should_skip_mgmt = (src == mgmt_port and len([p for p in target_ports if p.startswith('ether')]) > 1)
                if should_skip_mgmt:
                    print(f"[INTERFACE MAPPING] Skipping management port {src} (target has multiple ethernet ports)")
                    continue

                # Don't attempt to map QSFP breakout ports onto non-QSFP targets; strip them later instead.
                if src.startswith('qsfp') and not target_has_qsfp:
                    print(f"[INTERFACE MAPPING] Skipping QSFP port {src} (target has no QSFP ports)")
                    continue

                # Get interface purpose from classification
                iface_info = interface_info.get(src, {})
                purpose = iface_info.get('purpose', 'unknown')
                comment = iface_info.get('comment', '')

                # Preserve numbering when migrating between sfp28-N and sfp-sfpplusN families.
                # This prevents re-ordering of ports and keeps the config intuitive.
                m_sfp28 = re.fullmatch(r'sfp28-(\d+)', src)
                if m_sfp28:
                    n = int(m_sfp28.group(1))
                    candidate = f"sfp-sfpplus{n}"
                    if candidate in target_ports and candidate not in mapping.values():
                        mapping[src] = candidate
                        print(f"[INTERFACE MAPPING] {src} → {candidate} (preserve index)")
                        continue

                m_sfpplus = re.fullmatch(r'sfp-sfpplus(\d+)', src)
                if m_sfpplus:
                    n = int(m_sfpplus.group(1))
                    candidate = f"sfp28-{n}"
                    if candidate in target_ports and candidate not in mapping.values():
                        mapping[src] = candidate
                        print(f"[INTERFACE MAPPING] {src} → {candidate} (preserve index)")
                        continue
                
                # Map based on purpose according to policy
                if purpose == 'backhaul':
                    # Backhauls start at sfp28-4, then sfp28-5, then sfp28-9+
                    if purpose_counters['backhaul'] < len(policy_ports['backhaul']):
                        dst = policy_ports['backhaul'][purpose_counters['backhaul']]
                        mapping[src] = dst
                        print(f"[INTERFACE MAPPING] {src} → {dst} (BACKHAUL: '{comment}' → policy port)")
                        purpose_counters['backhaul'] += 1
                    else:
                        # Fallback to unknown ports if backhaul ports exhausted
                        if purpose_counters['unknown'] < len(policy_ports['unknown']):
                            dst = policy_ports['unknown'][purpose_counters['unknown']]
                            mapping[src] = dst
                            print(f"[INTERFACE MAPPING] {src} → {dst} (BACKHAUL fallback: '{comment}')")
                            purpose_counters['unknown'] += 1
                    continue
                
                elif purpose == 'olt':
                    # OLT: Use switch ports (sfp28-1-2) or sfp28-3
                    if purpose_counters['olt'] < len(policy_ports['olt']):
                        dst = policy_ports['olt'][purpose_counters['olt']]
                        mapping[src] = dst
                        print(f"[INTERFACE MAPPING] {src} → {dst} (OLT: '{comment}' → policy port)")
                        purpose_counters['olt'] += 1
                    else:
                        # Fallback to unknown ports
                        if purpose_counters['unknown'] < len(policy_ports['unknown']):
                            dst = policy_ports['unknown'][purpose_counters['unknown']]
                            mapping[src] = dst
                            print(f"[INTERFACE MAPPING] {src} → {dst} (OLT fallback: '{comment}')")
                            purpose_counters['unknown'] += 1
                    continue
                
                elif purpose == 'switch':
                    # Switches: sfp28-1, sfp28-2
                    if purpose_counters['switch'] < len(policy_ports['switch']):
                        dst = policy_ports['switch'][purpose_counters['switch']]
                        mapping[src] = dst
                        print(f"[INTERFACE MAPPING] {src} → {dst} (SWITCH: '{comment}' → policy port)")
                        purpose_counters['switch'] += 1
                    else:
                        # Fallback to unknown ports
                        if purpose_counters['unknown'] < len(policy_ports['unknown']):
                            dst = policy_ports['unknown'][purpose_counters['unknown']]
                            mapping[src] = dst
                            print(f"[INTERFACE MAPPING] {src} → {dst} (SWITCH fallback: '{comment}')")
                            purpose_counters['unknown'] += 1
                    continue
                
                elif purpose == 'lte':
                    # LTE: sfp28-6
                    if purpose_counters['lte'] < len(policy_ports['lte']):
                        dst = policy_ports['lte'][purpose_counters['lte']]
                        mapping[src] = dst
                        print(f"[INTERFACE MAPPING] {src} → {dst} (LTE: '{comment}' → policy port)")
                        purpose_counters['lte'] += 1
                    continue
                
                elif purpose == 'tarana':
                    # Tarana: sfp28-6, sfp28-7, sfp28-8
                    if purpose_counters['tarana'] < len(policy_ports['tarana']):
                        dst = policy_ports['tarana'][purpose_counters['tarana']]
                        mapping[src] = dst
                        print(f"[INTERFACE MAPPING] {src} → {dst} (TARANA: '{comment}' → policy port)")
                        purpose_counters['tarana'] += 1
                    continue
                
                # For unknown purpose or ether ports: map sequentially
                if src.startswith('ether') and src != mgmt_port:
                    ether_num = int(re.search(r'(\d+)', src).group(1)) if re.search(r'(\d+)', src) else None
                    if ether_num and ether_num > 1:
                        # Map ether2+ to SFP ports when target has only 1 ethernet OR when source has more ethernet ports than target
                        ethernet_ports = [p for p in target_ports if p.startswith('ether')]
                        source_ethernet_ports = [p for p in source_ports if p.startswith('ether')] if source_ports else []
                        
                        # Map if: target has only 1 ethernet, OR source has more ethernet ports than target
                        should_map_ether = (len(ethernet_ports) == 1) or (len(source_ethernet_ports) > len(ethernet_ports))
                        
                        if should_map_ether:
                            # Map ether2+ sequentially to available SFP ports
                            # ether2 → first SFP port, ether3 → second SFP port, etc.
                            ether_offset = ether_num - 2  # ether2 = offset 0, ether3 = offset 1, etc.
                            if ether_offset < len(policy_ports['unknown']):
                                dst = policy_ports['unknown'][ether_offset]
                                mapping[src] = dst
                                print(f"[INTERFACE MAPPING] {src} → {dst} (ether{ether_num} → SFP port {ether_offset+1} for device migration)")
                                purpose_counters['unknown'] = max(purpose_counters['unknown'], ether_offset + 1)
                            continue
                
                # Legacy SFP ports or unknown: map to unknown ports
                if purpose_counters['unknown'] < len(policy_ports['unknown']):
                    dst = policy_ports['unknown'][purpose_counters['unknown']]
                    mapping[src] = dst
                    print(f"[INTERFACE MAPPING] {src} → {dst} (unknown/legacy: '{comment}')")
                    purpose_counters['unknown'] += 1
            
            if not mapping:
                print(f"[INTERFACE MAPPING] No mappings needed - all interfaces already match target device")
                # Still do port name normalization
            else:
                print(f"[INTERFACE MAPPING] Applying {len(mapping)} interface mappings to ALL references...")
                # Apply replacements comprehensively - update ALL references throughout the config
                # Sort by length (longest first) to avoid partial matches
                for src in sorted(mapping.keys(), key=len, reverse=True):
                    dst = mapping[src]
                    # Apply to ALL occurrences with word boundaries
                    # This updates: /interface ethernet set, /ip address interface=, /routing ospf interfaces=, bridge ports, etc.
                    text = re.sub(rf"(?<![A-Za-z0-9_-]){re.escape(src)}(?![A-Za-z0-9_-])", dst, text)
                    print(f"[INTERFACE MAPPING] Applied: {src} → {dst} (all occurrences)")
             
            # Also update embedded port tokens in common vlan interface name patterns
            # (e.g., vlan1000sfp-sfpplus1 -> vlan1000sfp28-1) so references remain consistent.
            if mapping:
                for src in sorted(mapping.keys(), key=len, reverse=True):
                    dst = mapping[src]
                    text = re.sub(rf"(?m)\b(vlan\d+[-_]?)" + re.escape(src) + r"(?!\d)\b", rf"\1{dst}", text)

            # Port name normalization: convert legacy port names to target device port format
            target_port_prefix = None
            # Prefer the dominant port family on the target (CCR2004 should stay sfp-sfpplus, even though it has 2 sfp28 ports).
            tgt_sfp28 = [p for p in target_seq if p.startswith('sfp28-')]
            tgt_sfp_sfpplus = [p for p in target_seq if p.startswith('sfp-sfpplus')]
            tgt_sfp = [p for p in target_seq if p.startswith('sfp') and not p.startswith('sfp28-') and not p.startswith('sfp-sfpplus')]
            if tgt_sfp_sfpplus and len(tgt_sfp_sfpplus) >= len(tgt_sfp28):
                target_port_prefix = 'sfp-sfpplus'
            elif tgt_sfp28:
                target_port_prefix = 'sfp28'
            elif tgt_sfp:
                target_port_prefix = 'sfp'
            
            # Convert legacy port names to target device format if needed
            if target_port_prefix:
                # Convert sfp-sfpplusN to target format (if target uses sfp28-N)
                if target_port_prefix == 'sfp28':
                    for i in range(1, 13):
                        text = re.sub(rf"(?<![A-Za-z0-9_-])sfp\-sfpplus{i}(?![A-Za-z0-9_-])", f"sfp28-{i}", text)
                # Convert old SFP ports (sfp1, sfp2, etc.) to target format
                if target_port_prefix in ['sfp28', 'sfp-sfpplus']:
                    for i in range(1, 5):
                        old_pattern = rf"(?<![A-Za-z0-9_-])sfp{i}(?![A-Za-z0-9_-])"
                        new_port = f"sfp-sfpplus{i}" if target_port_prefix == 'sfp-sfpplus' else f"sfp28-{i}"
                        text = re.sub(old_pattern, new_port, text)

            # If target doesn't have QSFP ports, strip qsfp* interface lines (common on CCR2216 exports).
            # This prevents invalid ports from remaining after a downgrade/migration.
            target_has_qsfp = any(p.startswith('qsfp') for p in (target_ports or []))
            if not target_has_qsfp:
                text = re.sub(r"(?m)^\s*set\s+\[\s*find\s+default-name=qsfp[^\]]+\][^\n]*\n?", "", text)
            
            return text

        # STATE DETECTION AND POLICY SELECTION
        def detect_state_and_policy(config: str) -> dict:
            """Detect state from config and return appropriate policy key"""
            config_upper = config.upper()
            
            # Detect state from site names (TX-*, KS-*, IL-*)
            state = None
            policy_key = None
            
            # Check for state prefixes in site names
            if re.search(r'\b(TX-|TEXAS)', config_upper):
                state = 'Texas'
                # Check if MPLS (out-of-state) or non-MPLS (in-state)
                if 'MPLS' in config_upper or 'VPLS' in config_upper or 'bridge999' in config_upper:
                    policy_key = 'nextlink-texas-out-of-state-mpls-config-policy'  # If exists
                else:
                    policy_key = 'nextlink-texas-in-statepolicy'
            elif re.search(r'\b(KS-|KANSAS)', config_upper):
                state = 'Kansas'
                policy_key = 'nextlink-kansas-out-of-state-mpls-config-policy'
            elif re.search(r'\b(IL-|ILLINOIS)', config_upper):
                state = 'Illinois'
                policy_key = 'nextlink-illinois-out-of-state-mpls-config-policy'
            
            # Also check IP ranges (Kansas: 10.248.x.x, 10.249.x.x; Illinois: 10.247.x.x; Texas: varies)
            if not state:
                ip_matches = re.findall(r'\b(10\.(?:247|248|249)\.\d+\.\d+)', config)
                if ip_matches:
                    first_ip = ip_matches[0]
                    if first_ip.startswith('10.248.') or first_ip.startswith('10.249.'):
                        state = 'Kansas'
                        policy_key = 'nextlink-kansas-out-of-state-mpls-config-policy'
                    elif first_ip.startswith('10.247.'):
                        state = 'Illinois'
                        policy_key = 'nextlink-illinois-out-of-state-mpls-config-policy'
            
            return {
                'state': state or 'Unknown',
                'policy_key': policy_key,
                'has_mpls': 'MPLS' in config_upper or 'VPLS' in config_upper or 'bridge999' in config_upper or 'bridge600' in config_upper or 'bridge800' in config_upper
            }

        # INTELLIGENT DETECTION AND ANALYSIS
        # Define apply_intelligent_translation BEFORE using it
        def apply_intelligent_translation(config, source_device_info, source_syntax_info, target_syntax_info, target_device_info, target_version, strict_preserve: bool = True):
            """
            IMPROVED intelligent translation - handles structure preservation + syntax changes.
            Philosophy: Preserve ALL content, fix structure, ensure proper section organization.
            """
            translated = config
            print(f"[TRANSLATION] Starting intelligent translation...")
            print(f"[TRANSLATION] Source device: {source_device_info.get('model', 'unknown')} → Target: {target_device_info.get('model', 'unknown')}")
            
            # 1. Update device model and identity - CRITICAL FIX
            if source_device_info.get('model', 'unknown') != 'unknown' and target_device_info.get('model', 'unknown') != 'unknown':
                old_model_short = source_device_info['model'].split('-')[0]  # e.g., "CCR1072"
                new_model_short = target_device_info['model'].split('-')[0]  # e.g., "CCR2004"
                
                # Extract device digits from both models
                source_digits_match = re.search(r"(\d{3,4})", source_device_info['model'])
                target_digits_match = re.search(r"(\d{3,4})", target_device_info['model'])
                source_digits = source_digits_match.group(1) if source_digits_match else None
                target_digits = target_digits_match.group(1) if target_digits_match else None
                
                print(f"[IDENTITY] Updating device identity: {old_model_short} → {new_model_short} (digits: {source_digits} → {target_digits})")
                
                # Update system identity - handle multiple formats
                # Format 1: /system identity set name="RTR-CCR1072-SITE-1"
                if source_digits and target_digits:
                    # Replace device model in identity (e.g., CCR1072 → CCR2004)
                    translated = re.sub(
                        rf'(?i)(/system identity\s+set\s+name=["\']?RTR-){re.escape(old_model_short)}(-.*?["\']?)',
                        rf'\1{new_model_short}\2',
                        translated
                    )
                    # Also handle without quotes
                    translated = re.sub(
                        rf'(?i)(/system identity\s+set\s+name=RTR-){re.escape(old_model_short)}(-)',
                        rf'\1{new_model_short}\2',
                        translated
                    )
                    # Format 2: set name=RTR-CCR1072-SITE-1 (standalone)
                    translated = re.sub(
                        rf'(?i)(set\s+name=["\']?RTR-){re.escape(old_model_short)}(-.*?["\']?)',
                        rf'\1{new_model_short}\2',
                        translated
                    )
                    # Format 3: Just the digits in the identity (if model name is already correct)
                    translated = re.sub(
                        rf'(?i)(RTR-[A-Z]+-){re.escape(source_digits)}(-)',
                        rf'\g<1>{target_digits}\g<2>',
                        translated
                    )
                
                # Also update any comments or references to the device model
                translated = re.sub(
                    rf'(?i)\b{re.escape(old_model_short)}\b',
                    new_model_short,
                    translated
                )

                # RouterOS export header uses a full model string (e.g., CCR2216-1G-12XS-2XQ).
                # The generic replacement above can produce invalid hybrids (e.g., CCR2004-1G-12XS-2XQ).
                # Force the header to the exact target model when we know it.
                try:
                    target_model_full = target_device_info.get('model', 'unknown')
                    if target_model_full and target_model_full != 'unknown':
                        translated = re.sub(
                            r'(?m)^#\s*model\s*=.*$',
                            f"# model ={target_model_full}",
                            translated
                        )
                except Exception:
                    pass

                # Handle common Nextlink identity format: RTR-MT####-...
                # Example: RTR-MT2216-AR1 -> RTR-MT2004-AR1
                if source_digits and target_digits:
                    translated = re.sub(
                        rf'(?i)(/system identity\s+set\s+name=["\']?RTR-MT){re.escape(source_digits)}(\b)',
                        rf'\g<1>{target_digits}\g<2>',
                        translated
                    )
                    translated = re.sub(
                        rf'(?i)(set\s+name=["\']?RTR-MT){re.escape(source_digits)}(\b)',
                        rf'\g<1>{target_digits}\g<2>',
                        translated
                    )
                
                print(f"[IDENTITY] Device identity updated successfully")
            
            # 2. Update RouterOS version header - handle multiple formats
            version_patterns = [
                r'by RouterOS \d+(?:\.\d+)+',
                r'RouterOS \d+(?:\.\d+)+',
                r'#.*RouterOS \d+(?:\.\d+)+',
                r'#.*by RouterOS \d+(?:\.\d+)+'
            ]
            for pattern in version_patterns:
                if re.search(pattern, translated, re.IGNORECASE):
                    # Replace with target version, preserving "by" prefix if present
                    translated = re.sub(
                        pattern,
                        lambda m: re.sub(r'\d+(?:\.\d+)+', target_version, m.group(0)) if re.search(r'\d+(?:\.\d+)+', m.group(0)) else f'by RouterOS {target_version}',
                        translated,
                        flags=re.IGNORECASE
                    )
                    print(f"[VERSION] Updated RouterOS version header to {target_version}")
                    break
            # Also ensure version is updated even if pattern didn't match exactly
            if f'RouterOS {target_version}' not in translated and 'RouterOS' in translated:
                # Try to find and replace any RouterOS version mention
                translated = re.sub(
                    r'RouterOS \d+(?:\.\d+)+',
                    f'RouterOS {target_version}',
                    translated,
                    count=1
                )
            
            # 3. Extract and preserve OSPF area definitions
            ospf_areas = []
            ospf_area_pattern = r'(?m)^/routing ospf area\s*\nadd\s+[^\n]+name=([^\s]+)[^\n]*$'
            for match in re.finditer(ospf_area_pattern, translated):
                ospf_areas.append(match.group(0).split('\n')[1])  # Get the "add ..." line
                print(f"[OSPF] Preserved area definition: {match.group(1)}")
            
            # 4. Simple syntax changes (RouterOS 6 → 7, or ensure ROS7 syntax is correct)
            is_v6_to_v7 = source_syntax_info['version'].startswith('6') and target_version.startswith('7')
            is_v7_to_v7 = source_syntax_info['version'].startswith('7') and target_version.startswith('7')
            
            # Apply syntax fixes for ROS6→ROS7 OR ensure ROS7 syntax is correct
            if is_v6_to_v7 or is_v7_to_v7:
                if is_v6_to_v7:
                    print(f"[SYNTAX] Converting ROS6 → ROS7 syntax")
                else:
                    print(f"[SYNTAX] Ensuring ROS7 syntax correctness")
                
                # BGP: peer → connection (if still using old syntax)
                if '/routing bgp peer' in translated or '/routing bgp instance' in translated:
                    # instance → template (ROS7)
                    translated = re.sub(r'(?m)^/routing bgp instance\b', '/routing bgp template', translated)
                    # peer → connection
                    translated = translated.replace('/routing bgp peer', '/routing bgp connection')
                    print(f"[SYNTAX] Updated BGP: instance→template, peer→connection")
                    # Parameter conversions
                    translated = translated.replace(' remote-address=', ' remote.address=')
                    translated = translated.replace(' remote-as=', ' remote.as=')
                    translated = translated.replace(' update-source=', ' local.address=')
                    translated = translated.replace(' local-address=', ' local.address=')
                    # Ensure template default line exists for ROS6 set default
                    translated = re.sub(
                        r'(?m)^/routing bgp template\s+set\s+\[\s*find\s+default=yes\s*\]\s+',
                        '/routing bgp template set default ',
                        translated
                    )
                    # Ensure connection lines include templates=default
                    def _ensure_templates_default(m):
                        line = m.group(0)
                        if 'templates=' not in line:
                            line += ' templates=default'
                        return line
                    translated = re.sub(r'(?m)^/routing bgp connection\s+add\b[^\n]*$', _ensure_templates_default, translated)
                
                # OSPF: interface → interface-template, instance syntax
                if '/routing ospf network' in translated:
                    translated = translated.replace('/routing ospf network', '/routing ospf interface-template')
                    print(f"[SYNTAX] Updated OSPF: network → interface-template")
                if '/routing ospf interface' in translated and '/routing ospf interface-template' not in translated:
                    # Only replace if it's the old "interface" syntax, not if it's already "interface-template"
                    translated = re.sub(
                        r'/routing ospf interface\s+add',
                        '/routing ospf interface-template add',
                        translated
                    )
                    print(f"[SYNTAX] Updated OSPF: interface → interface-template")
                # ROS6 instance "set default" → ROS7 instance "add name=default-v2"
                if re.search(r'(?m)^/routing ospf instance\s+set\s+\[\s*find\s+default=yes\s*\]', translated):
                    translated = re.sub(
                        r'(?m)^/routing ospf instance\s+set\s+\[\s*find\s+default=yes\s*\]\s+([^\n]*)$',
                        r'/routing ospf instance add name=default-v2 \1',
                        translated
                    )
                    print(f"[SYNTAX] Updated OSPF: instance default→add name=default-v2")
                
                # OSPF parameter changes: ONLY rewrite inside the OSPF interface-template section.
                # Do NOT touch other sections like /ip address (they use interface= and network=).
                def _rewrite_ospf_params_in_template_section(t: str) -> str:
                    out_lines = []
                    in_it_block = False
                    for ln in t.splitlines():
                        stripped = ln.strip()
                        if stripped == '/routing ospf interface-template':
                            in_it_block = True
                            out_lines.append(ln)
                            continue
                        if stripped.startswith('/routing ospf interface-template add '):
                            fixed = re.sub(r'\binterface=', 'interfaces=', ln)
                            fixed = re.sub(r'\bnetwork=', 'networks=', fixed)
                            out_lines.append(fixed)
                            continue
                        if stripped.startswith('/') and re.match(r'^/[a-z]', stripped) and stripped != '/routing ospf interface-template':
                            in_it_block = False
                            out_lines.append(ln)
                            continue
                        if in_it_block and stripped.startswith('add '):
                            fixed = re.sub(r'\binterface=', 'interfaces=', ln)
                            fixed = re.sub(r'\bnetwork=', 'networks=', fixed)
                            out_lines.append(fixed)
                            continue
                        out_lines.append(ln)
                    return '\n'.join(out_lines)

                translated = _rewrite_ospf_params_in_template_section(translated)
            
            # 5. Ensure OSPF area section exists and has proper content
            ospf_instance_name = None
            ospf_area_name = None
            
            if '/routing ospf' in translated:
                # Check if OSPF instance exists
                ospf_instance_match = re.search(r'(?m)^/routing ospf instance\s*\nadd\s+[^\n]+router-id=([^\s]+)', translated)
                if ospf_instance_match:
                    router_id = ospf_instance_match.group(1)
                    ospf_instance_name = 'default-v2'
                    ospf_area_name = 'backbone-v2'
                    
                    # Check if /routing ospf area section exists and is empty (no add line after it)
                    # Pattern: /routing ospf area followed by newline and NOT an add line
                    area_section_match = re.search(r'(?m)^/routing ospf area\s*\n(?!add\s)', translated)
                    if area_section_match:
                        # Area section exists but is empty - insert area definition right after the header
                        if not re.search(rf'name={ospf_area_name}', translated):
                            print(f"[OSPF FIX] Adding missing OSPF area definition: {ospf_area_name}")
                            # Insert right after the /routing ospf area header
                            translated = re.sub(
                                r'(?m)^(/routing ospf area)\s*\n',
                                rf'\1\nadd disabled=no instance={ospf_instance_name} name={ospf_area_name}\n\n',
                                translated,
                                count=1
                            )
                # Ensure all OSPF area add lines have instance=default-v2 on ROS7
                if re.search(r'(?m)^/routing ospf area\s+add\b', translated):
                    def _ensure_area_instance(m):
                        line = m.group(0)
                        if 'instance=' not in line:
                            line += ' instance=default-v2'
                        return line
                    translated = re.sub(r'(?m)^/routing ospf area\s+add\b[^\n]*$', _ensure_area_instance, translated)
            
            # 6. Interface mapping (hardware changes only)
            target_ports = target_device_info.get('ports', [])
            source_ports = source_device_info.get('ports', [])
            mgmt_port = target_device_info.get('management', '')
            
            if target_ports and source_ports:
                if strict_preserve:
                    print(f"[INTERFACE] strict_preserve: defer mapping ({len(source_ports)} source -> {len(target_ports)} target ports)")
                else:
                    print(f"[INTERFACE] Dynamic mapping: {len(source_ports)} source -> {len(target_ports)} target ports")
                    translated = map_interfaces_dynamically(translated, source_ports, target_ports, mgmt_port, target_device_info.get('type',''))
            
            # 7. Postprocessing (non-strict mode only).
            # Strict mode is intended for "Upgrade Existing": preserve structure and lines exactly.
            if (not strict_preserve) and target_version.startswith('7.'):
                translated = postprocess_to_v7(translated, target_version)
            
            # 8. FINAL CLEANUP (non-strict mode only) - strict mode skips any line-removal logic.
            if not strict_preserve:
                translated = final_structure_cleanup(translated, ospf_instance_name, ospf_area_name)
            
            print(f"[TRANSLATION] Intelligent translation completed")
            return translated
        
        def final_structure_cleanup(text, ospf_instance_name, ospf_area_name):
            """
            FINAL cleanup pass - consolidate scattered sections and fix structure.
            Runs at the very end to catch anything the postprocessor missed.
            """
            print("[FINAL CLEANUP] Starting final structure consolidation...")
            
            lines = text.splitlines()
            cleaned = []
            current_section = None
            bgp_connections = []
            bgp_template_set = None
            skip_line = False
            
            for i, line in enumerate(lines):
                if skip_line:
                    skip_line = False
                    continue
                
                # Track section headers (lines starting with /)
                if line.strip().startswith('/') and not line.strip().startswith('//'):
                    # Check if this is a section header (not a command embedded in text)
                    if re.match(r'^/[a-z]', line):
                        current_section = line.strip()
                        cleaned.append(line)
                        continue
                
                # Remove orphan BGP connection lines (not in proper BGP section)
                if current_section and 'bgp' not in current_section.lower():
                    # Check if this line looks like a BGP connection
                    if re.search(r'\bas=26077\b.*\bremote\.address=.*\btcp-md5-key=', line):
                        print(f"[FINAL CLEANUP] Removing orphan BGP connection from '{current_section}': {line[:80]}...")
                        continue
                
                # Remove duplicate BGP template headers (keep only first)
                if line.strip() == '/routing bgp template' and bgp_template_set is not None:
                    print(f"[FINAL CLEANUP] Removing duplicate BGP template header at line {i}")
                    continue
                elif line.strip() == '/routing bgp template':
                    bgp_template_set = True
                
                # Remove firewall rules that are mixed in BGP template section
                if current_section and 'bgp template' in current_section.lower():
                    if 'chain=forward' in line or 'chain=input' in line or 'chain=prerouting' in line:
                        print(f"[FINAL CLEANUP] Removing firewall rule from BGP section: {line[:80]}...")
                        continue
                
                # Fix empty OSPF area section
                if line.strip() == '/routing ospf area':
                    cleaned.append(line)
                    # Check if next line is an add command
                    if i + 1 < len(lines):
                        next_line = lines[i + 1].strip()
                        if not next_line.startswith('add'):
                            # Empty area section - add the definition
                            if ospf_instance_name and ospf_area_name:
                                print(f"[FINAL CLEANUP] Adding missing OSPF area definition")
                                cleaned.append(f"add disabled=no instance={ospf_instance_name} name={ospf_area_name}")
                                cleaned.append("")
                    continue
                
                cleaned.append(line)
            
            result = '\n'.join(cleaned)
            print("[FINAL CLEANUP] Final structure consolidation completed")
            return result

        print(f"[INTELLIGENT ANALYSIS] Analyzing source config...")
        source_syntax_info = detect_routeros_syntax(source_config)
        source_device_info = detect_source_device(source_config)
        target_device_info = get_target_device_info(target_device)
        target_syntax_info = get_target_syntax(target_version)

        def _extract_identity(cfg_text: str):
            if not cfg_text:
                return None
            # Handles both compact and line-separated exports.
            m = re.search(r'(?ms)^/system identity\s*\n\s*set\s+name=([^\n]+)\s*$', cfg_text)
            if m:
                return m.group(1).strip().strip('"').strip("'")
            m = re.search(r'(?m)^\s*/system identity\s+set\s+name=([^\s]+)\s*$', cfg_text)
            if m:
                return m.group(1).strip().strip('"').strip("'")
            return None

        source_info = {
            'model': source_device_info.get('model', 'unknown'),
            'type': source_device_info.get('type', 'unknown'),
            'identity': _extract_identity(source_config) or '',
        }
        target_info = {
            'model': target_device_info.get('model', 'unknown'),
            'type': target_device_info.get('type', target_device),
            'routeros': target_version,
        }

        def _rewrite_identity_for_target(identity_name: str) -> str:
            """Rewrite the extracted identity so it reflects the target device/model/digits.

            This handles several common formats (quoted/unquoted, MT####, plain digits, model shortnames)
            and performs word-boundary substitutions to avoid accidental partial matches.
            """
            name = (identity_name or '').strip()
            target_model = (target_device_info.get('model') or '').strip()
            src_model = (source_device_info.get('model') or '').strip()

            tgt_digits = re.search(r'(\d{3,4})', target_model) if target_model else None
            src_digits = re.search(r'(\d{3,4})', src_model) if src_model else None

            # Replace explicit MT#### occurrences and standalone digit sequences where present.
            if src_digits and tgt_digits:
                name = re.sub(rf'(?i)\bMT{re.escape(src_digits.group(1))}\b', f"MT{tgt_digits.group(1)}", name)
                name = re.sub(rf'(?i)\b{re.escape(src_digits.group(1))}\b', tgt_digits.group(1), name)

            # Replace short model names (e.g., CCR2004 -> CCR2216)
            old_model_short = (src_model.split('-')[0] if src_model else '').strip()
            new_model_short = (target_model.split('-')[0] if target_model else '').strip()
            if old_model_short and new_model_short and old_model_short.lower() != 'unknown' and new_model_short.lower() != 'unknown':
                name = re.sub(rf'(?i)\b{re.escape(old_model_short)}\b', new_model_short, name)

            # If the identity is digits-only (e.g., "2216") or contains a standalone 3-4 digit token
            # and the target model includes digits, replace that token with the target digits.
            if tgt_digits:
                if re.fullmatch(r'\d{3,4}', name):
                    name = tgt_digits.group(1)
                elif (not src_digits) and re.search(r'(?<!\d)\d{3,4}(?!\d)', name):
                    name = re.sub(r'(?<!\d)\d{3,4}(?!\d)', tgt_digits.group(1), name, count=1)

            return name

        def _enforce_management_port_policy(cfg_text: str) -> str:
            """
            NextLink policy: ether1 is MANAGEMENT only.

            If a migration results in ether1 being used as a routed uplink/backhaul (IP/OSPF/mangle),
            remap those non-management references to a deterministic SFP port on the target device and
            keep ether1 labeled as Management.
            """
            text = cfg_text or ''
            mgmt = (target_device_info.get('management') or 'ether1').strip() or 'ether1'
            if mgmt.lower() != 'ether1':
                return text

            target_ports = target_device_info.get('ports') or []
            candidates = [p for p in target_ports if p != mgmt]
            if not candidates:
                return text

            # Collect used interface references to pick an unused destination port.
            used = set()
            for m in re.finditer(r'(?i)(?:^|\s)(?:interface|interfaces|in-interface|out-interface)=([A-Za-z0-9._-]+)', text):
                used.add(m.group(1))

            def _looks_mgmt_comment(c: str) -> bool:
                c = (c or '').lower()
                return ('mgmt' in c) or ('management' in c)

            def _ip_in_192168(ip: str) -> bool:
                try:
                    return ipaddress.ip_address(ip) in ipaddress.ip_network('192.168.0.0/16')
                except Exception:
                    return False

            # Detect non-mgmt usage on ether1.
            non_mgmt_ip_on_ether1 = False
            ether1_comment = None
            for line in text.splitlines():
                if re.search(r'\bdefault-name=ether1\b', line) and 'comment=' in line:
                    cm = re.search(r'\bcomment=([^\s]+|"[^"]*")', line)
                    if cm:
                        ether1_comment = cm.group(1).strip().strip('"')
                if re.search(r'(?i)\binterface=ether1\b', line) and line.lstrip().startswith('add') and 'address=' in line:
                    ipm = re.search(r'\baddress=([0-9.]+)', line)
                    cm = re.search(r'\bcomment=([^\s]+|"[^"]*")', line)
                    cmt = (cm.group(1).strip().strip('"') if cm else '')
                    if ipm:
                        ip = ipm.group(1)
                        if not _ip_in_192168(ip) and not _looks_mgmt_comment(cmt):
                            non_mgmt_ip_on_ether1 = True
                if re.search(r'(?i)\binterfaces=ether1\b', line):
                    non_mgmt_ip_on_ether1 = True
                if re.search(r'(?i)\b(out-interface|in-interface)=ether1\b', line):
                    non_mgmt_ip_on_ether1 = True
                # Any other ether1 attachment (bridge ports, DHCP server interface, etc.) is not management.
                if re.search(r'(?i)\binterface=ether1\b', line) and 'address=' not in line and 'default-name=ether1' not in line:
                    if 'source=' not in line:
                        non_mgmt_ip_on_ether1 = True

            if not non_mgmt_ip_on_ether1:
                return text

            dest = next((p for p in candidates if p not in used), candidates[0])

            # Two-pass update for /interface ethernet lines so we can copy comments and un-disable dest.
            lines = text.splitlines()
            in_eth = False
            saw_dest_set = False

            def _strip_disabled_yes(s: str) -> str:
                return re.sub(r'\s+disabled\s*=\s*yes\b', '', s, flags=re.IGNORECASE)

            out = []
            for line in lines:
                stripped = line.strip()
                if stripped.startswith('/'):
                    in_eth = (stripped == '/interface ethernet')
                    out.append(line)
                    continue

                if in_eth and re.search(r'\bdefault-name=ether1\b', line):
                    # Ensure ether1 is labeled as Management.
                    if 'comment=' in line and not _looks_mgmt_comment(ether1_comment or ''):
                        line = re.sub(r'\bcomment=([^\s]+|"[^"]*")', 'comment="Management"', line)
                    elif 'comment=' not in line:
                        line = line.rstrip() + ' comment="Management"'
                    out.append(line)
                    continue

                if in_eth and re.search(rf'\bdefault-name={re.escape(dest)}\b', line):
                    saw_dest_set = True
                    line = _strip_disabled_yes(line)
                    if ether1_comment and not _looks_mgmt_comment(ether1_comment):
                        if 'comment=' in line:
                            # Append without destroying existing comment.
                            m = re.search(r'\bcomment=("([^"]*)"|[^\s]+)', line)
                            if m:
                                existing = m.group(1).strip().strip('"')
                                combined = f"{existing} | {ether1_comment}"
                                line = re.sub(r'\bcomment=("([^"]*)"|[^\s]+)', f'comment="{combined}"', line)
                        else:
                            line = line.rstrip() + f' comment="{ether1_comment}"'
                    out.append(line)
                    continue

                out.append(line)

            # If dest has no explicit set line, add a minimal one under /interface ethernet.
            if not saw_dest_set:
                updated = []
                inserted = False
                for i, line in enumerate(out):
                    updated.append(line)
                    if line.strip() == '/interface ethernet':
                        continue
                    if not inserted and (i + 1 < len(out)) and out[i].strip().startswith('set') and out[i + 1].strip().startswith('/'):
                        # Insert before leaving the ethernet section.
                        cmt = ether1_comment if (ether1_comment and not _looks_mgmt_comment(ether1_comment)) else 'Uplink'
                        updated.append(f'set [ find default-name={dest} ] comment="{cmt}"')
                        updated.append(f'set [ find default-name={dest} ] disabled=no')
                        inserted = True
                out = updated

            # Now rewrite non-management references from ether1 -> dest.
            rewritten = []
            for line in out:
                s = line
                # Never rewrite inside script source payloads.
                if 'source=' in s:
                    rewritten.append(s)
                    continue
                if re.search(r'(?i)\binterfaces=ether1\b', s):
                    s = re.sub(r'(?i)\binterfaces=ether1\b', f'interfaces={dest}', s)
                if re.search(r'(?i)\binterface=ether1\b', s) and s.lstrip().startswith('add') and 'address=' in s:
                    ipm = re.search(r'\baddress=([0-9.]+)', s)
                    cm = re.search(r'\bcomment=([^\s]+|"[^"]*")', s)
                    cmt = (cm.group(1).strip().strip('"') if cm else '')
                    ip = ipm.group(1) if ipm else ''
                    if ip and (not _ip_in_192168(ip)) and (not _looks_mgmt_comment(cmt)):
                        s = re.sub(r'(?i)\binterface=ether1\b', f'interface={dest}', s)
                # For all other lines, any `interface=ether1` is non-management and must move.
                elif re.search(r'(?i)\binterface=ether1\b', s) and 'default-name=ether1' not in s:
                    s = re.sub(r'(?i)\binterface=ether1\b', f'interface={dest}', s)
                if re.search(r'(?i)\b(out-interface|in-interface)=ether1\b', s):
                    s = re.sub(r'(?i)\bout-interface=ether1\b', f'out-interface={dest}', s)
                    s = re.sub(r'(?i)\bin-interface=ether1\b', f'in-interface={dest}', s)
                rewritten.append(s)

            return "\n".join(rewritten)

        def _postprocess_translated(cfg_text: str) -> str:
            t = cfg_text or ''

            # Normalize RouterOS header version and model to the target.
            t = re.sub(r'(?m)^(#.*by RouterOS )\d+(?:\.\d+)+', rf'\g<1>{target_version}', t)
            t = re.sub(r'(?m)^(#.*RouterOS )\d+(?:\.\d+)+', rf'\g<1>{target_version}', t)

            target_model_full = target_device_info.get('model', 'unknown')
            if target_model_full and target_model_full != 'unknown':
                t = re.sub(r'(?m)^#\s*model\s*=.*$', f"# model ={target_model_full}", t)

            # Normalize legacy speed tokens for RouterOS v7 targets.
            if target_version.startswith('7.'):
                t = t.replace('speed=1G-baseX', 'speed=1G-baseT-full')
                t = t.replace('speed=10G-baseSR', 'speed=10G-baseSR-LR')

            # Rewrite identity to match target device family (e.g., MT1036 -> MT2004).
            def _format_identity_for_set(name: str) -> str:
                s = (name or '').strip()
                # Use double quotes if name contains spaces or unusual characters.
                if re.search(r'[^A-Za-z0-9._-]', s):
                    # Replace any double quotes to avoid malformed output
                    s = s.replace('"', "'")
                    return f'"{s}"'
                return s

            # If a /system identity block exists, replace the set name line robustly.
            if re.search(r'(?m)^\s*/system identity\b', t):
                def _replace_set_name(m):
                    current = m.group('name').strip().strip('"').strip("'")
                    newname = _rewrite_identity_for_target(current)
                    return m.group('prefix') + _format_identity_for_set(newname)

                t = re.sub(r'(?m)^(?P<prefix>\s*/system identity\s*\n\s*set\s+name=)(?P<name>[^\n]+)', _replace_set_name, t)
            else:
                base = _extract_identity(source_config) or f"RTR-{(target_device_info.get('type') or target_device).upper()}-UNKNOWN"
                t = t.rstrip() + "\n\n/system identity\nset name=" + _format_identity_for_set(_rewrite_identity_for_target(base)) + "\n"

            t = _enforce_management_port_policy(t)

            # Deterministic formatting + cleanup
            t = format_config_spacing(t)
            return t

        def _enforce_target_interfaces(cfg_text: str) -> str:
            """
            Ensure all interface references exist on the target device.
            Any interface token not in the target port set is remapped deterministically.
            """
            text = cfg_text or ''
            # Normalize known interface typos from source configs.
            def _normalize_iface_typos(s: str) -> str:
                if not s:
                    return s
                s = re.sub(r'\bsfp-fpplus(\d+)\b', r'sfp-sfpplus\1', s)
                return s

            text = _normalize_iface_typos(text)
            target_ports = target_device_info.get('ports') or []
            mgmt = (target_device_info.get('management') or 'ether1').strip() or 'ether1'
            if not target_ports:
                return text

            target_set = set(target_ports)

            def _extract_iface_entries(config_text: str) -> list[dict]:
                entries = []
                lines = (config_text or '').splitlines()
                in_eth = False
                for line in lines:
                    stripped = line.strip()
                    if stripped.startswith('/'):
                        in_eth = (stripped == '/interface ethernet')
                        continue
                    if not in_eth:
                        continue
                    m_iface = re.search(r'default-name=([^\s\]]+)', line)
                    m_comment = re.search(r'comment=([^\s\n"]+|"[^"]+")', line)
                    if not m_iface or not m_comment:
                        continue
                    iface = m_iface.group(1).strip()
                    comment = m_comment.group(1).strip().strip('"')
                    entries.append({'iface': iface, 'comment': comment})
                return entries

            def _classify_interface_purpose(comment: str, iface_name: str) -> str:
                c = (comment or '').upper()
                iface_upper = (iface_name or '').upper()
                if any(k in c for k in ['BACKHAUL', 'BH', 'TX-', 'KS-', 'IL-', 'TOWER']):
                    return 'backhaul'
                if 'OLT' in c or 'NOKIA' in c:
                    return 'olt'
                if any(k in c for k in ['SWITCH', 'SW', 'NETONIX']):
                    return 'switch'
                if any(k in c for k in ['UPS', 'ICT', 'POWER', 'WPS']):
                    return 'power'
                if 'LTE' in c:
                    return 'lte'
                if 'TARANA' in c or any(k in c for k in ['ALPHA', 'BETA', 'GAMMA', 'DELTA']):
                    return 'tarana'
                if 'MANAGEMENT' in c or 'MGMT' in c or iface_upper == 'ETHER1':
                    return 'management'
                return 'unknown'

            def _normalize_comment_key(comment: str) -> str:
                return re.sub(r'[^A-Z0-9]', '', (comment or '').upper())

            # Extract comments from /interface ethernet set lines (ordered) from both source and translated config
            source_entries = _extract_iface_entries(_normalize_iface_typos(source_config or ''))
            current_entries = _extract_iface_entries(text)
            # Also gather interface comments from /ip address lines
            ip_comment_map = {}
            in_ip = False
            for line in text.splitlines():
                stripped = line.strip()
                if stripped.startswith('/'):
                    in_ip = (stripped == '/ip address')
                    continue
                if not in_ip:
                    continue
                m_iface = re.search(r'interface=([^\s]+)', line)
                m_comment = re.search(r'comment=([^\s\n"]+|"[^"]+")', line)
                if not m_iface or not m_comment:
                    continue
                iface = m_iface.group(1).strip()
                comment = m_comment.group(1).strip().strip('"')
                ip_comment_map[iface] = comment
            source_by_comment = {}
            for e in source_entries:
                if e.get('comment'):
                    key = _normalize_comment_key(e['comment'])
                    source_by_comment.setdefault(key, []).append(e['iface'])
            # Include /ip address comments from source config
            in_ip_src = False
            for line in _normalize_iface_typos(source_config or '').splitlines():
                stripped = line.strip()
                if stripped.startswith('/'):
                    in_ip_src = (stripped == '/ip address')
                    continue
                if not in_ip_src:
                    continue
                m_iface = re.search(r'interface=([^\s]+)', line)
                m_comment = re.search(r'comment=([^\s\n"]+|"[^"]+")', line)
                if not m_iface or not m_comment:
                    continue
                iface = m_iface.group(1).strip()
                comment = m_comment.group(1).strip().strip('"')
                key = _normalize_comment_key(comment)
                source_by_comment.setdefault(key, []).append(iface)

            # Build preferred destination pools (exclude management).
            def _pool(prefix: str):
                return [p for p in target_ports if p.startswith(prefix) and p != mgmt]

            pool_sfp_plus = _pool('sfp-sfpplus')
            pool_sfp28 = _pool('sfp28-')
            pool_sfp = _pool('sfp')
            pool_ether = [p for p in target_ports if p.startswith('ether') and p != mgmt]
            pool_qsfp = _pool('qsfp')

            dest_pool = pool_sfp_plus + pool_sfp28 + pool_sfp + pool_ether + pool_qsfp
            if not dest_pool:
                return text

            # Collect interface tokens used in the config (ordered by appearance).
            iface_pattern = r"\b(ether\d+|sfp\d+(?:-\d+)?|sfp-sfpplus\d+|sfp28-\d+|qsfp28-\d+-\d+|qsfpplus\d+-\d+|qsfp\d+(?:-\d+)?|combo\d+)\b"
            seen = set()
            used = []
            for m in re.finditer(iface_pattern, text):
                name = m.group(1)
                if name not in seen:
                    seen.add(name)
                    used.append(name)

            # Build mapping (policy-aware for CCR2004/CCR2216).
            mapping = {}

            def _assign_ports(source_ifaces, port_pool):
                assigned = {}
                idx = 0
                for iface in source_ifaces:
                    if iface == mgmt:
                        continue
                    if idx < len(port_pool):
                        assigned[iface] = port_pool[idx]
                        idx += 1
                return assigned

            target_type = (target_device_info.get('type') or '').lower()
            if target_type in ['ccr2004', 'ccr2216']:
                # Policy pools
                if target_type == 'ccr2004':
                    switch_pool = ['sfp-sfpplus1', 'sfp-sfpplus2']
                    backhaul_pool = ['sfp-sfpplus4', 'sfp-sfpplus5', 'sfp-sfpplus6', 'sfp-sfpplus7', 'sfp-sfpplus8',
                                     'sfp-sfpplus9', 'sfp-sfpplus10', 'sfp-sfpplus11', 'sfp-sfpplus12', 'sfp28-1', 'sfp28-2']
                    power_pool = ['sfp-sfpplus3']
                else:
                    switch_pool = ['sfp28-1', 'sfp28-2']
                    backhaul_pool = ['sfp28-4', 'sfp28-5', 'sfp28-6', 'sfp28-7', 'sfp28-8', 'sfp28-9', 'sfp28-10', 'sfp28-11', 'sfp28-12']
                    power_pool = ['sfp28-3']
                # Reserve backhaul ports first so non-backhaul devices can't steal sfp28-4+
                backhaul_needed = 0
                lines = text.splitlines()
                in_eth_scan = False
                eth_set_pattern = re.compile(r'^(\s*set\s+\[\s*find\s+default-name=)([^\s\]]+)(\s*\].*)$', re.IGNORECASE)
                for line in lines:
                    stripped = line.strip()
                    if stripped.startswith('/'):
                        in_eth_scan = (stripped == '/interface ethernet')
                        continue
                    if not in_eth_scan:
                        continue
                    m_scan = eth_set_pattern.match(line)
                    if not m_scan:
                        continue
                    iface_scan = m_scan.group(2)
                    if iface_scan == mgmt:
                        continue
                    cm_scan = re.search(r'comment=([^\s\n"]+|"[^"]+")', line)
                    comment_scan = cm_scan.group(1).strip().strip('"') if cm_scan else ''
                    if _classify_interface_purpose(comment_scan, iface_scan) == 'backhaul':
                        backhaul_needed += 1

                reserved_backhaul = backhaul_pool[:backhaul_needed]
                remaining_backhaul = [p for p in backhaul_pool if p not in reserved_backhaul]
                non_qsfp = [p for p in dest_pool if not p.startswith('qsfp')]
                extra_pool = [p for p in non_qsfp if p not in switch_pool + backhaul_pool + power_pool]
                other_pool = remaining_backhaul + extra_pool
                qsfp_pool = [p for p in dest_pool if p.startswith('qsfp')]

                # Rewrite /interface ethernet set lines directly to enforce policy.
                used_dest = set()
                mapping = {}
                out_lines = []
                in_eth = False

                def _next_from(pool):
                    for dest in pool:
                        if dest not in used_dest:
                            used_dest.add(dest)
                            return dest
                    return None

                for line in lines:
                    stripped = line.strip()
                    if stripped.startswith('/'):
                        in_eth = (stripped == '/interface ethernet')
                        out_lines.append(line)
                        continue
                    if in_eth:
                        m = eth_set_pattern.match(line)
                        if m:
                            iface = m.group(2)
                            if iface == mgmt:
                                out_lines.append(line)
                                continue
                            # extract comment to classify
                            cm = re.search(r'comment=([^\\s\\n"]+|"[^"]+")', line)
                            comment = cm.group(1).strip().strip('"') if cm else ''
                            if not comment and iface in ip_comment_map:
                                comment = ip_comment_map.get(iface, '')
                            purpose = _classify_interface_purpose(comment, iface)
                            if purpose == 'management':
                                dest = mgmt
                            elif purpose == 'switch':
                                dest = _next_from(switch_pool) or _next_from(other_pool) or _next_from(qsfp_pool)
                            elif purpose == 'backhaul':
                                dest = _next_from(backhaul_pool) or _next_from(other_pool) or _next_from(qsfp_pool)
                            elif purpose == 'power':
                                dest = _next_from(power_pool) or _next_from(other_pool) or _next_from(qsfp_pool)
                            else:
                                dest = _next_from(other_pool) or _next_from(power_pool) or _next_from(qsfp_pool)
                            if dest:
                                mapping[iface] = dest
                                # Also map original source interfaces that used the same comment.
                                for src_iface in source_by_comment.get(_normalize_comment_key(comment), []):
                                    if src_iface not in mapping:
                                        mapping[src_iface] = dest
                                line = m.group(1) + dest + m.group(3)
                                # Remove renaming of physical ports to avoid clashes on target
                                line = re.sub(r'\s+name=[^\s]+', '', line)
                        out_lines.append(line)
                        continue
                    out_lines.append(line)

                text = "\n".join(out_lines)

                # Add fallback mappings for any remaining invalid interfaces (e.g., bridge ports)
                used_updated = []
                seen_updated = set()
                for m in re.finditer(iface_pattern, text):
                    name = m.group(1)
                    if name not in seen_updated:
                        seen_updated.add(name)
                        used_updated.append(name)
                fallback_pool = other_pool + backhaul_pool + power_pool + switch_pool + qsfp_pool
                for iface in used_updated:
                    if iface == mgmt:
                        continue
                    if iface in target_set:
                        continue
                    if iface in mapping:
                        continue
                    dest = _next_from(fallback_pool)
                    if dest:
                        mapping[iface] = dest

                # Enforce management port: ensure "Management" comment lands on ether1 only.
                lines = text.splitlines()
                in_eth = False
                mgmt_written = False
                saw_eth_section = False
                cleaned = []
                for line in lines:
                    stripped = line.strip()
                    if stripped.startswith('/'):
                        in_eth = (stripped == '/interface ethernet')
                        if in_eth:
                            saw_eth_section = True
                        cleaned.append(line)
                        continue
                    if not in_eth:
                        cleaned.append(line)
                        continue
                    m = eth_set_pattern.match(line)
                    if not m:
                        cleaned.append(line)
                        continue
                    iface = m.group(2)
                    cm = re.search(r'comment=([^\s\n"]+|"[^"]+")', line)
                    comment = cm.group(1).strip().strip('"') if cm else ''
                    if 'MANAGEMENT' in (comment or '').upper() and iface != mgmt:
                        # Move management label to ether1
                        line = re.sub(r'(default-name=)[^\s\]]+', rf'\\1{mgmt}', line)
                        iface = mgmt
                    if iface == mgmt:
                        if mgmt_written:
                            continue
                        mgmt_written = True
                        if 'comment=' in line:
                            line = re.sub(r'comment=([^\s\n"]+|"[^"]+")', 'comment="Management"', line)
                        else:
                            line = line.rstrip() + ' comment="Management"'
                    else:
                        # Strip stray management comment from non-mgmt ports
                        if 'MANAGEMENT' in (comment or '').upper():
                            line = re.sub(r'\s*comment=([^\s\n"]+|"[^"]+")', '', line)
                    cleaned.append(line)
                if not mgmt_written:
                    if saw_eth_section:
                        inserted = False
                        updated = []
                        for i, line in enumerate(cleaned):
                            updated.append(line)
                            if (not inserted) and line.strip() == '/interface ethernet':
                                updated.append(f'set [ find default-name={mgmt} ] comment="Management"')
                                inserted = True
                        cleaned = updated
                    else:
                        cleaned.append('/interface ethernet')
                        cleaned.append(f'set [ find default-name={mgmt} ] comment="Management"')
                text = "\n".join(cleaned)
            else:
                # Default: map only invalid interfaces in order
                pool_idx = 0
                for name in used:
                    if name == mgmt:
                        continue
                    if name in target_set:
                        continue
                    # Map legacy combo to first SFP if present
                    if name.startswith('combo'):
                        if pool_sfp_plus:
                            mapping[name] = pool_sfp_plus[0]
                            continue
                        if pool_sfp28:
                            mapping[name] = pool_sfp28[0]
                            continue
                        if pool_sfp:
                            mapping[name] = pool_sfp[0]
                            continue
                    if pool_idx < len(dest_pool):
                        mapping[name] = dest_pool[pool_idx]
                        pool_idx += 1

            if not mapping:
                return text

            # Replace remaining interface tokens using temporary placeholders to avoid cross-mapping.
            temp_map = {src: f"__TMP_PORT_{idx}__" for idx, src in enumerate(mapping.keys())}
            # Use custom token boundaries because interface names include hyphens (e.g., sfp-sfpplus7).
            for src in sorted(temp_map.keys(), key=len, reverse=True):
                text = re.sub(rf"(?<![A-Za-z0-9_-]){re.escape(src)}(?![A-Za-z0-9_-])", temp_map[src], text)
            for src, tmp in temp_map.items():
                dst = mapping[src]
                text = text.replace(tmp, dst)

            # Update embedded port tokens in common VLAN interface naming patterns
            # (e.g., vlan1000sfp-sfpplus7 -> vlan1000sfp28-4)
            for src in sorted(mapping.keys(), key=len, reverse=True):
                dst = mapping[src]
                text = re.sub(rf"(?m)\b(vlan\d+[-_]?)" + re.escape(src) + r"(?!\d)\b", rf"\1{dst}", text)

            # Final safety pass: remap interface parameters explicitly (handles interface= and interfaces= lists).
            def _map_iface_list(val: str) -> str:
                parts = [p.strip() for p in val.split(',')]
                return ','.join(mapping.get(p, p) for p in parts if p)

            def _remap_iface_params(line: str) -> str:
                def _repl(m):
                    return m.group(1) + _map_iface_list(m.group(2))
                line = re.sub(r'(\\binterfaces?=)([^\\s]+)', _repl, line)
                return line

            remapped_lines = []
            comment_to_iface = {}
            for e in _extract_iface_entries(text):
                if e.get('comment'):
                    key = _normalize_comment_key(e['comment'])
                    if key not in comment_to_iface:
                        comment_to_iface[key] = e['iface']
            def _find_best_comment_iface(key: str) -> str | None:
                if not key:
                    return None
                if key in comment_to_iface:
                    return comment_to_iface[key]
                for k, v in comment_to_iface.items():
                    if key in k or k in key:
                        return v
                def _ed1(a: str, b: str) -> bool:
                    if abs(len(a) - len(b)) > 1:
                        return False
                    if len(a) == len(b):
                        diffs = sum(1 for i in range(len(a)) if a[i] != b[i])
                        return diffs <= 1
                    if len(a) + 1 == len(b):
                        short, long = a, b
                    else:
                        short, long = b, a
                    i = j = 0
                    used = False
                    while i < len(short) and j < len(long):
                        if short[i] == long[j]:
                            i += 1
                            j += 1
                        elif not used:
                            used = True
                            j += 1
                        else:
                            return False
                    return True
                for k, v in comment_to_iface.items():
                    if _ed1(key, k):
                        return v
                return None
            for line in text.splitlines():
                if 'interface=' in line or 'interfaces=' in line:
                    # Prefer comment-driven correction when present
                    cm = re.search(r'comment=([^\s\n"]+|"[^"]+")', line)
                    if cm:
                        ckey = _normalize_comment_key(cm.group(1).strip().strip('"'))
                        mapped_iface = _find_best_comment_iface(ckey)
                        if mapped_iface:
                            line = re.sub(r'(\binterfaces?=)([^\s]+)', r'\1' + mapped_iface, line)
                            remapped_lines.append(line)
                            continue
                    remapped_lines.append(_remap_iface_params(line))
                else:
                    remapped_lines.append(line)
            text = "\n".join(remapped_lines)

            return text
        
        print(f"[DETECTED] Source: {source_syntax_info['version']} on {source_device_info['model']}")
        print(f"[TARGET] Converting to: {target_version} on {target_device_info['model']}")
        print(f"[SYNTAX] BGP: {source_syntax_info['bgp_syntax']} → {target_syntax_info['bgp_peer']}")
        print(f"[SYNTAX] OSPF: {source_syntax_info['ospf_syntax']} → {target_syntax_info['ospf_interface']}")
        
        # STRICT-PRESERVE MODE (Upgrade Existing):
        # Deterministic output; preserve *all* sections/objects and only apply syntax + interface mapping.
        # This avoids AI-based rewrites that can silently drop critical routing/firewall/RADIUS sections.
        if strict_preserve:
            print("[STRICT MODE] strict_preserve=true -> bypassing AI translation and using deterministic translation only")
            translated = apply_intelligent_translation(
                source_config,
                source_device_info,
                source_syntax_info,
                target_syntax_info,
                target_device_info,
                target_version,
                strict_preserve=True,
            )
            translated = _postprocess_translated(translated)
            compliance_validation = None

            # Apply compliance in strict mode if requested
            if apply_compliance and HAS_COMPLIANCE:
                print("[COMPLIANCE] Applying RFC-09-10-25 compliance standards (strict mode)...")
                try:
                    loopback_ip = None
                    loopback_match = re.search(r'/ip address\s+add address=([0-9.]+/[0-9]+)\s+interface=loop0', translated, re.IGNORECASE)
                    if not loopback_match:
                        loopback_match = re.search(r'interface=loop0.*?address=([0-9.]+/[0-9]+)', translated, re.IGNORECASE)
                    if loopback_match:
                        loopback_ip = loopback_match.group(1)
                    if not loopback_ip:
                        source_loopback_match = re.search(r'/ip address\s+add address=([0-9.]+/[0-9]+)\s+interface=loop0', source_config, re.IGNORECASE)
                        if source_loopback_match:
                            loopback_ip = source_loopback_match.group(1)

                    compliance_blocks = get_all_compliance_blocks(loopback_ip or "10.0.0.1/32")
                    translated = inject_compliance_blocks(translated, compliance_blocks)
                    compliance_validation = validate_compliance(translated)
                except Exception as e:
                    print(f"[COMPLIANCE ERROR] Failed to apply compliance in strict mode: {e}")
                    compliance_validation = {'compliant': False, 'error': str(e)}

            # Final interface policy enforcement (after compliance injection).
            translated = _enforce_target_interfaces(translated)
            # Re-validate after compliance injection (if any)
            validation = validate_translation(source_config, translated)
            return jsonify({
                'success': True,
                'translated_config': translated,
                'validation': validation,
                'compliance': compliance_validation,
                'bypass_ai': True,
                'strict_preserve': True,
                'source_info': source_info,
                'target_info': target_info,
                'message': 'Strict-preserve mode: deterministic syntax/interface mapping only (no AI)'
            })

        # BYPASS AI FOR LARGE CONFIGS (prevent timeouts)
        config_size = len(source_config.split('\n'))
        config_size_mb = len(source_config.encode('utf-8')) / (1024 * 1024)
        
        print(f"[CONFIG SIZE] Lines: {config_size}, Size: {config_size_mb:.2f}MB")
        
        # For very large configs, skip AI entirely
        if config_size > 800 or config_size_mb > 2.0:  # Lowered threshold for better performance
            print(f"[BYPASS AI] Large config ({config_size} lines, {config_size_mb:.2f}MB) - using intelligent translation only")
            translated = apply_intelligent_translation(
                source_config,
                source_device_info,
                source_syntax_info,
                target_syntax_info,
                target_device_info,
                target_version,
                strict_preserve=strict_preserve,
            )
            translated = _postprocess_translated(translated)
            translated = _enforce_target_interfaces(translated)
            validation = validate_translation(source_config, translated)
            return jsonify({
                'success': True,
                'translated_config': translated,
                'validation': validation,
                'bypass_ai': True,
                'source_info': source_info,
                'target_info': target_info,
                'message': f'Large config ({config_size} lines, {config_size_mb:.2f}MB) - used intelligent translation for speed'
            })
        
        # If both are v7 AND same exact device, skip AI for speed
        # BUT if device changed, we MUST use AI to properly map interfaces (e.g., sfp-sfpplus → sfp28)
        if is_source_v7 and is_target_v7 and same_exact_device:
            print(f"[FAST MODE] Same device, ROS7→ROS7 - using intelligent translation only (no AI needed)")
            translated_fast = apply_intelligent_translation(
                source_config,
                source_device_info,
                detect_routeros_syntax(source_config),
                get_target_syntax(target_version),
                target_device_info,
                target_version,
                strict_preserve=strict_preserve,
            )
            translated_fast = _postprocess_translated(translated_fast)
            translated_fast = _enforce_target_interfaces(translated_fast)
            validation_fast = validate_translation(source_config, translated_fast)
            return jsonify({
                'success': True,
                'translated_config': translated_fast,
                'validation': validation_fast,
                'fast_mode': True,
                'message': 'ROS7→ROS7, same device - optimized translation (no AI needed)'
            })
        elif is_source_v7 and is_target_v7 and not same_exact_device:
            print(f"[AI MODE] Device change: {source_device_info['model']} → {target_device_info['model']} (ROS7→ROS7 but hardware differs - using AI for proper port mapping)")
            # Continue to AI translation below

        # INTELLIGENT DYNAMIC TRANSLATION FUNCTION
        # (Function already defined above, no need to redefine)

        # Detect state and select appropriate policy
        state_info = detect_state_and_policy(source_config)
        print(f"[STATE DETECTION] Detected: {state_info['state']}, Policy: {state_info['policy_key']}, MPLS: {state_info['has_mpls']}")
        
        # Load state-specific policy if available
        policies = get_config_policies()
        state_policy_content = ""
        if state_info['policy_key'] and state_info['policy_key'] in policies:
            state_policy_content = policies[state_info['policy_key']]['content']
            print(f"[POLICY] Loaded state-specific policy: {state_info['policy_key']}")
        
        # Always include global internet policy
        global_policy_content = ""
        if 'nextlink-internet-policy' in policies:
            global_policy_content = policies['nextlink-internet-policy']['content']
            print(f"[POLICY] Loaded global policy: nextlink-internet-policy")

        training_context = build_training_context()
        compliance_note = ""
        if HAS_COMPLIANCE:
            compliance_note = """
MANDATORY COMPLIANCE (RFC-09-10-25):
- All configurations MUST include NextLink compliance standards
- The backend will automatically add compliance blocks after translation
- Ensure DNS servers are 142.147.112.3,142.147.112.19
- Ensure firewall rules, IP services, NTP, SNMP, and logging follow NextLink standards
- Compliance will be validated and enforced automatically
"""
        
        system_prompt = f"""You are a RouterOS config translator with deep knowledge of RouterOS syntax differences and NextLink policies.

CRITICAL SECTION SEPARATION (TOP PRIORITY):
RouterOS configs are organized in STRICT SECTIONS. Each section MUST be separate and complete:

CORRECT OUTPUT STRUCTURE (ALWAYS FOLLOW THIS ORDER):
/interface bridge
  add ...
  
/interface ethernet
  set [ find default-name=...] ...
  
/interface bonding
  add ...
  
/interface vlan
  add ...
  
/ip address
  add address=... interface=... comment=...
  (ALL IP addresses go HERE, nowhere else!)
  
/routing ospf instance
  add name=...
  
/routing ospf area
  add name=...
  
/routing ospf interface-template
  add area=... interfaces=... networks=...
  (ALL OSPF interface lines go HERE, not in /routing ospf area!)
  
/routing bgp template
  set default ...
  
/routing bgp connection
  add as=... remote.address=... tcp-md5-key=...
  (ALL BGP connections go HERE, not in /routing bfd!)
  
/routing bfd configuration
  add ...
  (ONLY BFD config HERE, no BGP!)

NEVER MIX SECTIONS! Example of WRONG output (NEVER do this):
WRONG: /routing bfd configuration
   add as=26077 remote.address=... tcp-md5-key=...  <-- THIS IS BGP, NOT BFD!
   
WRONG: /routing ospf area
   add interfaces=loop0 networks=10.18.2.4/32  <-- THIS IS INTERFACE-TEMPLATE, NOT AREA!

CRITICAL PRESERVATION RULES (MANDATORY - NO EXCEPTIONS):
1. COPY EVERY SINGLE LINE from source config - DO NOT SKIP ANY LINES
2. PRESERVE ALL IP ADDRESSES EXACTLY as provided - DO NOT MODIFY OR REMOVE ANY IP
3. PRESERVE ALL PASSWORDS, SECRETS, AND AUTHENTICATION KEYS EXACTLY
4. PRESERVE ALL USER ACCOUNTS, GROUPS, AND PERMISSIONS EXACTLY
5. PRESERVE ALL FIREWALL RULES, NAT RULES, MANGLE RULES, RAW RULES EXACTLY - ALL OF THEM, NOT JUST A FEW
6. PRESERVE ALL VLAN IDs, BRIDGE NAMES, INTERFACE NAMES (except hardware port mapping per policy)
7. PRESERVE ALL ROUTING PROTOCOL CONFIGURATIONS (areas, AS numbers, router IDs, network statements)
8. PRESERVE ALL COMMENTS AND DOCUMENTATION
9. PRESERVE ALL CONFIG SECTIONS: /ip address, /routing, /interface, /ip firewall, /mpls, /interface vpls, /snmp, /radius, /user, etc.
10. PRESERVE ALL INTERFACE PARAMETERS: l2mtu, mtu, speed, auto-negotiation, disabled, advertise, etc. - DO NOT REMOVE ANY PARAMETER
11. PRESERVE ALL BONDING CONFIGURATIONS: slaves, lacp-rate, mode, transmit-hash-policy, etc.
12. DO NOT REMOVE, MODIFY, OR SUMMARIZE ANY LINES - COPY EVERYTHING
13. DO NOT CHANGE VALUES - ONLY CHANGE SYNTAX/COMMAND STRUCTURE
14. VERIFY: Count IP addresses in source vs translated - they MUST match exactly
15. VERIFY: All interface parameters (l2mtu, mtu, speed, etc.) are preserved exactly
16. DO NOT MIX SECTIONS - Keep /ip address separate from /routing ospf, keep /routing bgp separate from other sections
17. OUTPUT COMPLETE CONFIG - If source has 200+ lines, translated MUST have similar line count (only syntax changes, not content removal)

TRANSLATION SCOPE (ONLY THESE CHANGES ALLOWED):
- Apply RouterOS {source_syntax_info['version']} → {target_version} syntax changes ONLY
- Update device model references to {target_device_info['model']}
- Map hardware interfaces to {target_device.upper()} ports according to NextLink port assignment policy
- Update command syntax (e.g., /routing bgp peer → /routing bgp connection)
- Update parameter names (e.g., update-source → update.source)

STATE-SPECIFIC POLICY ({state_info['state']}):
{state_policy_content[:2000] if state_policy_content else 'No state-specific policy found - using global standards'}

GLOBAL NEXTLINK POLICY (Port Assignments & Standards):
{global_policy_content[:2000] if global_policy_content else 'Global policy not available'}

FORBIDDEN ACTIONS (NEVER DO THESE):
- DO NOT remove any configuration lines
- DO NOT modify IP addresses, passwords, or secrets
- DO NOT change VLAN IDs or network addresses
- DO NOT summarize or combine multiple rules
- DO NOT remove comments or documentation
- DO NOT remove interface parameters (l2mtu, mtu, speed, auto-negotiation, disabled, advertise, etc.)
- DO NOT modify bonding slave lists - preserve exact interface names
- DO NOT renumber interfaces when source and target use same port format (e.g., both sfp28-)
- DO NOT create incomplete sections (e.g., /ip address with no addresses, /ip firewall filter with only 1 rule)
- DO NOT mix section contents (e.g., OSPF lines in /ip address section, IP lines in /routing section)
- DO NOT output truncated configs - if you can't fit everything, say so rather than outputting partial config
{compliance_note}
SYNTAX CHANGES NEEDED:
- BGP: {source_syntax_info['bgp_syntax']} → {target_syntax_info['bgp_peer']}
- OSPF: {source_syntax_info['ospf_syntax']} → {target_syntax_info['ospf_interface']}
- Parameters: {target_syntax_info['bgp_params']}

HARDWARE CHANGES (INTERFACE MAPPING ONLY):
- Model: {target_device_info['model']}
- Ports: {', '.join(target_device_info['ports'])}
- Management: {target_device_info['management']}
- Map source device ports to target device ports (e.g., sfp-sfpplus1 → sfp28-1 if needed)

REMEMBER: Your job is SYNTAX TRANSLATION ONLY. Preserve ALL data values exactly."""

        if training_context:
            system_prompt += "\n\n" + training_context

        syntax_rules = get_syntax_rules(target_version)

        user_prompt = f"""Translate this RouterOS config from source device to {target_device.upper()} for RouterOS {target_version}:

{source_config}

CRITICAL REQUIREMENTS:
1. Copy EVERY line from source - do not skip any sections
2. Preserve ALL IP addresses exactly (count must match)
3. Preserve ALL interface structures:
   - If source has bonding (e.g., bond3000 slaves=sfp-sfpplus8,9,10,11) → preserve bonding with correct target ports
   - If source has NO bonding → do NOT add bonding
   - Map interface names intelligently based on comments (TX-*, NOKIA, etc.)
   - PRESERVE interface parameters: l2mtu, mtu, speed, auto-negotiation, disabled, advertise
4. Apply syntax changes for RouterOS {target_version}:
   - BGP: peer → connection, update parameter names
   - OSPF: interface → interface-template
   - Keep sections separate (see structure rules above)
5. Ensure all config sections are preserved: /ip address, /routing, /interface, /ip firewall, /mpls, /interface vpls, etc.

IMPORTANT: Each site is unique. DO NOT enforce specific port layouts. PRESERVE the source config structure and simply update syntax + interface names for target device.

Output (copy every line, update device model to {target_device.upper()}, preserve all IPs and structure):"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # EMERGENCY: BYPASS AI COMPLETELY - AI is producing broken output
        # Force intelligent translation for ALL configs until AI issues are resolved
        safe_print("[EMERGENCY MODE] Bypassing AI - using intelligent translation for reliability")
        safe_print("[REASON] AI translations failing validation - producing broken sections, missing IPs")
        translated = apply_intelligent_translation(
            source_config,
            source_device_info,
            source_syntax_info,
            target_syntax_info,
            target_device_info,
            target_version,
            strict_preserve=strict_preserve,
        )
        
        # Original AI path (DISABLED for safety):
        # try:
        #     translated = call_ai(messages, max_tokens=16000, task_type='translation', config_size=len(source_config))
        # except Exception as e:
        #     print(f"[AI ERROR] {str(e)} - Using intelligent fallback")
        #     translated = apply_intelligent_translation(source_config, source_device_info, source_syntax_info, target_syntax_info, target_device_info, target_version)
            

        # Clean up any markdown formatting
        translated = translated.replace('```routeros', '').replace('```', '').strip()

        # CRITICAL: Extract and preserve ALL IP addresses from source before any processing
        # This ensures IP addresses are never lost
        # Enhanced regex to capture all variations: "add address=..." and "/ip address add ..."
        source_ip_addresses = []
        
        # Pattern 1: Full line with /ip address add
        for m in re.finditer(r"(?m)^/ip address\s+add\s+[^\n]+$", source_config):
            ip_line = m.group(0).strip()
            if ip_line not in source_ip_addresses:
                source_ip_addresses.append(ip_line)
        
        # Pattern 2: Lines with "add address=" (in case /ip address section header is separate)
        ip_section_match = re.search(r"(?ms)^/ip address\s*\n(.*?)(?=\n/[a-z]|\Z)", source_config)
        if ip_section_match:
            ip_section_content = ip_section_match.group(1)
            for m in re.finditer(r"(?m)^add\s+.*?address=[^\n]+$", ip_section_content):
                ip_line = m.group(0).strip()
                # Reconstruct as full command
                full_line = "/ip address " + ip_line
                if full_line not in source_ip_addresses and ip_line not in [x.replace('/ip address ', '') for x in source_ip_addresses]:
                    source_ip_addresses.append(full_line)
        
        # Deduplicate while preserving order
        seen = set()
        unique_ips = []
        for ip_addr in source_ip_addresses:
            normalized = re.sub(r'\s+', ' ', ip_addr.strip())
            if normalized not in seen:
                seen.add(normalized)
                unique_ips.append(ip_addr)
        source_ip_addresses = unique_ips
        
        print(f"[IP PRESERVATION] Found {len(source_ip_addresses)} IP address entries in source config")
        if len(source_ip_addresses) > 0:
            print(f"[IP PRESERVATION] Sample: {source_ip_addresses[0][:80]}...")
        
        # Also extract all IP addresses (not just from /ip address section) for validation
        all_source_ips = set(re.findall(r"\b(?:\d{1,3}\.){3}\d{1,3}(?:/\d{1,2})?\b", source_config))
        print(f"[IP PRESERVATION] Total unique IP addresses found in source config: {len(all_source_ips)}")

        # CRITICAL: Apply comprehensive interface mapping after AI translation
        # This ensures ALL interface references are properly mapped (e.g., CCR1072 → CCR2216)
        # Interface mapping vars used for any preservation/injection steps below.
        source_ports_list = source_device_info.get('ports', [])
        target_ports_list = target_device_info.get('ports', [])
        mgmt_port = target_device_info.get('management', '')

        # Heavy postprocessing (to repair broken AI output) is non-strict behavior.
        # "Upgrade Existing" uses strict mode and should preserve the source config as closely as possible.
        if not strict_preserve:
            print("[POST-AI] Applying comprehensive interface mapping...")
            if target_ports_list:
                translated = map_interfaces_dynamically(
                    translated, source_ports_list, target_ports_list, mgmt_port, target_device_info.get('type', '')
                )
            translated = postprocess_to_v7(translated, target_version)

            # FINAL CONSISTENCY CHECK: Ensure BGP connections are in correct block (run after postprocessing)
            bgp_in_wrong_blocks = []
            lines_final = translated.splitlines()
            current_block_final = None
            for line in lines_final:
                if re.match(r'^/(routing|interface|ip|mpls|system)', line):
                    block_match = re.match(r'^/([^\s]+(?:\s+[^\s]+)*)', line)
                    if block_match:
                        current_block_final = block_match.group(0)
                    else:
                        current_block_final = line.split()[0] if line.split() else None
                elif current_block_final and line.strip() and not line.strip().startswith('#'):
                    bgp_indicators_final = [
                        r'\bas=\d+',
                        r'\bremote\.address=',
                        r'\bremote\.as=',
                        r'\btcp\.md5\.key=',
                        r'\blocal\.address=',
                        r'\brouter-id=',
                        r'\btemplates=',
                        r'\boutput\.network=',
                        r'\bcisco-vpls',
                        r'\.role=ibgp',
                        r'\.role=ebgp'
                    ]
                    bgp_match_count_final = sum(1 for pattern in bgp_indicators_final if re.search(pattern, line, re.IGNORECASE))
                    if bgp_match_count_final >= 3:
                        block_lower_final = current_block_final.lower() if current_block_final else ''
                        if 'bgp connection' not in block_lower_final and 'bgp template' not in block_lower_final:
                            bgp_in_wrong_blocks.append((line, current_block_final))

            if bgp_in_wrong_blocks:
                print(f"[CONSISTENCY CHECK] Found {len(bgp_in_wrong_blocks)} BGP connections in wrong blocks - fixing...")
                bgp_lines_to_add = []
                for bgp_line, wrong_block in bgp_in_wrong_blocks:
                    print(f"[CONSISTENCY CHECK] Moving BGP from '{wrong_block}' to /routing bgp connection")
                    translated = translated.replace(bgp_line + '\n', '')
                    translated = translated.replace('\n' + bgp_line, '')
                    translated = translated.replace(bgp_line, '')
                    clean_line = bgp_line.strip()
                    if not clean_line.startswith('add '):
                        clean_line = 'add ' + clean_line.lstrip('add ')
                    bgp_lines_to_add.append(clean_line)

                if bgp_lines_to_add:
                    if '/routing bgp connection' in translated:
                        translated = re.sub(
                            r'(/routing bgp connection\s*\n)',
                            r'\1' + '\n'.join(bgp_lines_to_add) + '\n',
                            translated,
                            count=1
                        )
                    else:
                        if '/routing bgp template' in translated:
                            translated = translated.replace(
                                '/routing bgp template',
                                '/routing bgp connection\n' + '\n'.join(bgp_lines_to_add) + '\n\n/routing bgp template',
                            )
                        else:
                            translated += '\n/routing bgp connection\n' + '\n'.join(bgp_lines_to_add) + '\n'
        
        # CRITICAL: Ensure ALL IP addresses are preserved - NO IPs can be missing
        print(f"[IP PRESERVATION] Verifying all {len(source_ip_addresses)} IP addresses are preserved...")
        
        # Extract all IP addresses from source (normalize for comparison)
        source_ip_set = set()
        for ip_line in source_ip_addresses:
            addr_match = re.search(r'address=([^\s]+)', ip_line)
            if addr_match:
                addr = addr_match.group(1)
                # Normalize /32 to bare IP for comparison
                addr_normalized = addr[:-3] if addr.endswith('/32') else addr
                source_ip_set.add(addr_normalized)
        
        # Extract all IP addresses from translated config
        translated_ip_set = set()
        translated_ip_lines = []
        for m in re.finditer(r"(?m)^/ip address\s+add\s+address=([^\s]+)[^\n]*$", translated):
            addr = m.group(1)
            addr_normalized = addr[:-3] if addr.endswith('/32') else addr
            translated_ip_set.add(addr_normalized)
            translated_ip_lines.append(m.group(0))
        
        # Find missing IPs
        missing_ip_addresses = source_ip_set - translated_ip_set
        
        if missing_ip_addresses:
            print(f"[IP PRESERVATION] CRITICAL: Missing {len(missing_ip_addresses)} IP addresses!")
            print(f"[IP PRESERVATION] Missing IPs: {list(missing_ip_addresses)[:10]}{'...' if len(missing_ip_addresses) > 10 else ''}")
            print(f"[IP PRESERVATION] Re-injecting ALL missing IP addresses...")
            
            # Find corresponding source lines for missing IPs
            missing_ips = []
            for ip_line in source_ip_addresses:
                addr_match = re.search(r'address=([^\s]+)', ip_line)
                if addr_match:
                    addr = addr_match.group(1)
                    addr_normalized = addr[:-3] if addr.endswith('/32') else addr
                    if addr_normalized in missing_ip_addresses:
                        # Update interface names in the IP line to match mapped interfaces
                        updated_line = ip_line
                        # Re-apply interface mapping
                        updated_line = map_interfaces_dynamically(updated_line, source_ports_list, target_ports_list, mgmt_port, target_device_info.get('type', ''))
                        missing_ips.append(updated_line)
            
            if missing_ips:
                # Insert missing IP addresses into /ip address block
                if '/ip address' in translated:
                    # Find the /ip address section
                    ip_block_match = re.search(r'(?ms)^(/ip address\s*\n)(.*?)(?=\n/[a-z]|\Z)', translated)
                    if ip_block_match:
                        ip_header = ip_block_match.group(1)
                        ip_content = ip_block_match.group(2)
                        # Append missing IPs to existing content
                        translated = translated.replace(ip_header + ip_content, ip_header + ip_content + '\n' + '\n'.join(missing_ips))
                    else:
                        # Section exists but no content - add all missing IPs
                        translated = translated.replace('/ip address', '/ip address\n' + '\n'.join(missing_ips))
                else:
                    # Create /ip address block
                    if '/ip firewall' in translated:
                        translated = translated.replace('/ip firewall', '/ip address\n' + '\n'.join(missing_ips) + '\n\n/ip firewall')
                    elif '/ip pool' in translated:
                        translated = translated.replace('/ip pool', '/ip address\n' + '\n'.join(missing_ips) + '\n\n/ip pool')
                    else:
                        # Insert after /interface section
                        if '/interface' in translated:
                            interface_end = translated.rfind('/interface')
                            next_section = translated.find('\n/', interface_end + 1)
                            if next_section > 0:
                                translated = translated[:next_section] + '\n/ip address\n' + '\n'.join(missing_ips) + '\n' + translated[next_section:]
                            else:
                                translated += '\n/ip address\n' + '\n'.join(missing_ips) + '\n'
                        else:
                            translated = '/ip address\n' + '\n'.join(missing_ips) + '\n\n' + translated
                
                print(f"[IP PRESERVATION] ✓ Re-injected {len(missing_ips)} missing IP addresses")
        else:
            print(f"[IP PRESERVATION] ✓ All {len(source_ip_set)} IP addresses are preserved")
        
        # Non-strict postprocessing is intended to repair broken AI output (dedup/orphan fixes/reordering).
        # In strict mode we avoid any step that might drop or reorder lines.
        if not strict_preserve:
            # CRITICAL: Remove all duplicates before final formatting
            print("[DEDUPLICATION] Removing duplicate entries from all sections...")
            translated = remove_duplicate_entries(translated)
             
            # FINAL CONSISTENCY VERIFICATION: Verify all critical sections exist
            required_sections = ['/ip address', '/interface ethernet', '/routing']
            missing_sections = []
            for section in required_sections:
                if section not in translated:
                    missing_sections.append(section)
                    print(f"[CONSISTENCY WARNING] Missing required section: {section}")
            
            # Verify BGP connections are in correct block (final check)
            if '/routing bgp connection' in translated:
                bgp_in_bfd = len(re.findall(r"(?m)^/routing bfd configuration\s+add\b[^\n]*(?:as=|remote\.address=)[^\n]*$", translated))
                if bgp_in_bfd > 0:
                    print(f"[CONSISTENCY WARNING] Found {bgp_in_bfd} BGP connections still in /routing bfd configuration block")
            
            # Verify interface mapping followed policy (check backhauls start at sfp28-4)
            backhaul_interfaces = re.findall(r"(?m)^/interface ethernet\s+set\s+[^\n]*default-name=(sfp28-[^\]]+)[^\n]*comment=([^\s\n\"]+)[^\n]*$", translated)
            for iface, comment in backhaul_interfaces:
                if any(keyword in comment.upper() for keyword in ['TX-', 'KS-', 'IL-', 'BACKHAUL', 'BH']):
                    iface_num = int(re.search(r'(\d+)', iface).group(1)) if re.search(r'(\d+)', iface) else None
                    if iface_num and iface_num < 4:
                        print(f"[CONSISTENCY WARNING] Backhaul '{comment}' on {iface} - should be sfp28-4+ per policy")

        # FINAL FORMATTING (always): spacing + safe kv whitespace normalization for readability.
        # This is intentionally deterministic and does not reorder or drop any config lines.
        translated = format_config_spacing(translated)

        # Validate translation (existing validation for IPs, secrets, users, firewall)
        validation = validate_translation(source_config, translated)
        
        # NEW: Check for missing critical sections
        is_complete, missing_sections, completeness_warnings = validate_translation_completeness(source_config, translated)
        if not is_complete:
            safe_print(f"[VALIDATION WARNING] Missing critical sections: {', '.join(missing_sections)}")
            validation['missing_sections'] = missing_sections
        if completeness_warnings:
            safe_print(f"[VALIDATION WARNING] {len(completeness_warnings)} warnings: {completeness_warnings[:3]}")
            validation['completeness_warnings'] = completeness_warnings
        
        # CRITICAL: If ANY IPs are missing, use intelligent fallback to preserve everything
        should_fallback = False
        fallback_reason = []
        
        if validation.get('missing_ips', []) and len(validation['missing_ips']) > 0:
            should_fallback = True
            fallback_reason.append(f"Missing {len(validation['missing_ips'])} IPs - CRITICAL: All IPs must be preserved")
            print(f"[CRITICAL] Missing IPs detected: {validation['missing_ips'][:10]}{'...' if len(validation['missing_ips']) > 10 else ''}")
        
        if validation.get('missing_secrets', []) and len(validation['missing_secrets']) > 0:
            should_fallback = True
            fallback_reason.append(f"Missing secrets ({len(validation['missing_secrets'])})")
        
        if validation.get('missing_users', []) and len(validation['missing_users']) > 0:
            should_fallback = True
            fallback_reason.append(f"Missing users ({len(validation['missing_users'])})")
        
        if validation.get('missing_sections', []) and len(validation['missing_sections']) > 0:
            should_fallback = True
            fallback_reason.append(f"Missing critical sections: {', '.join(validation['missing_sections'])}")
        
        if should_fallback:
            print(f"[INTELLIGENT FALLBACK] Information loss detected: {', '.join(fallback_reason)} - using intelligent translation to preserve all data")
            translated = apply_intelligent_translation(
                source_config,
                source_device_info,
                source_syntax_info,
                target_syntax_info,
                target_device_info,
                target_version,
                strict_preserve=strict_preserve,
            )
            validation = validate_translation(source_config, translated)
            
            # If still missing IPs after intelligent fallback, log warning but continue
            if validation.get('missing_ips', []) and len(validation['missing_ips']) > 0:
                print(f"[WARNING] Still missing {len(validation['missing_ips'])} IPs after intelligent fallback - manual review required")
        
        # FORCE INTELLIGENT FALLBACK FOR LARGE CONFIGS (prevent timeouts)
        config_size = len(source_config.split('\n'))
        if config_size > 500:  # Large configs
            print(f"[LARGE CONFIG] Detected {config_size} lines - using intelligent translation to prevent timeout")
            translated = apply_intelligent_translation(
                source_config,
                source_device_info,
                source_syntax_info,
                target_syntax_info,
                target_device_info,
                target_version,
                strict_preserve=strict_preserve,
            )
            validation = validate_translation(source_config, translated)

        # ========================================
        # RFC-09-10-25 COMPLIANCE ENFORCEMENT
        # ========================================
        # Extract loopback IP from translated config for compliance blocks
        loopback_ip = None
        loopback_match = re.search(r'/ip address\s+add address=([0-9.]+/[0-9]+)\s+interface=loop0', translated, re.IGNORECASE)
        if not loopback_match:
            # Try alternative patterns
            loopback_match = re.search(r'interface=loop0.*?address=([0-9.]+/[0-9]+)', translated, re.IGNORECASE)
        if loopback_match:
            loopback_ip = loopback_match.group(1)
        
        # If loopback not found, try to extract from source config
        if not loopback_ip:
            source_loopback_match = re.search(r'/ip address\s+add address=([0-9.]+/[0-9]+)\s+interface=loop0', source_config, re.IGNORECASE)
            if source_loopback_match:
                loopback_ip = source_loopback_match.group(1)
        
        # Apply compliance only if requested (optional)
        compliance_validation = None
        if apply_compliance and HAS_COMPLIANCE:
            print("[COMPLIANCE] Applying RFC-09-10-25 compliance standards (optional)...")
            try:
                # Get compliance blocks
                compliance_blocks = get_all_compliance_blocks(loopback_ip or "10.0.0.1/32")
                
                # Inject compliance into translated config
                translated = inject_compliance_blocks(translated, compliance_blocks)
                
                # Validate compliance
                compliance_validation = validate_compliance(translated)
                
                if not compliance_validation['compliant']:
                    print(f"[COMPLIANCE WARNING] {len(compliance_validation['missing_items'])} compliance items missing")
                else:
                    print("[COMPLIANCE] Configuration is compliant")
                    
            except Exception as e:
                print(f"[COMPLIANCE ERROR] Failed to apply compliance: {e}")
                compliance_validation = {'compliant': False, 'error': str(e)}

        # Final safety pass: enforce correct header/model, and ensure system identity isn't lost.
        def finalize_metadata(config_text: str) -> str:
            t = config_text or ''

            # Fix RouterOS header version line(s) (avoid 7.19.4.4.2 style hybrids).
            t = re.sub(r'(?m)^(#.*by RouterOS )\d+(?:\.\d+)+', rf'\g<1>{target_version}', t)
            t = re.sub(r'(?m)^(#.*RouterOS )\d+(?:\.\d+)+', rf'\g<1>{target_version}', t)

            # Fix export header model line to the exact target model.
            target_model_full = target_device_info.get('model', 'unknown')
            if target_model_full and target_model_full != 'unknown':
                t = re.sub(r'(?m)^#\s*model\s*=.*$', f"# model ={target_model_full}", t)

            # Ensure system identity exists.
            if '/system identity' not in t:
                # Try to reuse source identity name (updated to match target digits/model where possible).
                name = None
                m = re.search(r'(?ms)^/system identity\s*\n\s*set\s+name=([^\n]+)\s*$', source_config)
                if m:
                    name = m.group(1).strip().strip('"').strip("'")

                if not name:
                    name = f"RTR-{(target_device_info.get('type') or target_device).upper()}-UNKNOWN"

                # Update MT#### identity tokens if present.
                src_digits = re.search(r'(\d{3,4})', source_device_info.get('model', '') or '')
                tgt_digits = re.search(r'(\d{3,4})', target_device_info.get('model', '') or '')
                if src_digits and tgt_digits:
                    name = re.sub(rf'(?i)\bMT{re.escape(src_digits.group(1))}\b', f"MT{tgt_digits.group(1)}", name)

                # Update CCR#### tokens if present.
                old_model_short = (source_device_info.get('model', 'unknown').split('-')[0] if source_device_info.get('model') else '').strip()
                new_model_short = (target_device_info.get('model', 'unknown').split('-')[0] if target_device_info.get('model') else '').strip()
                if old_model_short and new_model_short and old_model_short != 'unknown' and new_model_short != 'unknown':
                    name = re.sub(rf'(?i)\b{re.escape(old_model_short)}\b', new_model_short, name)

                identity_block = f"/system identity\nset name={name}\n"
                marker = '# ========================================\n# RFC-09-10-25 COMPLIANCE STANDARDS'
                idx = t.find(marker)
                if idx != -1:
                    prefix = t[:idx].rstrip() + "\n\n" + identity_block + "\n"
                    suffix = t[idx:]
                    t = prefix + suffix.lstrip()
                else:
                    t = t.rstrip() + "\n\n" + identity_block

            return t

        translated = _postprocess_translated(finalize_metadata(translated))
        translated = _enforce_target_interfaces(translated)

        return jsonify({
            'success': True,
            'translated_config': translated,
            'validation': validation,
            'compliance': compliance_validation,
            'source_info': source_info,
            'target_info': target_info,
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ========================================
# ENDPOINT 4: Apply Compliance to Config
# ========================================

@app.route('/api/apply-compliance', methods=['POST'])
def apply_compliance():
    """
    Apply RFC-09-10-25 compliance standards to a RouterOS configuration.
    Used by both Non-MPLS and MPLS Enterprise config generators.
    
    IMPORTANT: This endpoint is ADDITIVE and NON-DESTRUCTIVE:
    - Adds compliance blocks without removing existing configurations
    - Skips frontend-only tabs (Tarana, 6GHz) that are production-ready
    - Preserves all tab-specific functionality
    - Does not override existing firewall rules, IP services, or other configs
    """
    try:
        data = request.get_json(force=True)
        config = data.get('config', '')
        loopback_ip = data.get('loopback_ip', '')
        
        if not config:
            return jsonify({'success': False, 'error': 'No configuration provided'}), 400
        
        if not HAS_COMPLIANCE:
            return jsonify({
                'success': True,
                'config': config,
                'compliance': {'compliant': False, 'error': 'Compliance reference not available'}
            })
        
        # Check if this is a frontend-only tab (Tarana, 6GHz) - these are production-ready
        config_lower = config.lower()
        is_tarana_config = ('tarana' in config_lower or 'sector' in config_lower or ('alpha' in config_lower and 'beta' in config_lower))
        is_6ghz_config = ('6ghz' in config_lower or '6ghz switch' in config_lower)
        
        if is_tarana_config or is_6ghz_config:
            print("[COMPLIANCE] Skipping compliance injection for frontend-only tab (Tarana/6GHz - production ready, self-contained)")
            return jsonify({
                'success': True,
                'config': config,
                'compliance': {
                    'compliant': True,
                    'note': 'Frontend-only tab (production ready) - compliance not needed'
                }
            })
        
        # Extract loopback IP from config if not provided
        if not loopback_ip:
            loopback_match = re.search(r'/ip address\s+add address=([0-9.]+/[0-9]+)\s+interface=loop0', config, re.IGNORECASE)
            if loopback_match:
                loopback_ip = loopback_match.group(1)
            else:
                # Try alternative pattern
                loopback_match = re.search(r'interface=loop0.*?address=([0-9.]+/[0-9]+)', config, re.IGNORECASE)
                if loopback_match:
                    loopback_ip = loopback_match.group(1)
        
        if not loopback_ip:
            loopback_ip = "10.0.0.1/32"  # Default fallback
        
        print(f"[COMPLIANCE] Applying RFC-09-10-25 compliance to configuration (additive, non-destructive)...")
        
        # Get compliance blocks
        compliance_blocks = get_all_compliance_blocks(loopback_ip)
        
        # Inject compliance into config (additive, preserves existing configs)
        compliant_config = inject_compliance_blocks(config, compliance_blocks)
        
        # Validate compliance
        compliance_validation = validate_compliance(compliant_config)
        
        if not compliance_validation['compliant']:
            print(f"[COMPLIANCE WARNING] {len(compliance_validation['missing_items'])} compliance items missing")
        else:
            print("[COMPLIANCE] ✅ Configuration is compliant")
        
        return jsonify({
            'success': True,
            'config': compliant_config,
            'compliance': compliance_validation
        })
        
    except Exception as e:
        print(f"[COMPLIANCE ERROR] Failed to apply compliance: {e}")
        return jsonify({
            'success': False,
            'error': str(e),
            'config': config  # Return original config on error - preserves functionality
        }), 500

# ========================================
# ENDPOINT 5: AI Config Explanation
# ========================================

@app.route('/api/explain-config', methods=['POST'])
def explain_config():
    """
    Explains what a config section does (for training/documentation)
    """
    try:
        data = request.json
        config_section = data.get('config', '')

        system_prompt = """You are a RouterOS configuration explainer.
Explain what each section does in simple terms for network administrators.
Include:
- Purpose of each command
- Security implications
- Performance impact
- RFC standards involved"""

        user_prompt = f"""Explain this RouterOS configuration:

```
{config_section}
```

Provide clear explanations for NOC staff."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        explanation = call_ai(messages, max_tokens=2000)

        return jsonify({
            'success': True,
            'explanation': explanation
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ========================================
# ENDPOINT 5: AI Auto-Fill From Export
# ========================================

@app.route('/api/autofill-from-export', methods=['POST'])
def autofill_from_export():
    """
    Parses an exported config and auto-fills the form fields
    Most useful feature for NOC workflow!
    """
    try:
        data = request.json
        exported_config = data.get('exported_config', '')
        target_form = data.get('target_form', 'tower')  # Which form to fill

        system_prompt = """You are a RouterOS configuration parser.
Extract relevant fields from an exported configuration and map them to form fields.

Return JSON format with extracted values:
{
  "site_name": "extracted value",
  "router_id": "x.x.x.x",
  "loopback_ip": "x.x.x.x/32",
  "uplinks": [{"interface": "ether1", "ip": "x.x.x.x/30"}],
  "ospf_area": "backbone",
  "bgp_as": "65000",
  "vlans": ["1000", "2000", "3000"],
  ...
}
"""

        user_prompt = f"""Parse this exported RouterOS configuration and extract values for the {target_form} form:

```
{exported_config}
```

Return JSON with all extractable fields."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        result = call_ai(messages, max_tokens=3000)

        try:
            parsed_fields = json.loads(result)
        except:
            parsed_fields = {"error": "Could not parse config", "raw": result}

        return jsonify({
            'success': True,
            'fields': parsed_fields
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ========================================
# ENDPOINT 6: Non‑MPLS Enterprise Generator
# ========================================

def _cidr_details_gen(cidr: str) -> dict:
    """
    Return network facts for an interface CIDR.

    Notes:
    - Users sometimes paste the subnet network address (e.g. `.200/29`). RouterOS cannot
      assign the network/broadcast address, so we normalize to the first usable host.
    - DHCP pools are generated as a contiguous range starting *after* the router IP
      (router_ip+1 .. last_host), matching the UI's "gateway+1" convention.
    """
    iface = ipaddress.ip_interface(cidr)
    net = iface.network
    hosts = list(net.hosts())

    first_host = str(hosts[0]) if hosts else str(net.network_address)
    last_host = str(hosts[-1]) if hosts else str(net.broadcast_address)

    router_ip_obj = iface.ip
    # For IPv4 /0-/30 networks, network/broadcast addresses are not usable on interfaces.
    if isinstance(net, ipaddress.IPv4Network) and net.prefixlen < 31 and hosts:
        if router_ip_obj == net.network_address or router_ip_obj == net.broadcast_address:
            router_ip_obj = hosts[0]

    router_ip = str(router_ip_obj)

    pool_start = ''
    pool_end = ''
    if hosts:
        try:
            idx = hosts.index(router_ip_obj)
        except ValueError:
            idx = -1
        if 0 <= idx < (len(hosts) - 1):
            pool_start = str(hosts[idx + 1])
            pool_end = last_host

    return {
        'network': str(net.network_address),
        'prefix': net.prefixlen,
        'router_ip': router_ip,
        'first_host': first_host,
        'last_host': last_host,
        'pool_start': pool_start,
        'pool_end': pool_end,
        'broadcast': str(net.broadcast_address),
    }


def _ros_quote(value: str) -> str:
    v = (value or "").replace('"', '\\"')
    return f"\"{v}\""

@app.route('/api/gen-enterprise-non-mpls', methods=['POST'])
@app.route('/api/gen-enterprise-Non-MPLS', methods=['POST'])  # Legacy UI alias
def gen_enterprise_non_mpls():
    try:
        data = request.get_json(force=True)
        device = (data.get('device') or 'RB5009').upper()
        target_version = data.get('target_version', '7.19.4')
        public_cidr = data['public_cidr']
        bh_cidr = data['bh_cidr']
        loopback_ip = data['loopback_ip']  # /32 expected
        uplink_if = data.get('uplink_interface', 'sfp-sfpplus1')
        public_port = data.get('public_port', 'ether7')
        nat_port = data.get('nat_port', 'ether8')
        # Use environment variables or form data - RFC-09-10-25 Compliance defaults
        # Default to NextLink compliance DNS servers (142.147.112.3, 142.147.112.19)
        dns1 = data.get('dns1') or os.getenv('NEXTLINK_DNS_PRIMARY', '142.147.112.3')
        dns2 = data.get('dns2') or os.getenv('NEXTLINK_DNS_SECONDARY', '142.147.112.19')
        if not dns1 or not dns2:
            return jsonify({'success': False, 'error': 'DNS servers must be configured. Set NEXTLINK_DNS_PRIMARY and NEXTLINK_DNS_SECONDARY environment variables or configure in nextlink_constants.js'}), 400
        snmp_community = data.get('snmp_community', 'CHANGE_ME')
        syslog_ip = data.get('syslog_ip')
        coords = data.get('coords')
        identity = data.get('identity', f"RTR-{device}.AUTO-GEN")
        uplink_comment = data.get('uplink_comment', '').strip()  # Uplink comment/location for backhaul

        pub = _cidr_details_gen(public_cidr)
        bh = _cidr_details_gen(bh_cidr)
        private_cidr = data.get('private_cidr', '')  # e.g., 192.168.88.1/24
        private_ip_range = data.get('private_pool', '')  # e.g., 192.168.88.10-192.168.88.254
        
        if not syslog_ip:
            syslog_ip = loopback_ip.split('/')[0]
        
        # Determine speed syntax based on RouterOS version
        def get_speed_syntax(version):
            """Determine speed syntax based on RouterOS version"""
            if version.startswith('7.16') or version.startswith('7.19'):
                return '1G-baseX'  # For SFP ports
            return '1G-baseX'  # Default
        
        speed_syntax = get_speed_syntax(target_version)
        loopback_ip_clean = loopback_ip.replace('/32', '').strip()
        
        # Parse private CIDR if provided
        private_network = ''
        private_gateway = ''
        if private_cidr:
            private_parts = private_cidr.split('/')
            if len(private_parts) == 2:
                private_ip = private_parts[0]
                private_prefix = private_parts[1]
                private_net = _cidr_details_gen(private_cidr)
                private_network = private_net['network']
                private_gateway = private_ip  # Use provided IP as gateway
        
        # Use standard reference blocks if available
        standard_blocks = {}
        if HAS_REFERENCE:
            try:
                standard_blocks = get_all_standard_blocks()
            except Exception as e:
                print(f"[WARN] Could not load standard blocks: {e}")
        
        # Build config blocks in proper order
        blocks = []
        
        # System Identity
        blocks.append(f"/system identity\nset name={identity}\n")
        
        # Queue Type
        blocks.append("/queue type\nset default-small pfifo-limit=50\n")
        
        # Interface Bridge
        blocks.append("/interface bridge\nadd name=loop0\nadd name=nat-bridge priority=0x1\nadd name=public-bridge priority=0x1\n")
        
        # Interface Ethernet (with speed for uplink if SFP)
        ethernet_block = f"/interface ethernet\n"
        ethernet_block += f"set [ find default-name={public_port} ] comment=\"CX HANDOFF\"\n"
        ethernet_block += f"set [ find default-name={nat_port} ] comment=NAT\n"
        # Add uplink interface comment - Use uplink comment if provided, otherwise use identity
        uplink_comment_value = uplink_comment if uplink_comment else identity
        uplink_comment_ros = _ros_quote(uplink_comment_value)
        if uplink_if.startswith('sfp'):
            # Determine speed based on RouterOS version
            speed = get_speed_syntax(target_version)
            ethernet_block += f"set [ find default-name={uplink_if} ] auto-negotiation=no comment={uplink_comment_ros} speed={speed}\n"
        else:
            # Non-SFP port - still add comment
            ethernet_block += f"set [ find default-name={uplink_if} ] comment={uplink_comment_ros}\n"
        blocks.append(ethernet_block)
        
        # Interface Bridge Port
        blocks.append("/interface bridge port\n" +
                      f"add bridge=public-bridge interface={public_port}\n" +
                      f"add bridge=nat-bridge interface={nat_port}\n")
        
        # IP Addresses (with proper network calculation)
        ip_block = "/ip address\n"
        ip_block += f"add address={loopback_ip_clean} comment=loop0 interface=loop0 network={loopback_ip_clean}\n"
        
        # Public IP - normalize network/broadcast to first usable host
        pub_router_ip = pub['router_ip']
        pub_network = pub['network']
        ip_block += f"add address={pub_router_ip}/{pub['prefix']} comment=\"PUBLIC(S)\" interface=public-bridge network={pub_network}\n"
        
        # Private IP - use provided or calculate
        private_base = ''  # Initialize for use in firewall NAT later
        if private_cidr and private_gateway:
            ip_block += f"add address={private_gateway}/{private_parts[1]} comment=PRIVATES interface=nat-bridge network={private_network}\n"
            # Extract base for firewall NAT
            private_base = private_network.rsplit('.', 1)[0] if '.' in private_network else private_network.rsplit('/', 1)[0].rsplit('.', 1)[0]
        else:
            # Fallback to calculated private
            private_base = pub['first_host'].rsplit('.', 1)[0]
            ip_block += f"add address={private_base}.1/24 comment=PRIVATES interface=nat-bridge network={private_base}.0\n"
        
        # Backhaul IP address - normalize network/broadcast to first usable host
        bh_router_ip = bh['router_ip']
        bh_network = bh['network']
        ip_block += f"add address={bh_router_ip}/{bh['prefix']} comment={uplink_comment_ros} interface={uplink_if} network={bh_network}\n"
        blocks.append(ip_block)
        
        # IP Pool
        pool_block = "/ip pool\n"
        # Public pool - use provided range or calculate
        # For /30: pool_start and pool_end are the same (customer IP only, excluding gateway)
        if pub.get('pool_start') and pub.get('pool_end'):
            if pub['pool_start'] == pub['pool_end']:
                # Single IP pool (e.g., /30 networks)
                pool_block += f"add name=public ranges={pub['pool_start']}\n"
            else:
                # Range pool
                pool_block += f"add name=public ranges={pub['pool_start']}-{pub['pool_end']}\n"
        else:
            pool_block += f"add name=public ranges={pub['router_ip']}\n"
        # Private pool
        if private_ip_range:
            pool_block += f"add name=private ranges={private_ip_range}\n"
        else:
            private_base = pub['first_host'].rsplit('.', 1)[0]
            pool_block += f"add name=private ranges={private_base}.10-{private_base}.254\n"
        blocks.append(pool_block)
        
        # Queue Tree
        blocks.append("/queue tree\n" +
                      f"add max-limit=200M name=UPLOAD parent={uplink_if}\n" +
                      "add max-limit=200M name=DOWNLOAD-PUB parent=public-bridge\n" +
                      "add max-limit=200M name=DOWNLOAD-NAT parent=nat-bridge\n" +
                      "add name=VOIP-UP packet-mark=VOIP parent=UPLOAD priority=1\n" +
                      "add name=VOIP-DOWN-PUB packet-mark=VOIP parent=DOWNLOAD-PUB priority=1\n" +
                      "add name=VOIP-DOWN-NAT packet-mark=VOIP parent=DOWNLOAD-NAT priority=1\n" +
                      "add name=ALL-DOWN-PUB packet-mark=ALL parent=DOWNLOAD-PUB\n" +
                      "add name=ALL-DOWN-NAT packet-mark=ALL parent=DOWNLOAD-NAT\n" +
                      "add name=ALL-UP packet-mark=ALL parent=UPLOAD\n")
        
        # IP Neighbor Discovery (tab-specific, not compliance)
        if standard_blocks.get('ip_neighbor_discovery'):
            blocks.append(standard_blocks['ip_neighbor_discovery'])
        
        # DHCP Server (tab-specific configuration)
        blocks.append("/ip dhcp-server\n" +
                      f"add address-pool=public interface=public-bridge lease-time=1h name=public-server\n" +
                      f"add address-pool=private interface=nat-bridge lease-time=1h name=nat-server\n")
        
        # DHCP Server Network (tab-specific configuration)
        dhcp_net_block = "/ip dhcp-server network\n"
        # Public DHCP network
        pub_gateway = pub_router_ip
        # Add dhcp-option-set=optset if address doesn't start with "10."
        pub_dhcp_optset = "" if pub_network.startswith("10.") else " dhcp-option-set=optset"
        dhcp_net_block += f"add address={pub_network}/{pub['prefix']} dns-server={dns1},{dns2} gateway={pub_gateway}{pub_dhcp_optset}\n"
        # Private DHCP network
        if private_cidr and private_gateway:
            # Add dhcp-option-set=optset if address doesn't start with "10."
            private_dhcp_optset = "" if private_network.startswith("10.") else " dhcp-option-set=optset"
            dhcp_net_block += f"add address={private_network}/{private_parts[1]} comment=PRIVATES dns-server={dns1},{dns2} gateway={private_gateway} netmask={private_parts[1]}{private_dhcp_optset}\n"
        else:
            private_base = pub['first_host'].rsplit('.', 1)[0]
            # Add dhcp-option-set=optset if address doesn't start with "10."
            private_dhcp_optset = "" if private_base.startswith("10.") else " dhcp-option-set=optset"
            dhcp_net_block += f"add address={private_base}.0/24 comment=PRIVATES dns-server={dns1},{dns2} gateway={private_base}.1 netmask=24{private_dhcp_optset}\n"
        blocks.append(dhcp_net_block)
        
        # Firewall NAT (tab-specific rules - NTP, private NAT)
        # NOTE: Compliance will add additional NAT rules, but these are tab-specific
        blocks.append("/ip firewall nat\n" +
                      f"add action=src-nat chain=srcnat packet-mark=NTP to-addresses={loopback_ip_clean}\n" +
                      f"add action=src-nat chain=srcnat src-address={private_base}.0/24 to-addresses={pub_router_ip}\n")
        
        # Firewall Service Port (tab-specific)
        blocks.append("/ip firewall service-port\nset sip disabled=yes\n")
        
        # IP Route (tab-specific configuration)
        # Default route gateway should be a *neighbor* on the backhaul subnet (not the subnet network address).
        bh_net = ipaddress.ip_interface(bh_cidr).network
        bh_hosts = list(bh_net.hosts())
        if bh_hosts:
            bh_gateway = str(bh_hosts[0]) if str(bh_hosts[0]) != bh_router_ip else (str(bh_hosts[1]) if len(bh_hosts) > 1 else bh_router_ip)
        else:
            bh_gateway = bh_router_ip

        # Optional override: accept only a single IP (no CIDR) that isn't the router IP.
        gw_override = (data.get('gateway_ip') or '').strip()
        if gw_override and '/' not in gw_override:
            try:
                gw_ip = ipaddress.ip_address(gw_override)
                if gw_ip in bh_net and str(gw_ip) != bh_router_ip:
                    bh_gateway = str(gw_ip)
            except ValueError:
                pass
        blocks.append(f"/ip route\nadd disabled=no distance=1 dst-address=0.0.0.0/0 gateway={bh_gateway} routing-table=main scope=30 suppress-hw-offload=no target-scope=10\n")
        
        # SNMP (tab-specific - location and basic settings)
        # NOTE: Compliance will add SNMP community and additional settings
        loc = f" location=\"{coords}\"" if coords else ""
        blocks.append(f"/snmp\nset enabled=yes src-address={loopback_ip_clean} trap-community={snmp_community}{loc}\n")
        
        # NOTE: Compliance script will handle:
        # - /ip firewall address-list (all lists)
        # - /ip firewall filter (all input/forward rules)
        # - /ip firewall raw (all raw rules)
        # - /ip dns (DNS servers)
        # - /ip service (all service settings)
        # - /system logging action and /system logging (logging configuration)
        # - /user group (all user groups)
        # - /user aaa (RADIUS settings)
        # - /system clock and /system ntp client (time/NTP)
        # - /system routerboard settings (auto-upgrade)
        # - Additional firewall NAT rules (unauth proxy, SSH redirect)
        # - Firewall mangle rules
        # - DHCP options
        # - RADIUS configuration
        # - LDP filters

        # Join blocks with double newline, but ensure no trailing newlines in blocks
        cleaned_blocks = [block.rstrip('\n\r') for block in blocks if block.strip()]
        cfg = "\n\n".join(cleaned_blocks)
        
        # ========================================
        # RFC-09-10-25 COMPLIANCE ENFORCEMENT
        # ========================================
        if HAS_COMPLIANCE:
            try:
                print("[COMPLIANCE] Adding RFC-09-10-25 compliance to new configuration...")
                compliance_blocks = get_all_compliance_blocks(loopback_ip)
                cfg = inject_compliance_blocks(cfg, compliance_blocks)
                
                # Validate compliance
                compliance_validation = validate_compliance(cfg)
                if not compliance_validation['compliant']:
                    print(f"[COMPLIANCE WARNING] {len(compliance_validation['missing_items'])} compliance items missing")
                else:
                    print("[COMPLIANCE] Configuration is compliant")
            except Exception as e:
                print(f"[COMPLIANCE ERROR] Failed to add compliance: {e}")
        
        # Normalize and deduplicate configuration before returning
        cfg = normalize_config(cfg)
        return jsonify({'success': True, 'config': cfg, 'device': device, 'version': target_version})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 400

# ========================================
# HELPER FUNCTIONS
# ========================================

def get_syntax_rules(target_version):
    """Returns syntax change rules for target RouterOS version"""
    if target_version.startswith('7.'):
        return """V7 KEY CHANGES:
- OSPF: Use /routing ospf interface-template (not /routing ospf interface)
- BGP: Use /routing bgp connection (not /routing bgp peer)
- MOST SYNTAX STAYS THE SAME - only change if broken"""
    return "Keep syntax as-is"

def inject_compliance_blocks(config: str, compliance_blocks: dict) -> str:
    """
    Intelligently inject compliance blocks into a RouterOS configuration.
    Checks if compliance was already applied to avoid duplicates.
    
    IMPORTANT: Compliance blocks use RouterOS 'rem' commands to remove existing entries
    and then re-add them with compliance standards. If compliance section already exists,
    we skip to avoid duplication.
    
    Args:
        config: Existing RouterOS configuration
        compliance_blocks: Dictionary of compliance blocks from get_all_compliance_blocks()
        
    Returns:
        Updated configuration with compliance blocks (only if not already present)
    """
    # Check if config is from a frontend-only tab (Tarana, 6GHz) that shouldn't get compliance
    config_lower = config.lower()
    is_tarana_config = ('tarana' in config_lower or 'sector' in config_lower or 'alpha' in config_lower or 'beta' in config_lower or 'gamma' in config_lower)
    is_6ghz_config = ('6ghz' in config_lower or '6ghz switch' in config_lower or 'vlan3000' in config_lower or 'vlan4000' in config_lower)
    
    if is_tarana_config or is_6ghz_config:
        print("[COMPLIANCE] Skipping compliance injection for frontend-only tab (Tarana/6GHz - production ready)")
        return config
    
    # Check if compliance section already exists (to avoid double-injection)
    # Look for the compliance header comment in the last 3000 characters (where it would be appended)
    if "# RFC-09-10-25 COMPLIANCE STANDARDS" in config or "RFC-09-10-25 COMPLIANCE STANDARDS" in config[-3000:]:
        print("[COMPLIANCE] Compliance section already exists, skipping duplicate injection")
        return config
    
    # Append compliance blocks at the end (they use 'rem' commands to handle existing entries)
    compliance_section = "\n\n# ========================================\n# RFC-09-10-25 COMPLIANCE STANDARDS\n# ========================================\n# These blocks ensure NextLink policy compliance\n# They use 'rem' commands to remove existing entries and re-add with compliance standards\n# ========================================\n\n"
    
    # Add compliance blocks in order
    compliance_order = [
        'ip_services', 'dns', 'firewall_address_lists', 
        'firewall_filter_input', 'firewall_raw', 'firewall_forward',
        'firewall_nat', 'firewall_mangle', 'clock_ntp', 'snmp',
        'system_settings', 'vpls_edge', 'logging', 'user_aaa',
        'user_groups', 'dhcp_options', 'radius', 'ldp_filters'
    ]
    
    for key in compliance_order:
        if key in compliance_blocks:
            compliance_section += f"# {key.upper().replace('_', ' ')}\n"
            compliance_section += compliance_blocks[key]
            compliance_section += "\n\n"
    
    return config.rstrip() + "\n" + compliance_section

def validate_translation(source, translated):
    """Comprehensive validation to ensure all important information is preserved.
    Validates IP addresses, passwords, users, firewall rules, and routing configs.
    Normalizes /32 to bare IP to avoid false negatives on ROS7 fields like remote.address.
    """

    def strip_noise(text: str) -> str:
        # Remove router prompts
        text = re.sub(r"(?m)^\s*\[[^\]]+\]\s*", "", text)
        # Drop full-line comments
        text = re.sub(r"(?m)^\s*#.*$", "", text)
        # Remove /system script blocks (very noisy and may embed many IPs in strings)
        lines = text.splitlines()
        out = []
        in_script = False
        for l in lines:
            if l.startswith('/system script'):
                in_script = True
                continue
            if in_script and l.startswith('/'):
                in_script = False
            if in_script:
                continue
            out.append(l)
        return "\n".join(out)

    def extract_ips(text: str) -> set[str]:
        text = strip_noise(text)
        ips = set(re.findall(r"\b(?:\d{1,3}\.){3}\d{1,3}(?:/\d{1,2})?\b", text))
        # Normalize /32 to bare IP and filter out obviously invalid IPs
        norm = set()
        for ip in ips:
            # Skip common false positives
            if ip.startswith('0.0.') or ip.startswith('255.255.255.'):
                continue
            # Normalize /32 to bare IP
            base_ip = ip[:-3] if ip.endswith('/32') else ip
            try:
                # Validate it's a real IP
                ipaddress.ip_address(base_ip.split('/')[0])
                norm.add(base_ip)
            except:
                pass
        return norm

    source_ips = extract_ips(source)
    translated_ips = extract_ips(translated)
    missing_ips = source_ips - translated_ips
    
    # Additional check: Are IPs present but with different CIDR notation?
    # e.g., source has 10.1.1.1/24, translated has 10.1.1.1/32
    actually_missing = []
    for ip in missing_ips:
        # Extract base IP without CIDR
        base = ip.split('/')[0]
        # Check if base IP exists in translated with any CIDR
        found = False
        for trans_ip in translated_ips:
            if trans_ip.startswith(base):
                found = True
                break
        if not found:
            actually_missing.append(ip)
    
    # Update missing_ips to only include truly missing ones
    if len(actually_missing) < len(missing_ips):
        print(f"[VALIDATION] {len(missing_ips) - len(actually_missing)} IPs found with different CIDR notation - not counting as missing")
        missing_ips = set(actually_missing)
    
    # Additional validation: Check for preserved passwords/secrets
    def extract_secrets(text: str) -> set:
        # Extract password=, secret=, auth-key= values (but not the actual values for security)
        secrets = set(re.findall(r'\b(password|secret|auth-key|auth-id)=[^\s]+', text, re.IGNORECASE))
        return secrets
    
    source_secrets = extract_secrets(source)
    translated_secrets = extract_secrets(translated)
    missing_secrets = source_secrets - translated_secrets
    
    # Check for preserved user accounts
    def extract_users(text: str) -> set:
        users = set(re.findall(r'/user\s+(?:add|set)\s+name=([^\s]+)', text, re.IGNORECASE))
        return users
    
    source_users = extract_users(source)
    translated_users = extract_users(translated)
    missing_users = source_users - translated_users
    
    # Check firewall rule count preservation
    source_fw_rules = len(re.findall(r'/ip firewall\s+(?:filter|nat|mangle|raw)\s+(?:add|set)', source, re.IGNORECASE))
    translated_fw_rules = len(re.findall(r'/ip firewall\s+(?:filter|nat|mangle|raw)\s+(?:add|set)', translated, re.IGNORECASE))
    
    is_valid = len(missing_ips) == 0 and len(missing_secrets) == 0 and len(missing_users) == 0
    
    validation_result = {
        "valid": is_valid,
        "source_ip_count": len(source_ips),
        "translated_ip_count": len(translated_ips),
        "missing_ips": sorted(list(missing_ips)),
        "source_secret_count": len(source_secrets),
        "translated_secret_count": len(translated_secrets),
        "missing_secrets": sorted(list(missing_secrets)),
        "source_user_count": len(source_users),
        "translated_user_count": len(translated_users),
        "missing_users": sorted(list(missing_users)),
        "source_firewall_rules": source_fw_rules,
        "translated_firewall_rules": translated_fw_rules,
        "firewall_rules_preserved": translated_fw_rules >= source_fw_rules * 0.95  # Allow 5% tolerance for consolidation
    }
    
    if not is_valid:
        print(f"[VALIDATION WARNING] Missing information detected:")
        if missing_ips:
            print(f"  - Missing IPs: {len(missing_ips)}")
        if missing_secrets:
            print(f"  - Missing secrets: {len(missing_secrets)}")
        if missing_users:
            print(f"  - Missing users: {len(missing_users)}")
        if not validation_result["firewall_rules_preserved"]:
            print(f"  - Firewall rules: {source_fw_rules} → {translated_fw_rules}")
    
    return validation_result

# ========================================
# ENDPOINT 5: Tarana Config Generation & Validation
# ========================================

def validate_tarana_config(config_text, device, routeros_version):
    """
    Validates Tarana sector configuration for accuracy and device compatibility.
    Returns (is_valid, errors, warnings)
    """
    errors = []
    warnings = []
    
    if not config_text or len(config_text.strip()) < 50:
        errors.append("Configuration is empty or too short")
        return False, errors, warnings
    
    # Validate required sections exist
    required_sections = [
        '/interface bridge',
        '/interface ethernet',
        '/interface vlan',
        '/interface bridge port'
    ]
    missing_sections = []
    for section in required_sections:
        if section not in config_text:
            missing_sections.append(section)
    
    if missing_sections:
        errors.append(f"Missing required sections: {', '.join(missing_sections)}")
    
    # ============================================================
    # BNG1 FORMAT VALIDATION - CRITICAL FOR LIVE DEVICES
    # ============================================================
    # BNG1 Format Requirements (IMMUTABLE):
    # - Bridge name: bridge3000 (NOT UNICORNMGMT)
    # - IP interface: bridge3000
    # - OSPF interface: bridge3000
    # - NO path-cost parameters
    # - NO port-cost-mode
    # ============================================================
    
    # Validate UNICORNMGMT IP address line exists and is correct
    # BNG1 MUST use bridge3000 interface (not UNICORNMGMT)
    # Support both quoted and unquoted comments: comment="UNICORN MGMT" or comment=UNICORNMGMT
    unicorn_ip_match = re.search(r'add address=(\d+\.\d+\.\d+\.\d+)/(\d+)\s+comment=([^\s"]+|"[^"]+")\s+interface=bridge3000\s+network=(\d+\.\d+\.\d+\.\d+)', config_text)
    if not unicorn_ip_match:
        errors.append("Missing or malformed UNICORNMGMT IP address line")
    else:
        gateway_ip = unicorn_ip_match.group(1)
        prefix = unicorn_ip_match.group(2)
        network_addr = unicorn_ip_match.group(4)  # network is group 4
        
        # Validate IP address format
        try:
            import ipaddress
            ipaddress.IPv4Address(gateway_ip)
            ipaddress.IPv4Address(network_addr)
            prefix_int = int(prefix)
            if prefix_int < 8 or prefix_int > 30:
                errors.append(f"Invalid prefix length: /{prefix} (must be between /8 and /30)")
        except ValueError as e:
            errors.append(f"Invalid IP address format: {str(e)}")
        
        # Validate network calculation (Nextlink convention: network = IP - 1)
        try:
            import ipaddress
            gateway_obj = ipaddress.IPv4Address(gateway_ip)
            expected_network = str(ipaddress.IPv4Address(int(gateway_obj) - 1))
            if network_addr != expected_network:
                errors.append(f"Network address calculation incorrect: expected {expected_network} (IP-1), got {network_addr}")
        except Exception as e:
            errors.append(f"Network calculation validation error: {str(e)}")
    
    # Validate OSPF interface-template exists and uses correct network
    # Format: /routing ospf interface-template add interfaces=bridge3000 ... network=10.243.211.112/29
    ospf_match = re.search(r'/routing ospf interface-template.*?network=(\d+\.\d+\.\d+\.\d+/\d+)', config_text, re.DOTALL)
    if not ospf_match:
        warnings.append("OSPF interface-template for UNICORNMGMT not found")
    else:
        ospf_network = ospf_match.group(1)
        # Verify OSPF uses network address, not gateway IP
        if unicorn_ip_match:
            gateway_ip = unicorn_ip_match.group(1)
            if gateway_ip in ospf_network:
                errors.append("OSPF network parameter should use network address (IP-1), not gateway IP")
    
    # Validate RouterOS syntax for specified version
    if routeros_version.startswith('7.'):
        # RouterOS 7.x uses interface-template, not interface
        if '/routing ospf interface add' in config_text and '/routing ospf interface-template' not in config_text:
            errors.append("RouterOS 7.x requires /routing ospf interface-template, not /routing ospf interface")
    elif routeros_version.startswith('6.'):
        # RouterOS 6.x uses interface, not interface-template
        if '/routing ospf interface-template' in config_text and '/routing ospf interface add' not in config_text:
            warnings.append("RouterOS 6.x typically uses /routing ospf interface, not interface-template")
    
    # Validate bridge port assignments
    bridge_port_count = len(re.findall(r'/interface bridge port\s+add', config_text))
    vlan_count = len(re.findall(r'/interface vlan\s+add', config_text))
    if bridge_port_count == 0:
        errors.append("No bridge ports configured")
    if vlan_count == 0:
        errors.append("No VLAN interfaces configured")
    
    # Validate device compatibility
    if device.lower() not in ['ccr2004', 'ccr2216']:
        warnings.append(f"Tarana sectors are typically configured on CCR2004 or CCR2216, not {device}")
    
    is_valid = len(errors) == 0
    return is_valid, errors, warnings

@app.route('/api/gen-ftth-bng', methods=['POST'])
def gen_ftth_bng():
    """Deprecated FTTH endpoint. Use /api/generate-ftth-bng with full payload."""
    try:
        data = request.get_json(force=True) or {}

        required_full = [
            'loopback_ip',
            'cpe_network',
            'cgnat_private',
            'cgnat_public',
            'unauth_network',
            'olt_network'
        ]
        if all(data.get(k) for k in required_full):
            config = render_ftth_config(data)
            return jsonify({'success': True, 'config': config})

        return jsonify({
            'success': False,
            'error': 'FTTH generator now requires full FTTH fields (CGNAT Public, UNAUTH, OLT name, location). Use the FTTH BNG tab and /api/generate-ftth-bng.'
        }), 400
    except Exception as exc:
        print(f"[FTTH BNG] Error generating ftth bng: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 500


@app.route('/api/preview-ftth-bng', methods=['POST','OPTIONS'])
def preview_ftth_bng():
    """Return parsed FTTH CIDR details for previewing in the UI."""
    try:
        # Respond to preflight / OPTIONS gracefully
        if request.method == 'OPTIONS':
            return jsonify({'success': True}), 200
        data = request.get_json(force=True)
        loopback_ip = data.get('loopback_ip')
        cpe_cidr = data.get('cpe_cidr')
        cgnat_cidr = data.get('cgnat_cidr')
        olt_cidr = data.get('olt_cidr')

        if not (loopback_ip and cpe_cidr and cgnat_cidr and olt_cidr):
            return jsonify({'success': False, 'error': 'Missing one of required CIDR params (loopback_ip, cpe_cidr, cgnat_cidr, olt_cidr)'}), 400

        try:
            olt_info = _cidr_details_gen(olt_cidr)
            cpe_info = _cidr_details_gen(cpe_cidr)
            cgnat_info = _cidr_details_gen(cgnat_cidr)
        except Exception as e:
            return jsonify({'success': False, 'error': f'Invalid CIDR provided: {e}'}), 400

        preview = {
            'loopback': loopback_ip,
            'olt': olt_info,
            'cpe': cpe_info,
            'cgnat': cgnat_info,
            'suggested_nat_comment': 'FTTH-CPE-NAT',
            'note': 'Preview only - use Generate to produce full configuration'
        }
        return jsonify({'success': True, 'preview': preview})
    except Exception as exc:
        print(f"[FTTH BNG] Preview error: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 500


@app.route('/api/gen-tarana-config', methods=['POST'])
def gen_tarana_config():
    """
    Generates and validates Tarana sector configuration with AI-powered network calculation.
    Only returns success if configuration is accurate and device-ready.
    """
    try:
        data = request.get_json(force=True)
        raw_config = data.get('config', '')
        device = data.get('device', 'ccr2004')
        routeros_version = data.get('routeros_version', '7.19.4')
        
        if not raw_config:
            # Return error - we need a config to validate
            return jsonify({
                'success': False,
                'error': 'No configuration provided',
                'config': '',
                'device': device,
                'version': routeros_version
            }), 400
        
        # ============================================================
        # NETWORK CALCULATION - BNG1 FORMAT VALIDATION
        # ============================================================
        # BNG1 MUST use bridge3000 interface (not UNICORNMGMT)
        # Pattern: add address=IP/prefix comment=... interface=bridge3000 network=...
        # ============================================================
        unicorn_cidr_match = re.search(r'add address=(\d+\.\d+\.\d+\.\d+)/(\d+)\s+comment=([^\s]+)\s+interface=bridge3000\s+network=(\d+\.\d+\.\d+\.\d+)', raw_config)
        
        # CRITICAL: Reject if UNICORNMGMT interface is found (should be bridge3000)
        if re.search(r'interface=UNICORNMGMT', raw_config):
            print("[TARANA] ERROR: Config uses UNICORNMGMT interface - BNG1 must use bridge3000")
            return jsonify({
                'success': False,
                'error': 'Invalid format: BNG1 must use interface=bridge3000, not UNICORNMGMT',
                'config': raw_config
            }), 400
        if unicorn_cidr_match:
            user_ip = unicorn_cidr_match.group(1)
            prefix_len = int(unicorn_cidr_match.group(2)) if unicorn_cidr_match.group(2) else 29
            comment = unicorn_cidr_match.group(3)
            interface_name = 'bridge3000'  # BNG1 always uses bridge3000
            existing_network = unicorn_cidr_match.group(4)
            
            # Calculate network address using Nextlink convention: network = IP - 1
            try:
                import ipaddress
                ip_obj = ipaddress.IPv4Address(user_ip)
                network_obj = ipaddress.IPv4Address(int(ip_obj) - 1)
                correct_network = str(network_obj)
                
                print(f"[TARANA] User IP (Gateway): {user_ip}, Prefix: /{prefix_len}")
                print(f"[TARANA] Existing network: {existing_network}, Correct network: {correct_network}")
                print(f"[TARANA] Interface: {interface_name}")
                
                # Only fix if the network address is incorrect
                if existing_network != correct_network:
                    print(f"[TARANA] Network address incorrect, fixing from {existing_network} to {correct_network}")
                    # Fix /ip address line - replace only the network parameter
                    raw_config = re.sub(
                        r'(add address=' + re.escape(user_ip) + r'/' + str(prefix_len) + r'\s+comment=' + re.escape(comment) + r'\s+interface=' + re.escape(interface_name) + r'\s+network=)' + re.escape(existing_network),
                        r'\1' + correct_network,
                        raw_config
                    )
                else:
                    print(f"[TARANA] Network address is already correct: {correct_network}")
                
                # Fix OSPF network parameter if it exists and is incorrect
                # Format: /routing ospf interface-template add interfaces=... network=10.246.2.25/29
                # Remove any corrupted OSPF lines first (lines that start with just an IP/prefix)
                raw_config = re.sub(r'^[H\d]\.\d+\.\d+\.\d+/\d+\s+priority=\d+', '', raw_config, flags=re.MULTILINE)
                
                ospf_match = re.search(r'(/routing ospf interface-template.*?network=)(\d+\.\d+\.\d+\.\d+/\d+)', raw_config, re.DOTALL)
                if ospf_match:
                    ospf_network = ospf_match.group(2)
                    expected_ospf_network = f"{correct_network}/{prefix_len}"
                    if ospf_network != expected_ospf_network:
                        print(f"[TARANA] OSPF network incorrect, fixing from {ospf_network} to {expected_ospf_network}")
                        raw_config = re.sub(
                            r'(/routing ospf interface-template.*?network=)' + re.escape(ospf_network),
                            r'\1' + expected_ospf_network,
                            raw_config,
                            flags=re.DOTALL
                        )
                    else:
                        print(f"[TARANA] OSPF network is already correct: {expected_ospf_network}")
                
            except Exception as e:
                print(f"[TARANA] Network calculation error: {e}")
                # Don't modify config if calculation fails
        
        # Use AI to validate and enhance the configuration
        training_context = build_training_context()
        system_prompt = f"""You are a Nextlink NOC MikroTik RouterOS configuration expert specializing in Tarana sector configurations.

Your task:
1. **CRITICAL**: Fix network address calculations using proper CIDR subnet mathematics
2. Ensure proper RouterOS v{routeros_version} syntax
3. Verify all bridge port assignments are correct
4. Check VLAN naming consistency
5. Ensure proper formatting and spacing
6. Validate IP addresses and network calculations

**CRITICAL NETWORK CALCULATION RULES (NEXTLINK CONVENTION):**
- **Nextlink Convention**: Network = Gateway IP - 1
- Example: Gateway IP 10.246.21.64/29 → Network = 10.246.21.63
- The address parameter keeps the user's IP (gateway)
- The network parameter is always IP - 1
- OSPF networks parameter MUST use (IP - 1)/prefix, not the gateway IP
- This is a Nextlink-specific convention for UNICORNMGMT networks

**VERIFICATION:**
- Check /ip address lines: network= parameter must be the calculated network address
- Check /routing ospf interface-template: networks= parameter must be network_address/prefix (not IP_address/prefix)

Return ONLY the corrected RouterOS configuration with proper network addresses calculated. NO explanations, NO markdown code blocks, just pure RouterOS commands."""
        
        if training_context:
            system_prompt += "\n\n" + training_context
        
        user_prompt = f"""Validate and correct this Tarana sector configuration for {device.upper()} running RouterOS {routeros_version}:

{raw_config}

Return the corrected configuration with proper network calculations and formatting."""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # Always ensure we have a valid config to return
        corrected_config = raw_config  # Start with original config (may have been fixed above)
        
        # Only use AI if the config needs enhancement - don't break valid configs
        try:
            # Try AI validation - but don't fail if it doesn't work
            try:
                ai_result = call_ai(messages, max_tokens=4000, task_type='validation')
                if ai_result:
                    # Clean up any markdown formatting
                    cleaned = ai_result.replace('```routeros', '').replace('```', '').strip()
                    # Only use AI result if it's substantial and contains the required elements
                    if cleaned and len(cleaned) > 100:
                        # Verify AI result has the required UNICORNMGMT line
                        if 'UNICORNMGMT' in cleaned and '/ip address' in cleaned:
                            corrected_config = cleaned
                            print(f"[TARANA] AI validation successful ({len(corrected_config)} chars)")
                        else:
                            print(f"[TARANA] AI result missing required elements, using original config")
                    else:
                        print(f"[TARANA] AI result too short, using original config")
                else:
                    print(f"[TARANA] AI returned empty result, using original config")
            except Exception as ai_error:
                # AI failed - that's OK, use the original config (which may have been network-corrected)
                print(f"[TARANA] AI validation unavailable: {ai_error} - Using original config")
        except Exception as e:
            # Any other error - use original config
            print(f"[TARANA] Error during processing: {e} - Using original config")
        
        # Normalize and deduplicate before returning
        try:
            corrected_config = normalize_config(corrected_config)
        except Exception as norm_error:
            print(f"[TARANA] Normalization error: {norm_error} - Using config as-is")
            # Keep corrected_config as-is if normalization fails
        
        # CRITICAL: Validate the corrected config for accuracy
        is_valid, validation_errors, validation_warnings = validate_tarana_config(corrected_config, device, routeros_version)
        
        if not is_valid:
            # Config is not accurate - return error with details
            print(f"[TARANA] Validation failed: {validation_errors}")
            return jsonify({
                'success': False,
                'error': 'Configuration validation failed',
                'errors': validation_errors,
                'warnings': validation_warnings,
                'config': corrected_config,  # Still return config so user can see what was generated
                'device': device,
                'version': routeros_version
            }), 400
        
        # Config is valid - return success
        if validation_warnings:
            print(f"[TARANA] Validation passed with warnings: {validation_warnings}")
        
        return jsonify({
            'success': True,
            'config': corrected_config,
            'warnings': validation_warnings,  # Include warnings even on success
            'device': device,
            'version': routeros_version
        })
        
    except Exception as e:
        # Log error but return proper error response
        print(f"[TARANA] Critical error: {e}")
        import traceback
        traceback.print_exc()
        
        try:
            data = request.get_json(force=True)
            raw_config = data.get('config', '')
            device = data.get('device', 'ccr2004')
            routeros_version = data.get('routeros_version', '7.19.4')
            
            # Return error response - don't pretend everything is OK
            return jsonify({
                'success': False,
                'error': f'Backend processing error: {str(e)}',
                'config': raw_config if raw_config else '',
                'device': device,
                'version': routeros_version
            }), 500
        except:
            # Ultimate fallback - return error
            return jsonify({
                'success': False,
                'error': 'Critical backend error',
                'config': '',
                'device': 'ccr2004',
                'version': '7.19.4'
            }), 500

# ========================================
# SSH CONFIG FETCH
# ========================================

@app.route('/api/fetch-config-ssh', methods=['POST'])
def fetch_config_ssh():
    """
    SSH into MikroTik device and fetch configuration via export command.
    Credentials can be provided in the request body, or via environment variables.
    """
    try:
        import paramiko
    except ImportError:
        return jsonify({
            'error': 'paramiko library not installed. Run: pip install paramiko'
        }), 500
    
    try:
        data = request.get_json(force=True)
        host = data.get('host', '').strip()
        ros_version = data.get('ros_version', '7')
        command = data.get('command', '').strip()
        
        if not host:
            return jsonify({'error': 'Device IP address is required'}), 400
        
        # Basic IP validation
        try:
            ipaddress.IPv4Address(host)
        except ValueError:
            return jsonify({'error': 'Invalid IP address format'}), 400
        
        # Credentials
        # Prefer request-provided credentials (not stored), fallback to env vars.
        SSH_USERNAME = (data.get('username') or os.getenv('NEXTLINK_SSH_USERNAME', '')).strip()
        SSH_PASSWORD = (data.get('password') or os.getenv('NEXTLINK_SSH_PASSWORD', '')).strip()

        if not SSH_USERNAME or not SSH_PASSWORD:
            return jsonify({
                'error': 'SSH credentials required. Provide username/password or set NEXTLINK_SSH_USERNAME and NEXTLINK_SSH_PASSWORD on the server.'
            }), 400
        
        # MikroTik SSH ports: default try 22 first, then 5022 as fallback.
        # Allow UI to pass a comma-separated string or list via `ports`, and/or a single `port` override.
        default_ports = [22, 5022]

        def _parse_ports(value):
            ports = []
            if value is None:
                return ports
            if isinstance(value, (list, tuple)):
                tokens = value
            elif isinstance(value, str):
                tokens = re.split(r'[\s,]+', value.strip())
            else:
                tokens = [value]

            for token in tokens:
                try:
                    port = int(str(token).strip())
                except (ValueError, TypeError):
                    continue
                if 1 <= port <= 65535 and port not in ports:
                    ports.append(port)
            return ports

        requested_ports = _parse_ports(data.get('ports'))
        port_override = _parse_ports(data.get('port'))

        SSH_PORTS = requested_ports[:] if requested_ports else []

        # Apply single-port override first if provided (backward compatibility with older clients).
        if port_override:
            for p in reversed(port_override):
                if p in SSH_PORTS:
                    SSH_PORTS.remove(p)
                SSH_PORTS.insert(0, p)

        # Ensure standard ports are always available as fallbacks.
        for p in default_ports:
            if p not in SSH_PORTS:
                SSH_PORTS.append(p)

        if not SSH_PORTS:
            SSH_PORTS = default_ports[:]
        
        # Determine export command based on RouterOS version
        if not command:
            if ros_version == '7':
                command = 'export'
            else:
                command = 'export'
        
        # Validate command (security: only allow export commands)
        allowed_commands = ['export', 'export show-sensitive', 'export hide-sensitive']
        if command not in allowed_commands:
            return jsonify({'error': f'Invalid command. Allowed: {", ".join(allowed_commands)}'}), 400
        
        # Try connecting via SSH - attempt both ports with detailed logging
        client = None
        last_error = None
        ports_tried = []
        connection_errors = []
        
        print(f"[SSH] Attempting connection to {host}, trying ports: {SSH_PORTS}")
        
        for port in SSH_PORTS:
            try:
                print(f"[SSH] Trying port {port}...")
                client = paramiko.SSHClient()
                client.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # Auto-accept host key for convenience
                
                # Connection with timeout
                client.connect(
                    hostname=host,
                    port=port,
                    username=SSH_USERNAME,
                    password=SSH_PASSWORD,
                    timeout=10,
                    banner_timeout=10,
                    auth_timeout=10
                )
                
                print(f"[SSH] Successfully connected on port {port}")
                ports_tried.append(port)
                
                # Execute export command
                stdin, stdout, stderr = client.exec_command(command, timeout=30)
                
                # Read output
                output = stdout.read().decode('utf-8', errors='replace')
                error_output = stderr.read().decode('utf-8', errors='replace')
                
                # Check for errors
                if error_output and 'error' in error_output.lower():
                    if client:
                        try:
                            client.close()
                        except:
                            pass
                    return jsonify({
                        'error': f'Device returned error on port {port}: {error_output[:200]}'
                    }), 500
                
                if not output or not output.strip():
                    if client:
                        try:
                            client.close()
                        except:
                            pass
                    return jsonify({
                        'error': f'Device returned empty configuration on port {port}. Check RouterOS version and command.'
                    }), 500
                
                # Normalize line breaks (remove \ continuations) before returning
                normalized_output = normalize_line_breaks(output)
                
                print(f"[SSH] Successfully fetched config from {host}:{port}")
                
                # Success! Return config (do not log sensitive data)
                return jsonify({
                    'config': normalized_output,
                    'host': host,
                    'port': port,
                    'command': command,
                    'message': f'Successfully connected on port {port}'
                })
                
            except paramiko.AuthenticationException as e:
                # Auth failure - don't try other ports, credentials are wrong
                error_msg = f'SSH authentication failed on port {port}'
                print(f"[SSH] {error_msg}: {str(e)}")
                if client:
                    try:
                        client.close()
                    except:
                        pass
                return jsonify({
                    'error': f'{error_msg}. Check credentials. Tried ports: {", ".join(map(str, SSH_PORTS))}'
                }), 401
            except paramiko.SSHException as e:
                # SSH-specific error - try next port
                error_msg = f'SSH error on port {port}: {str(e)}'
                print(f"[SSH] {error_msg}")
                last_error = error_msg
                connection_errors.append(f"Port {port}: {str(e)}")
                ports_tried.append(port)
                if client:
                    try:
                        client.close()
                    except:
                        pass
                client = None
                continue  # Try next port
            except (OSError, ConnectionError, TimeoutError) as e:
                # Network/connection error - try next port
                error_msg = f'Connection error on port {port}: {str(e)}'
                print(f"[SSH] {error_msg}")
                last_error = error_msg
                connection_errors.append(f"Port {port}: {str(e)}")
                ports_tried.append(port)
                if client:
                    try:
                        client.close()
                    except:
                        pass
                client = None
                continue  # Try next port
            except Exception as e:
                # Other error - try next port
                error_msg = f'Unexpected error on port {port}: {str(e)}'
                print(f"[SSH] {error_msg}")
                last_error = error_msg
                connection_errors.append(f"Port {port}: {str(e)}")
                ports_tried.append(port)
                if client:
                    try:
                        client.close()
                    except:
                        pass
                client = None
                continue  # Try next port
        
        # All ports failed
        ports_str = ', '.join(map(str, set(ports_tried))) if ports_tried else ', '.join(map(str, SSH_PORTS))
        error_details = '; '.join(connection_errors) if connection_errors else (last_error or "Unable to connect")
        print(f"[SSH] All ports failed for {host}. Tried: {ports_str}. Errors: {error_details}")
        return jsonify({
            'error': f'Connection failed on all ports ({ports_str}). Errors: {error_details}',
            'ports_tried': list(set(ports_tried)) if ports_tried else SSH_PORTS
        }), 502
                    
    except Exception as e:
        # Do not expose internal errors or credentials
        print(f"[SSH] Error: {e}")
        return jsonify({
            'error': 'Backend error while fetching config'
        }), 500

# ========================================
# NOKIA 7250 CONFIG GENERATION
# ========================================

@app.route('/api/generate-nokia7250', methods=['POST'])
def generate_nokia7250():
    """
    Generate Nokia 7250 configuration based on provided parameters.
    """
    try:
        data = request.json
        system_name = data.get('system_name', '').strip()
        system_ip = data.get('system_ip', '').strip()
        location = data.get('location', '').strip()
        port1_desc = data.get('port1_desc', 'Switch').strip()
        port2_desc = data.get('port2_desc', 'Switch').strip()
        port2_shutdown = data.get('port2_shutdown', False)
        enable_ospf = data.get('enable_ospf', True)
        enable_bgp = data.get('enable_bgp', True)
        # Auto-fill for IN-STATE configs (always use these values - no user input needed)
        bgp_group = 'DALLAS-RR'  # Always DALLAS-RR for instate
        bgp_neighbors = ['10.2.0.107', '10.2.0.108']  # Always these for instate
        vpls_services = ['1245', '2245', '3245', '4245']  # Always these for instate
        # SDP 101 and 102 are automatically configured (standard instate values)
        sdp_farend1 = '10.249.0.200'  # Standard SDP 101 far-end for instate
        sdp_farend2 = ''  # SDP 102 without far-end (as per example)
        enable_fiber = data.get('enable_fiber', False)
        fiber_interface = data.get('fiber_interface', 'FIBERCOMM').strip()
        fiber_ip = data.get('fiber_ip', '').strip()
        backhauls = data.get('backhauls', [])
        
        if not system_name or not system_ip:
            return jsonify({'error': 'System name and system IP are required'}), 400
        
        # Validate backhauls (at least one required)
        if not backhauls or len(backhauls) == 0:
            return jsonify({'error': 'At least one backhaul is required'}), 400
        
        # Validate each backhaul has name and ip
        for bh in backhauls:
            if not bh.get('name') or not bh.get('ip'):
                return jsonify({'error': 'Each backhaul must have both name and IP/netmask'}), 400
        
        # Extract IP and netmask
        ip_parts = system_ip.split('/')
        if len(ip_parts) != 2:
            return jsonify({'error': 'System IP must be in CIDR format (e.g., 10.42.12.88/32)'}), 400
        
        system_ip_addr = ip_parts[0]
        system_ip_netmask = ip_parts[1]
        
        config_lines = []
        config_lines.append("##################################")
        config_lines.append("# Generic Universal System Configs")
        config_lines.append("##################################")
        config_lines.append("# BOF")
        config_lines.append("/bof primary-config cf3:/startup-config")
        config_lines.append("/bof save")
        config_lines.append("/admin save")
        config_lines.append("")
        config_lines.append("# ROLLBACK LOCATION")
        config_lines.append("/configure system rollback rollback-location \"cf3:/checkpoint_db\"")
        config_lines.append("")
        config_lines.append("# SNMP")
        config_lines.append("/configure system snmp no shutdown")
        config_lines.append("/configure system security snmp community \"FBZ1yYdphf\" r version both")
        config_lines.append("")
        config_lines.append("# NTP")
        config_lines.append("/configure system time ntp server 52.128.59.240")
        config_lines.append("/configure system time ntp server 52.128.59.241")
        config_lines.append("/configure system time ntp server 52.128.59.242")
        config_lines.append("/configure system time ntp server 52.128.59.243")
        config_lines.append("/configure system time ntp no shutdown")
        config_lines.append("/configure system time zone CST")
        config_lines.append("/configure system time dst-zone cdt")
        config_lines.append("")
        config_lines.append("# USERS")
        config_lines.append("/configure system security user nlroot access ftp snmp console netconf grpc")
        config_lines.append("/configure system security user nlroot console member \"administrative\"")
        config_lines.append("/configure system security user nlroot password XAgYqY8jig!d")
        config_lines.append("/configure system security no user admin")
        config_lines.append("")
        config_lines.append("# IDLE TIMEOUT")
        config_lines.append("/configure system login-control idle-timeout 90")
        config_lines.append("")
        config_lines.append("# ACLS")
        # Add ACL entries (simplified - you can expand this)
        acl_entries = [
            "10.2.0.0/16", "24.240.243.114/32", "50.63.176.139/32", "52.128.48.29/32",
            "66.185.162.140/32", "67.219.122.201/32", "107.178.5.97/32", "107.178.15.1/32",
            "107.178.15.15/32", "142.147.112.18/32", "142.147.116.219/32", "142.147.124.26/32",
            "199.242.62.162/32", "10.0.172.0/22", "52.128.62.248/29", "67.219.126.240/28"
        ]
        config_lines.append("/configure system security management-access-filter ip-filter default-action permit")
        for idx, acl_ip in enumerate(acl_entries, 1):
            config_lines.append(f"/configure system security management-access-filter ip-filter entry {idx} src-ip {acl_ip}")
            config_lines.append(f"/configure system security management-access-filter ip-filter entry {idx} dst-port 22 65535")
            config_lines.append(f"/configure system security management-access-filter ip-filter entry {idx} action permit")
        config_lines.append("/configure system security management-access-filter ip-filter no shut")
        config_lines.append("")
        config_lines.append("# CARD CONFIGURATION")
        config_lines.append("/configure card 1 card-type imm24-sfp++8-sfp28+2-qsfp28")
        config_lines.append("/configure card 1 mda 1")
        config_lines.append("/configure card 1 no shutdown")
        if port2_shutdown:
            config_lines.append("#SHUTDOWN PORT 2")
            config_lines.append("/configure port 1/1/2 shutdown")
        config_lines.append("")
        config_lines.append("        ")
        config_lines.append("##################################")
        config_lines.append("# SYSTEM SPECIFIC NETWORK SETTINGS")
        config_lines.append("##################################")
        config_lines.append("# SYSTEM IP")
        config_lines.append(f"/configure router interface \"system\" address {system_ip}")
        config_lines.append("/configure router interface \"system\" no shut")
        config_lines.append("/configure router autonomous-system 26077")
        config_lines.append(f"/configure router router-id {system_ip_addr}")
        config_lines.append("")
        config_lines.append("# System INFO")
        config_lines.append(f"/configure system name \"{system_name}\"")
        if location:
            config_lines.append(f"/configure system location \"{location}\"")
        config_lines.append("")
        config_lines.append("# PORTS")
        config_lines.append("/configure port 1/1/1 ethernet encap-type dot1q")
        config_lines.append("/configure port 1/1/1 ethernet mode hybrid")
        config_lines.append("/configure port 1/1/1 ethernet speed 1000")
        config_lines.append(f"/configure port 1/1/1 description \"{port1_desc}\"")
        config_lines.append("/configure port 1/1/1 no shutdown")
        config_lines.append("/configure port 1/1/2 ethernet encap-type dot1q")
        config_lines.append("/configure port 1/1/2 ethernet mode hybrid")
        config_lines.append("/configure port 1/1/2 ethernet speed 1000")
        config_lines.append(f"/configure port 1/1/2 description \"{port2_desc}\"")
        if port2_shutdown:
            config_lines.append("/configure port 1/1/2 shutdown")
        else:
            config_lines.append("/configure port 1/1/2 no shutdown")
        config_lines.append("        ")
        
        if enable_ospf:
            config_lines.append("# OSPF")
            config_lines.append(f"/configure router ospf 1 {system_ip_addr} area 0.0.0.0 interface \"system\" no shut")
            config_lines.append("/configure router ospf 1 no shut")
            config_lines.append("")
        
        if enable_bgp:
            config_lines.append("# BGP")
            config_lines.append(f"/configure router bgp router-id {system_ip_addr}")
            config_lines.append(f"/configure router bgp group \"{bgp_group}\" description \"{bgp_group}\"")
            config_lines.append(f"/configure router bgp group \"{bgp_group}\" authentication-key nvla8Z")
            config_lines.append(f"/configure router bgp group \"{bgp_group}\" peer-as 26077")
            config_lines.append(f"/configure router bgp group \"{bgp_group}\" local-address {system_ip_addr}")
            for neighbor in bgp_neighbors:
                if neighbor and neighbor.strip():
                    config_lines.append(f"/configure router bgp group \"{bgp_group}\" neighbor {neighbor.strip()}")
            config_lines.append("/configure router bgp no shut")
            config_lines.append("")
        
        if vpls_services and sdp_farend1:
            config_lines.append("# VPLS SERVICE")
            config_lines.append("/configure service sdp 101 mpls create")
            config_lines.append(f"/configure service sdp 101 far-end {sdp_farend1}")
            config_lines.append("/configure service sdp 101 ldp")
            config_lines.append("/configure service sdp 101 no shut")
            if sdp_farend2 and sdp_farend2.strip():
                config_lines.append("/configure service sdp 102 mpls create")
                config_lines.append(f"/configure service sdp 102 far-end {sdp_farend2}")
                config_lines.append("/configure service sdp 102 ldp")
                config_lines.append("/configure service sdp 102 no shut")
            else:
                # SDP 102 without far-end (as shown in example)
                config_lines.append("/configure service sdp 102 mpls create")
                config_lines.append("/configure service sdp 102 far-end")
                config_lines.append("/configure service sdp 102 ldp")
                config_lines.append("/configure service sdp 102 no shut")
            config_lines.append("")
            config_lines.append("#MPLS")
            config_lines.append("/configure router mpls no shut")
            for vpls_id in vpls_services:
                if vpls_id and str(vpls_id).strip():
                    vpls_num = str(vpls_id).strip()
                    customer_num = vpls_num
                    config_lines.append(f"/configure service customer {customer_num} create")
                    config_lines.append(f"/configure service vpls {vpls_num} customer {customer_num} create")
                    config_lines.append(f"/configure service vpls {vpls_num} service-mtu 1594")
                    config_lines.append(f"/configure service vpls {vpls_num} fdb-table-size 1000")
                    config_lines.append(f"/configure service vpls {vpls_num} stp shutdown")
                    config_lines.append(f"/configure service vpls {vpls_num} mesh-sdp 101:{vpls_num} create")
                    config_lines.append(f"/configure service vpls {vpls_num} mesh-sdp 101:{vpls_num} no shutdown")
                    if sdp_farend2 and sdp_farend2.strip():
                        config_lines.append(f"/configure service vpls {vpls_num} mesh-sdp 102:{vpls_num} create")
                        config_lines.append(f"/configure service vpls {vpls_num} mesh-sdp 102:{vpls_num} no shutdown")
                    # SAP assignments based on VPLS ID (first digit * 1000)
                    vlan_id = int(vpls_num[0]) * 1000 if vpls_num[0].isdigit() else 1000
                    config_lines.append(f"/configure service vpls {vpls_num} sap 1/1/1:{vlan_id} create")
                    config_lines.append(f"/configure service vpls {vpls_num} no shutdown")
                    if not port2_shutdown:
                        config_lines.append(f"/configure service vpls {vpls_num} sap 1/1/2:{vlan_id} create")
                        config_lines.append(f"/configure service vpls {vpls_num} no shutdown")
            config_lines.append("    ")
            config_lines.append("")
        
        if enable_fiber and fiber_ip:
            config_lines.append("")
            config_lines.append("# FIBER PORT CONFIGURATION")
            config_lines.append("/configure port 1/1/24 ethernet mtu 1514")
            config_lines.append("/configure port 1/1/24 no shutdown")
            config_lines.append("")
            config_lines.append("# FIBER INTERFACE CONFIGURATION")
            config_lines.append(f"/configure router interface \"{fiber_interface}\" address {fiber_ip}")
            config_lines.append(f"/configure router interface \"{fiber_interface}\" description \"{fiber_interface}\"")
            config_lines.append(f"/configure router interface \"{fiber_interface}\" port 1/1/24")
            config_lines.append(f"/configure router interface \"{fiber_interface}\" no shutdown")
            config_lines.append("")
            config_lines.append("# FIBER OSPF CONFIGURATION")
            config_lines.append(f"/configure router ospf 1 area 0.0.0.0 interface \"{fiber_interface}\" interface-type point-to-point")
            config_lines.append(f"/configure router ospf 1 area 0.0.0.0 interface \"{fiber_interface}\" authentication-type message-digest")
            config_lines.append(f"/configure router ospf 1 area 0.0.0.0 interface \"{fiber_interface}\" message-digest-key 1 md5 \"m8M5JwvdYM\"")
            config_lines.append(f"/configure router ospf 1 area 0.0.0.0 interface \"{fiber_interface}\" no shutdown")
            config_lines.append("")
        
        if backhauls:
            config_lines.append("")
            config_lines.append("##################################")
            config_lines.append("# BACKHAULS")
            config_lines.append("##################################")
            config_lines.append("        ")
            config_lines.append("")
            for backhaul in backhauls:
                bh_name = backhaul.get('name', '').strip()
                bh_ip = backhaul.get('ip', '').strip()
                if bh_name and bh_ip:
                    config_lines.append("")
                    config_lines.append(f"# {bh_name}")
                    config_lines.append(f"/config port 1/1/1 no shut")
                    config_lines.append(f"/config port 1/1/1 ethernet mode network")
                    config_lines.append(f"/config port 1/1/1 description \"{bh_name}\"")
                    config_lines.append(f"/config router interface \"{bh_name}\" address {bh_ip}")
                    config_lines.append(f"/config router interface \"{bh_name}\" port 1/1/1")
                    config_lines.append(f"/config router interface \"{bh_name}\" no shut")
                    config_lines.append(f"/config router mpls interface \"{bh_name}\" no shut")
                    config_lines.append(f"/config router rsvp interface \"{bh_name}\" no shut")
                    config_lines.append(f"/config router ldp interface-parameters interface \"{bh_name}\" no shut")
                    config_lines.append(f"/config router ospf 1 area 0.0.0.0 interface \"{bh_name}\" interface-type point-to-point")
                    config_lines.append(f"/config router ospf 1 area 0.0.0.0 interface \"{bh_name}\" authentication-type message-digest")
                    config_lines.append(f"/config router ospf 1 area 0.0.0.0 interface \"{bh_name}\" message-digest-key 1 md5 \"m8M5JwvdYM\"")
                    config_lines.append(f"/config router ospf 1 area 0.0.0.0 interface \"{bh_name}\" no shut")
                    config_lines.append("    ")
        
        config_text = '\n'.join(config_lines)
        
        return jsonify({
            'success': True,
            'config': config_text
        })
        
    except Exception as e:
        print(f"[NOKIA 7250] Error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            'error': f'Error generating Nokia 7250 configuration: {str(e)}'
        }), 500

# ========================================
# MIKROTIK TO NOKIA MIGRATION
# ========================================

@app.route('/api/migrate-mikrotik-to-nokia', methods=['POST'])
def migrate_mikrotik_to_nokia():
    """
    Convert MikroTik RouterOS configuration to Nokia SR OS syntax.
    """
    try:
        data = request.json
        source_config = data.get('source_config', '').strip()
        preserve_ips = data.get('preserve_ips', True)
        
        if not source_config:
            return jsonify({'error': 'Source configuration is required'}), 400

        def _basic_mikrotik_to_nokia(config_text: str, preserve_all_ips: bool = True) -> str:
            """
            Deterministic (non-AI) MikroTik -> Nokia SR OS conversion fallback.
            Produces a usable starting-point Nokia configuration when AI is unavailable.
            """
            text = config_text or ""

            def _clean(value: str) -> str:
                return (value or "").strip().strip('"').strip("'").strip()

            def _nokia_iface_name(src_iface: str) -> str:
                # Keep RouterOS names to avoid guessing platform-specific mappings.
                return _clean(src_iface).replace("\\", "_")

            def _map_physical_port(src_iface: str):
                # Best-effort mapping to Nokia port style 1/1/<n>.
                iface = _clean(src_iface)
                m = re.fullmatch(r"ether(\d+)", iface)
                if m:
                    return f"1/1/{m.group(1)}"
                m = re.fullmatch(r"sfp(\d+)", iface)
                if m:
                    return f"1/1/{m.group(1)}"
                m = re.fullmatch(r"sfp-sfpplus(\d+)", iface)
                if m:
                    return f"1/1/{m.group(1)}"
                m = re.fullmatch(r"sfp28-(\d+)", iface)
                if m:
                    return f"1/1/{m.group(1)}"
                return None

            def _extract_identity():
                m = re.search(r"(?ms)^/system identity\s*\n\s*set\s+name=([^\n]+)\s*$", text)
                return _clean(m.group(1)) if m else None

            def _extract_loopback_ip():
                m = re.search(r"(?m)^\s*add\s+address=(\d+\.\d+\.\d+\.\d+)(?:/(\d+))?\b.*\binterface=loop0\b", text)
                if m:
                    ip = m.group(1)
                    prefix = m.group(2) or "32"
                    return f"{ip}/{prefix}"
                m = re.search(r"(?m)\brouter-id=(\d+\.\d+\.\d+\.\d+)\b", text)
                if m:
                    return f"{m.group(1)}/32"
                return None

            def _extract_ip_addresses():
                ip_entries = []
                in_ip_section = False
                for raw in text.splitlines():
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    if line.startswith("/ip address"):
                        in_ip_section = True
                        continue
                    if line.startswith("/") and not line.startswith("/ip address"):
                        in_ip_section = False
                    if not in_ip_section and not line.startswith("add "):
                        continue
                    if "address=" not in line or "interface=" not in line:
                        continue

                    addr_m = re.search(r"\baddress=([^\s]+)", line)
                    iface_m = re.search(r"\binterface=([^\s]+)", line)
                    if not addr_m or not iface_m:
                        continue

                    addr_raw = _clean(addr_m.group(1))
                    iface_raw = _clean(iface_m.group(1))
                    comment_m = re.search(r"\bcomment=([^\s].*?)(?=\s+\w+=|\s*$)", line)
                    comment = _clean(comment_m.group(1)) if comment_m else ""

                    if "/" in addr_raw:
                        ip_prefix = addr_raw
                    else:
                        ip_prefix = f"{addr_raw}/32" if iface_raw == "loop0" else f"{addr_raw}/24"

                    ip_entries.append({"interface": iface_raw, "address": ip_prefix, "comment": comment})
                return ip_entries

            def _extract_static_routes():
                routes = []
                in_route_section = False
                for raw in text.splitlines():
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    if line.startswith("/ip route"):
                        in_route_section = True
                        continue
                    if line.startswith("/") and not line.startswith("/ip route"):
                        in_route_section = False
                    if not in_route_section and not line.startswith("add "):
                        continue
                    if "dst-address=" not in line or "gateway=" not in line:
                        continue
                    dst_m = re.search(r"\bdst-address=([^\s]+)", line)
                    gw_m = re.search(r"\bgateway=([^\s]+)", line)
                    if not dst_m or not gw_m:
                        continue
                    routes.append({"dst": _clean(dst_m.group(1)), "gw": _clean(gw_m.group(1))})
                return routes

            identity = _extract_identity()
            loopback = _extract_loopback_ip()
            ip_entries = _extract_ip_addresses()
            routes = _extract_static_routes()

            out = []
            out.append("# ================================================")
            out.append("# Nokia SR OS configuration (basic conversion)")
            out.append("# Generated by NOC Config Maker (no AI required)")
            out.append("# ================================================")
            out.append("")
            out.append("# NOTE:")
            out.append("# - Best-effort syntax conversion when AI is unavailable.")
            out.append("# - Review port/SAP/service mappings before deployment.")
            out.append("")

            if identity:
                out.append(f"# Source identity: {identity}")
                out.append(f"/configure system name \"{identity}\"")
                out.append("")

            if loopback:
                out.append("# LOOPBACK / SYSTEM INTERFACE")
                out.append(f"/configure router interface \"system\" address {loopback}")
                out.append("/configure router interface \"system\" no shutdown")
                out.append("")

            if preserve_all_ips and ip_entries:
                out.append("# ROUTER INTERFACES (from /ip address)")
                for entry in ip_entries:
                    iface = entry.get("interface") or ""
                    addr = entry.get("address") or ""
                    if iface == "loop0":
                        continue
                    name = _nokia_iface_name(iface)
                    comment = entry.get("comment") or ""
                    if comment:
                        out.append(f"# {comment}")
                    out.append(f"/configure router interface \"{name}\" address {addr}")
                    port = _map_physical_port(iface)
                    if port:
                        out.append(f"/configure router interface \"{name}\" port {port}")
                    else:
                        out.append(f"# TODO: Attach \"{name}\" to the correct port/SAP (source interface={iface})")
                    out.append(f"/configure router interface \"{name}\" no shutdown")
                    out.append("")

            if routes:
                out.append("# STATIC ROUTES (from /ip route)")
                for route in routes:
                    out.append(f"/configure router static-route {route['dst']} next-hop {route['gw']}")
                out.append("")

            out.append("# OSPF/BGP/FIREWALL/SERVICES")
            out.append("# TODO: Convert routing and policy sections as needed (AI recommended when available).")
            out.append("")

            return "\n".join(out).rstrip() + "\n"
        
        # Use AI to convert MikroTik config to Nokia syntax
        system_prompt = """You are a network configuration expert specializing in converting MikroTik RouterOS configurations to Nokia SR OS syntax.

Your task is to convert the provided MikroTik RouterOS configuration to Nokia SR OS configuration format.

Key conversion rules:
1. RouterOS /ip address → Nokia /configure router interface
2. RouterOS /routing ospf → Nokia /configure router ospf
3. RouterOS /routing bgp → Nokia /configure router bgp
4. RouterOS /interface bridge → Nokia /configure service vpls (if applicable)
5. RouterOS /ip firewall → Nokia /configure router policy-options (simplified)
6. Preserve all IP addresses, subnets, and network settings exactly
7. Convert RouterOS interface names to Nokia port format (e.g., ether1 → 1/1/1)
8. Convert RouterOS commands to Nokia CLI format

Output the complete Nokia SR OS configuration in the standard Nokia format."""
        
        user_prompt = f"""Convert this MikroTik RouterOS configuration to Nokia SR OS syntax:

{source_config}

Requirements:
- Preserve all IP addresses and network settings
- Convert all RouterOS commands to Nokia SR OS syntax
- Maintain the same network topology and routing
- Use proper Nokia port notation (slot/port/channel)
- Output complete Nokia configuration ready for deployment"""
        
        try:
            # Try to use Ollama if available
            nokia_config = call_ai([
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ], max_tokens=16000, task_type='migration', config_size=len(source_config))
            
            # Clean up any markdown formatting
            nokia_config = nokia_config.replace('```nokia', '').replace('```', '').strip()
            
            return jsonify({
                'success': True,
                'nokia_config': nokia_config
            })
        except Exception as ai_error:
            print(f"[MIGRATION] AI conversion failed: {ai_error}")
            # Fallback: Basic conversion without AI (still returns a usable config).
            basic = _basic_mikrotik_to_nokia(source_config, preserve_all_ips=preserve_ips)
            return jsonify({
                'success': True,
                'nokia_config': basic,
                'ai_used': False,
                'warning': f'AI unavailable; generated a basic conversion instead. Details: {str(ai_error)}'
            })
        
    except Exception as e:
        # Last-resort fallback: if anything unexpected happens, still return a basic conversion so the
        # Nokia Migration tab remains usable even when AI/infra dependencies are down.
        print(f"[MIGRATION] Error: {e}")
        import traceback
        traceback.print_exc()
        try:
            if 'source_config' in locals() and (source_config or '').strip():
                basic = _basic_mikrotik_to_nokia(source_config, preserve_all_ips=preserve_ips)
                return jsonify({
                    'success': True,
                    'nokia_config': basic,
                    'ai_used': False,
                    'warning': f'Unexpected error; generated a basic conversion instead. Details: {str(e)}'
                })
        except Exception as fallback_error:
            print(f"[MIGRATION] Fallback conversion also failed: {fallback_error}")
        return jsonify({'error': f'Error converting MikroTik to Nokia: {str(e)}'}), 500

# ========================================
# HEALTH CHECK
# ========================================

@app.route('/api/get-config-policies', methods=['GET'])
def get_config_policies_endpoint():
    """Get list of available configuration policies with optional category filtering"""
    try:
        # Reload policies if requested
        if request.args.get('reload') == 'true':
            global CONFIG_POLICIES, _policies_loaded
            CONFIG_POLICIES = load_config_policies()
            _policies_loaded = True
        
        # Get policies (lazy load if needed)
        policies = get_config_policies()
        
        # Filter by category if requested
        category_filter = request.args.get('category')
        
        policies_list = []
        for policy_name, policy_data in policies.items():
            # Skip if category filter doesn't match
            if category_filter and policy_data.get('category') != category_filter:
                continue
            
            policies_list.append({
                'name': policy_name,
                'category': policy_data.get('category', 'unknown'),
                'filename': policy_data.get('filename', ''),
                'path': policy_data.get('path', ''),
                'relative_path': policy_data.get('relative_path', ''),
                'type': policy_data.get('type', 'markdown'),
                'description': policy_data.get('content', '')[:300] + '...' if len(policy_data.get('content', '')) > 300 else policy_data.get('content', '')
            })
        
        # Get unique categories
        categories = sorted(set(p.get('category', 'unknown') for p in policies.values()))
        
        return jsonify({
            'success': True,
            'policies': policies_list,
            'count': len(policies_list),
            'total_policies': len(policies),
            'categories': categories,
            'filtered_by': category_filter if category_filter else None
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/get-config-policy/<policy_name>', methods=['GET'])
def get_config_policy(policy_name):
    """Get a specific configuration policy by name"""
    try:
        policies = get_config_policies()
        if policy_name in policies:
            return jsonify({
                'success': True,
                'policy_name': policy_name,
                'content': policies[policy_name]['content'],
                'path': policies[policy_name].get('path', '')
            })
        else:
            return jsonify({'error': f'Policy "{policy_name}" not found'}), 404
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/get-config-policy-bundle', methods=['GET'])
def get_config_policy_bundle():
    """Return merged policy text for selected keys and optional references.
    Query params:
      keys: comma-separated policy keys (as listed by /api/get-config-policies)
      include: comma-separated extras: compliance,enterprise
    """
    try:
        keys_param = request.args.get('keys', '').strip()
        include_param = request.args.get('include', '').strip().lower()
        keys = [k.strip() for k in keys_param.split(',') if k.strip()] if keys_param else []
        includes = set([i.strip() for i in include_param.split(',') if i.strip()]) if include_param else set()

        policies = get_config_policies()
        parts = []
        for k in keys:
            p = policies.get(k)
            if p and p.get('content'):
                parts.append(f"# POLICY: {k}\n\n{p['content'].strip()}\n")

        if 'compliance' in includes and 'compliance-reference' in policies:
            parts.append(f"# REFERENCE: compliance\n\n{policies['compliance-reference']['content'].strip()}\n")
        if 'enterprise' in includes and 'enterprise-reference' in policies:
            parts.append(f"# REFERENCE: enterprise\n\n{policies['enterprise-reference']['content'].strip()}\n")

        merged = "\n\n".join(parts).strip()
        return jsonify({'success': True, 'keys': keys, 'include': list(includes), 'content': merged})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/reload-config-policies', methods=['POST'])
def reload_config_policies():
    """Reload configuration policies from disk"""
    try:
        global CONFIG_POLICIES, _policies_loaded
        CONFIG_POLICIES = load_config_policies()
        _policies_loaded = True
        policies = get_config_policies()
        return jsonify({
            'success': True,
            'message': f'Reloaded {len(policies)} policies',
            'policies': list(policies.keys())
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health():
    """Check if API server is running and configured
    
    This is the unified backend - api_server.py handles all AI internally.
    Backend is considered 'online' if this endpoint responds, regardless of Ollama/OpenAI status.
    The backend will handle AI provider availability and fallbacks internally.
    """
    # Check if Ollama is available (informational only - doesn't affect 'online' status)
    ollama_available = False
    if AI_PROVIDER == 'ollama':
        try:
            resp = requests.get(f"{OLLAMA_API_URL}/api/tags", timeout=2)
            ollama_available = resp.status_code == 200
        except:
            pass
    
    # Backend is always 'online' if this endpoint responds
    # AI provider availability is handled internally by api_server.py
    return jsonify({
        'status': 'online',  # Always 'online' if endpoint responds - unified backend is ready
        'ai_provider': AI_PROVIDER,
        'api_key_configured': bool(OPENAI_API_KEY) if AI_PROVIDER == 'openai' else None,
        'ollama_available': ollama_available if AI_PROVIDER == 'ollama' else None,  # Informational only
        'ollama_model': OLLAMA_MODEL if AI_PROVIDER == 'ollama' else None,
        'timestamp': get_cst_timestamp(),
        'message': 'Unified backend (api_server.py) is online and ready'
    })

@app.route('/api/app-config', methods=['GET'])
def app_config():
    """
    Lightweight runtime configuration consumed by the frontend.

    Keep this endpoint unauthenticated so the UI can load defaults during startup.
    """
    bng_peers = {
        'NE': os.getenv('BNG_PEER_NE', '10.254.247.3'),
        'IL': os.getenv('BNG_PEER_IL', '10.247.72.34'),
        'IA': os.getenv('BNG_PEER_IA', '10.254.247.3'),
        'KS': os.getenv('BNG_PEER_KS', '10.249.0.200'),
        'IN': os.getenv('BNG_PEER_IN', '10.254.247.3'),
    }
    default_bng_peer = os.getenv('BNG_PEER_DEFAULT', '10.254.247.3')
    return jsonify({'bng_peers': bng_peers, 'default_bng_peer': default_bng_peer})

@app.route('/api/feedback', methods=['POST'])
def submit_feedback():
    """Submit feedback/bug report/feature request (stored for admin review)."""
    try:
        data = request.json
        if not data:
            return jsonify({'error': 'No data provided'}), 400
        
        # Extract form data
        feedback_type = data.get('type', 'feedback').capitalize()
        subject = data.get('subject', 'No subject')
        category = data.get('category', 'Not specified')
        experience = data.get('experience', 'Not rated')
        details = data.get('details', 'No details provided')
        name = data.get('name', 'Anonymous')
        # Always timestamp feedback on the server so it is consistent regardless of the user's locale/timezone.
        timestamp = get_cst_timestamp()

        # Save feedback to database
        feedback_db = init_feedback_db()
        conn = sqlite3.connect(str(feedback_db))
        cursor = conn.cursor()
        
        email = data.get('email', '')
        cursor.execute('''
            INSERT INTO feedback (feedback_type, subject, category, experience, details, name, email, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (feedback_type, subject, category, experience, details, name, email, timestamp))
        
        feedback_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        safe_print(f"[FEEDBACK] Saved to database (ID: {feedback_id}) from {name}: {subject}")

        return jsonify({
            'success': True,
            'message': 'Feedback received and saved',
            'feedback_id': feedback_id,
        })
        
    except Exception as e:
        safe_print(f"[FEEDBACK ERROR] {str(e)}")
        return jsonify({'error': str(e)}), 500

# ========================================
# ADMIN ENDPOINTS: Feedback Management
# ========================================

def require_auth(f):
    """Decorator to require authentication"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        token = request.headers.get('Authorization')
        if not token:
            token = request.json.get('token') if request.json else None
        
        if token:
            # Remove 'Bearer ' prefix if present
            if token.startswith('Bearer '):
                token = token[7:]
        
        if not token:
            return jsonify({'error': 'Authentication required'}), 401
        
        user_info = verify_token(token)
        if not user_info:
            return jsonify({'error': 'Invalid or expired token'}), 401
        
        # Add user info to request context
        request.current_user = user_info
        return f(*args, **kwargs)
    return decorated_function

def is_admin_user():
    """Check if current user is admin"""
    try:
        token = request.headers.get('Authorization', '').replace('Bearer ', '')
        if not token:
            return False

        try:
            decoded = jwt.decode(token, JWT_SECRET, algorithms=['HS256'])
            user_email = decoded.get('email', '').lower()
            
            # Admin emails (can be set via environment variable or use default)
            admin_emails = os.getenv('ADMIN_EMAILS', 'netops@team.nxlink.com,whamza@team.nxlink.com').lower().split(',')
            admin_emails = [e.strip() for e in admin_emails]
            
            return user_email in admin_emails
        except:
            return False
    except:
        return False

@app.route('/api/infrastructure', methods=['GET'])
@require_auth
def infrastructure_config():
    """
    Authenticated infrastructure defaults for the frontend.

    This endpoint can include operational defaults/secrets (e.g., RADIUS secret) so NOC users
    don't have to manually paste them into the UI, improving consistency and reducing human error.
    """
    def _csv(value: str):
        return [v.strip() for v in (value or '').split(',') if v.strip()]

    dns_primary = os.getenv('NEXTLINK_DNS_PRIMARY', '142.147.112.3').strip()
    dns_secondary = os.getenv('NEXTLINK_DNS_SECONDARY', '142.147.112.19').strip()
    shared_key = os.getenv('NEXTLINK_SHARED_KEY', '').strip()

    radius_secret = os.getenv('NEXTLINK_RADIUS_SECRET', '').strip()
    radius_dhcp_servers = _csv(os.getenv('NEXTLINK_RADIUS_DHCP_SERVERS', ''))
    radius_login_servers = _csv(os.getenv('NEXTLINK_RADIUS_LOGIN_SERVERS', ''))

    # If a secret is provided but server lists are not, ship common defaults.
    if radius_secret and not radius_dhcp_servers and not radius_login_servers:
        radius_dhcp_servers = ['142.147.112.17', '142.147.112.18']
        radius_login_servers = ['142.147.112.17', '142.147.112.18']

    return jsonify({
        'dns_servers': {
            'primary': dns_primary,
            'secondary': dns_secondary,
        },
        'shared_key': shared_key or None,
        'snmp': {
            'contact': os.getenv('NEXTLINK_SNMP_CONTACT', 'netops@team.nxlink.com').strip(),
        },
        'radius': {
            'secret': radius_secret or None,
            'dhcp_servers': radius_dhcp_servers,
            'login_servers': radius_login_servers,
        },
    })

@app.route('/api/admin/feedback', methods=['GET'])
@require_auth
def get_feedback():
    """Get all feedback (admin only)"""
    try:
        # Check if user is admin
        if not is_admin_user():
            return jsonify({'error': 'Admin access required'}), 403
        
        feedback_db = init_feedback_db()
        conn = sqlite3.connect(str(feedback_db))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get filter parameters
        status_filter = request.args.get('status', 'all')
        type_filter = request.args.get('type', 'all')
        limit = int(request.args.get('limit', 100))
        offset = int(request.args.get('offset', 0))
        
        query = 'SELECT * FROM feedback WHERE 1=1'
        params = []
        
        if status_filter != 'all':
            query += ' AND status = ?'
            params.append(status_filter)
        
        if type_filter != 'all':
            query += ' AND feedback_type = ?'
            params.append(type_filter.capitalize())
        
        query += ' ORDER BY timestamp DESC LIMIT ? OFFSET ?'
        params.extend([limit, offset])
        
        cursor.execute(query, params)
        feedback_list = [dict(row) for row in cursor.fetchall()]
        
        # Get total count
        count_query = 'SELECT COUNT(*) FROM feedback WHERE 1=1'
        count_params = []
        if status_filter != 'all':
            count_query += ' AND status = ?'
            count_params.append(status_filter)
        if type_filter != 'all':
            count_query += ' AND feedback_type = ?'
            count_params.append(type_filter.capitalize())
        
        cursor.execute(count_query, count_params)
        total_count = cursor.fetchone()[0]
        
        conn.close()
        
        return jsonify({
            'success': True,
            'feedback': feedback_list,
            'total': total_count,
            'limit': limit,
            'offset': offset
        })
        
    except Exception as e:
        safe_print(f"[ADMIN FEEDBACK ERROR] {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/users/reset-password', methods=['POST'])
@require_auth
def admin_reset_user_password():
    """Admin-only password reset for users."""
    try:
        if not is_admin_user():
            return jsonify({'error': 'Admin access required'}), 403

        data = request.get_json() or {}
        email = (data.get('email') or '').strip().lower()
        new_password = (data.get('newPassword') or '').strip()
        require_change = bool(data.get('requirePasswordChange', True))

        if not email:
            return jsonify({'success': False, 'error': 'Email is required'}), 400

        if not validate_email_domain(email):
            return jsonify({'success': False, 'error': 'Invalid email domain'}), 403

        if new_password and len(new_password) < 8:
            return jsonify({'success': False, 'error': 'New password must be at least 8 characters'}), 400

        init_users_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'users.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        c.execute('SELECT id FROM users WHERE email = ?', (email,))
        user = c.fetchone()

        effective_password = new_password or DEFAULT_PASSWORD
        new_password_hash = hash_password(effective_password)

        if not user:
            # Create user on admin reset if they don't exist yet
            c.execute('''INSERT INTO users (email, password_hash, display_name, first_login)
                         VALUES (?, ?, ?, ?)''',
                      (email, new_password_hash, email.split('@')[0], 1 if require_change else 0))
            conn.commit()
            conn.close()
            return jsonify({
                'success': True,
                'message': 'User created and password set',
                'temporaryPassword': effective_password if not new_password else None,
                'requirePasswordChange': require_change
            })

        c.execute('''UPDATE users
                     SET password_hash = ?,
                         first_login = ?,
                         password_changed_at = CURRENT_TIMESTAMP,
                         reset_token = NULL,
                         reset_token_expires = NULL
                     WHERE id = ?''',
                 (new_password_hash, 1 if require_change else 0, user['id']))
        conn.commit()
        conn.close()

        return jsonify({
            'success': True,
            'message': 'Password reset successfully',
            'temporaryPassword': effective_password if not new_password else None,
            'requirePasswordChange': require_change
        })

    except Exception as e:
        safe_print(f"[ADMIN USER RESET ERROR] {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/admin/feedback/<int:feedback_id>/status', methods=['PUT'])
@require_auth
def update_feedback_status(feedback_id):
    """Update feedback status (admin only)"""
    try:
        # Check if user is admin
        if not is_admin_user():
            return jsonify({'error': 'Admin access required'}), 403
        data = request.json
        new_status = data.get('status', 'new')
        admin_notes = data.get('admin_notes', '')
        
        feedback_db = init_feedback_db()
        conn = sqlite3.connect(str(feedback_db))
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE feedback 
            SET status = ?, admin_notes = ?
            WHERE id = ?
        ''', (new_status, admin_notes, feedback_id))
        
        conn.commit()
        conn.close()
        
        return jsonify({'success': True, 'message': 'Feedback status updated'})
        
    except Exception as e:
        safe_print(f"[ADMIN FEEDBACK UPDATE ERROR] {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/admin/feedback/export', methods=['GET'])
@require_auth
def export_feedback_excel():
    """Export feedback to Excel (admin only)"""
    try:
        # Check if user is admin
        if not is_admin_user():
            return jsonify({'error': 'Admin access required'}), 403
        import pandas as pd
        from io import BytesIO
        
        feedback_db = init_feedback_db()
        conn = sqlite3.connect(str(feedback_db))
        
        # Get all feedback
        df = pd.read_sql_query('''
            SELECT 
                id,
                feedback_type,
                subject,
                category,
                experience,
                details,
                name,
                email,
                timestamp,
                status,
                admin_notes
            FROM feedback
            ORDER BY timestamp DESC
        ''', conn)
        
        conn.close()
        
        # Create Excel file in memory
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Feedback', index=False)
        
        output.seek(0)
        
        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=f'feedback_export_{get_cst_now().strftime("%Y%m%d_%H%M%S")}.xlsx'
        )
        
    except ImportError:
        return jsonify({'error': 'pandas and openpyxl required for Excel export. Install with: pip install pandas openpyxl'}), 500
    except Exception as e:
        safe_print(f"[EXCEL EXPORT ERROR] {str(e)}")
        return jsonify({'error': str(e)}), 500

# ========================================
# DEVICE-AWARE MIGRATION ENDPOINT
# ========================================

@app.route('/api/migrate-config', methods=['POST'])
def migrate_config():
    """
    Intelligent device-aware configuration migration
    
    Features:
    - Auto-detects source device model
    - Auto-detects RouterOS version
    - Preserves ether1 for management across all devices
    - Intelligently maps interfaces based on port types
    - Migrates all interface-related configs (IPs, OSPF, BGP, firewall, etc.)
    - Applies ROS6→ROS7 syntax conversion only when needed
    """
    try:
        data = request.json
        config = data.get('config', '')
        target_device = data.get('target_device', '')
        target_version = data.get('target_version', '7')
        source_device = data.get('source_device', '')
        
        if not config:
            return jsonify({'error': 'No configuration provided'}), 400
        
        if not target_device:
            return jsonify({'error': 'Target device required'}), 400
        
        # Auto-detect source device if not specified
        if not source_device:
            source_device = detect_device_from_config(config)
            if not source_device:
                return jsonify({
                    'error': 'Could not detect source device. Please specify manually.',
                    'available_devices': list(ROUTERBOARD_INTERFACES.keys())
                }), 400
        
        # Auto-detect source RouterOS version
        detected_version = detect_routeros_version(config)
        source_version = data.get('source_version', detected_version or 6)
        
        # Validate devices exist in database
        if source_device not in ROUTERBOARD_INTERFACES:
            return jsonify({
                'error': f'Unknown source device: {source_device}',
                'available_devices': list(ROUTERBOARD_INTERFACES.keys())
            }), 400
        
        if target_device not in ROUTERBOARD_INTERFACES:
            return jsonify({
                'error': f'Unknown target device: {target_device}',
                'available_devices': list(ROUTERBOARD_INTERFACES.keys())
            }), 400
        
        # Determine what migrations are needed
        needs_syntax_migration = (source_version == 6 and target_version == '7')
        needs_device_migration = (source_device != target_device)
        
        migrated_config = config
        interface_map = None
        
        # Step 1: Device migration (interface renaming)
        if needs_device_migration:
            safe_print(f"[MIGRATION] Device: {source_device} → {target_device}")
            
            # Build intelligent interface mapping
            interface_map = build_interface_migration_map(source_device, target_device)
            
            if not interface_map:
                return jsonify({
                    'error': f'No migration path available from {source_device} to {target_device}'
                }), 400
            
            # Apply interface migration
            migrated_config = migrate_interface_config(migrated_config, interface_map)
            
            safe_print(f"[MIGRATION] Mapped {len(interface_map)} interfaces")
            for old, new in list(interface_map.items())[:5]:  # Show first 5
                safe_print(f"[MIGRATION]   {old} → {new}")
        
        # Step 2: Syntax migration (ROS6 → ROS7)
        if needs_syntax_migration:
            safe_print(f"[MIGRATION] Syntax: ROS{source_version} → ROS{target_version}")
            migrated_config = apply_ros6_to_ros7_syntax(migrated_config)
        
        # Prepare response
        response_data = {
            'success': True,
            'migrated_config': migrated_config,
            'source_device': source_device,
            'target_device': target_device,
            'source_version': source_version,
            'target_version': target_version,
            'detected_source_device': source_device if not data.get('source_device') else None,
            'detected_source_version': detected_version,
            'syntax_migrated': needs_syntax_migration,
            'device_migrated': needs_device_migration,
            'interfaces_mapped': len(interface_map) if interface_map else 0,
            'migration_summary': {
                'ether1_preserved': 'ether1' in interface_map and interface_map['ether1'] == 'ether1' if interface_map else False,
                'total_interfaces': len(interface_map) if interface_map else 0,
                'source_ports': ROUTERBOARD_INTERFACES[source_device]['total_ports'],
                'target_ports': ROUTERBOARD_INTERFACES[target_device]['total_ports']
            }
        }
        
        if not interface_map:
            response_data['interface_map'] = interface_map
        
        safe_print(f"[MIGRATION] Complete - {len(migrated_config)} chars")
        return jsonify(response_data)
        
    except Exception as e:
        safe_print(f"[MIGRATION ERROR] {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500

@app.route('/api/get-routerboards', methods=['GET'])
def get_routerboards():
    """Get list of all supported RouterBoard models with specs"""
    try:
        devices = []
        for model, specs in ROUTERBOARD_INTERFACES.items():
            devices.append({
                'model': model,
                'series': specs['series'],
                'cpu': specs['cpu'],
                'total_ports': specs['total_ports'],
                'management_port': specs['management_port'],
                'port_types': list(specs['ports'].keys()),
                'typical_use': specs['typical_use']
            })
        
        return jsonify({
            'success': True,
            'devices': devices,
            'total_models': len(devices)
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ========================================
# AUTHENTICATION & USER MANAGEMENT
# ========================================

# JWT Secret Key (in production, use environment variable)
JWT_SECRET = os.getenv('JWT_SECRET', secrets.token_urlsafe(32))
DEFAULT_PASSWORD = os.getenv('DEFAULT_PASSWORD', 'NOCConfig2025!')  # Change this in production

# Azure AD Configuration (for Microsoft SSO)
AZURE_CLIENT_ID = os.getenv('AZURE_CLIENT_ID', '')
AZURE_CLIENT_SECRET = os.getenv('AZURE_CLIENT_SECRET', '')
AZURE_TENANT_ID = os.getenv('AZURE_TENANT_ID', 'common')

def init_users_db():
    """Initialize users database for authentication"""
    secure_dir = 'secure_data'
    if not os.path.exists(secure_dir):
        os.makedirs(secure_dir)
    
    db_path = os.path.join(secure_dir, 'users.db')
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    
    # Users table
    c.execute('''CREATE TABLE IF NOT EXISTS users
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  email TEXT UNIQUE NOT NULL,
                  password_hash TEXT NOT NULL,
                  display_name TEXT,
                  first_login INTEGER DEFAULT 1,
                  password_changed_at DATETIME,
                  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                  last_login DATETIME,
                  is_active INTEGER DEFAULT 1)''')

    # Add missing columns for password resets
    c.execute("PRAGMA table_info(users)")
    existing_cols = {row[1] for row in c.fetchall()}
    if 'reset_token' not in existing_cols:
        c.execute("ALTER TABLE users ADD COLUMN reset_token TEXT")
    if 'reset_token_expires' not in existing_cols:
        c.execute("ALTER TABLE users ADD COLUMN reset_token_expires INTEGER")
    
    # User sessions table
    c.execute('''CREATE TABLE IF NOT EXISTS user_sessions
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  user_id INTEGER,
                  token TEXT UNIQUE NOT NULL,
                  created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                  expires_at DATETIME,
                  FOREIGN KEY (user_id) REFERENCES users(id))''')
    
    conn.commit()
    conn.close()
    print(f"[AUTH] Users database initialized at {db_path}")

def hash_password(password):
    """Hash password using SHA-256 with salt"""
    salt = secrets.token_hex(16)
    password_hash = hashlib.sha256((password + salt).encode()).hexdigest()
    return f"{salt}:{password_hash}"

def verify_password(password, password_hash):
    """Verify password against hash"""
    try:
        salt, stored_hash = password_hash.split(':')
        computed_hash = hashlib.sha256((password + salt).encode()).hexdigest()
        return computed_hash == stored_hash
    except:
        return False

def generate_token(user_id, email):
    """Generate JWT token for user (or simple token if PyJWT not available)"""
    if HAS_JWT:
        payload = {
            'user_id': user_id,
            'email': email,
            'exp': datetime.utcnow().timestamp() + (7 * 24 * 60 * 60)  # 7 days
        }
        return jwt.encode(payload, JWT_SECRET, algorithm='HS256')
    else:
        # Simple token system (less secure, but works without PyJWT)
        import base64
        import json
        payload = {
            'user_id': user_id,
            'email': email,
            'exp': datetime.utcnow().timestamp() + (7 * 24 * 60 * 60)  # 7 days
        }
        token_data = base64.b64encode(json.dumps(payload).encode()).decode()
        return f"{token_data}.{hashlib.sha256((token_data + JWT_SECRET).encode()).hexdigest()[:32]}"

def validate_email_domain(email):
    """Validate that email is from @team.nxlink.com domain"""
    if not email:
        return False
    email = email.strip().lower()
    return email.endswith('@team.nxlink.com')

def verify_token(token):
    """Verify JWT token and return user info"""
    if not token:
        return None
    
    if HAS_JWT:
        try:
            payload = jwt.decode(token, JWT_SECRET, algorithms=['HS256'])
            return payload
        except jwt.ExpiredSignatureError:
            return None
        except jwt.InvalidTokenError:
            return None
    else:
        # Simple token verification
        try:
            import base64
            import json
            parts = token.split('.')
            if len(parts) != 2:
                return None
            
            token_data = parts[0]
            signature = parts[1]
            
            # Verify signature
            expected_sig = hashlib.sha256((token_data + JWT_SECRET).encode()).hexdigest()[:32]
            if signature != expected_sig:
                return None
            
            # Decode payload
            payload_str = base64.b64decode(token_data).decode()
            payload = json.loads(payload_str)
            
            # Check expiration
            if payload.get('exp', 0) < datetime.utcnow().timestamp():
                return None
            
            return payload
        except:
            return None

@app.route('/api/auth/login', methods=['POST'])
def auth_login():
    """Email/password login endpoint"""
    try:
        data = request.get_json()
        email = data.get('email', '').strip().lower()
        password = data.get('password', '')
        
        if not email or not password:
            return jsonify({'success': False, 'error': 'Email and password required'}), 400
        
        # Validate email domain - only @team.nxlink.com allowed
        if not validate_email_domain(email):
            return jsonify({
                'success': False, 
                'error': 'Only @team.nxlink.com email addresses are allowed. Please use your company email (e.g., netops@team.nxlink.com)'
            }), 403
        
        init_users_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'users.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        
        # Check if user exists
        c.execute('SELECT * FROM users WHERE email = ?', (email,))
        user = c.fetchone()
        
        if not user:
            # Create new user with default password
            password_hash = hash_password(DEFAULT_PASSWORD)
            c.execute('''INSERT INTO users (email, password_hash, display_name, first_login)
                         VALUES (?, ?, ?, 1)''',
                     (email, password_hash, email.split('@')[0]))
            conn.commit()
            user_id = c.lastrowid
            
            # Verify against default password
            if not verify_password(password, password_hash):
                conn.close()
                return jsonify({'success': False, 'error': 'Invalid email or password'}), 401
            
            # First login - require password change
            token = generate_token(user_id, email)
            c.execute('UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?', (user_id,))
            conn.commit()
            conn.close()
            
            return jsonify({
                'success': True,
                'token': token,
                'user': {
                    'id': user_id,
                    'email': email,
                    'displayName': email.split('@')[0],
                    'firstLogin': True
                },
                'requiresPasswordChange': True
            })
        else:
            # Existing user - verify password
            if not verify_password(password, user['password_hash']):
                conn.close()
                return jsonify({'success': False, 'error': 'Invalid email or password'}), 401
            
            # Check if first login (using default password)
            requires_password_change = user['first_login'] == 1
            
            # Update last login
            c.execute('UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?', (user['id'],))
            conn.commit()
            
            token = generate_token(user['id'], email)
            conn.close()
            
            return jsonify({
                'success': True,
                'token': token,
                'user': {
                    'id': user['id'],
                    'email': user['email'],
                    'displayName': user['display_name'] or email.split('@')[0],
                    'firstLogin': user['first_login'] == 1
                },
                'requiresPasswordChange': requires_password_change
            })
            
    except Exception as e:
        print(f"[AUTH ERROR] Login failed: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/auth/microsoft', methods=['POST'])
def auth_microsoft():
    """Microsoft SSO authentication endpoint"""
    try:
        # For production, implement full Microsoft OAuth2 flow
        # This is a placeholder that returns the OAuth URL
        if not AZURE_CLIENT_ID:
            return jsonify({
                'success': False,
                'error': 'Microsoft SSO not configured. Please set AZURE_CLIENT_ID environment variable.'
            }), 503
        
        # Note: Domain validation will be done in the OAuth callback
        # For now, we'll validate in the callback endpoint when implemented
        
        # Microsoft OAuth2 authorization URL
        auth_url = f"https://login.microsoftonline.com/{AZURE_TENANT_ID}/oauth2/v2.0/authorize"
        params = {
            'client_id': AZURE_CLIENT_ID,
            'response_type': 'code',
            'redirect_uri': f"{request.host_url}auth/callback",
            'response_mode': 'query',
            'scope': 'openid email profile',
            'state': secrets.token_urlsafe(32)
        }
        
        auth_url_full = f"{auth_url}?{'&'.join([f'{k}={v}' for k, v in params.items()])}"
        
        return jsonify({
            'success': True,
            'authUrl': auth_url_full
        })
        
    except Exception as e:
        print(f"[AUTH ERROR] Microsoft SSO failed: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/auth/change-password', methods=['POST'])
@require_auth
def change_password():
    """Change user password endpoint"""
    try:
        data = request.get_json()
        current_password = data.get('currentPassword', '')
        new_password = data.get('newPassword', '')
        user_info = request.current_user
        
        if not new_password or len(new_password) < 8:
            return jsonify({'success': False, 'error': 'New password must be at least 8 characters'}), 400
        
        init_users_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'users.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        
        c.execute('SELECT * FROM users WHERE id = ?', (user_info['user_id'],))
        user = c.fetchone()
        
        if not user:
            conn.close()
            return jsonify({'success': False, 'error': 'User not found'}), 404
        
        # If first login, current password might be default password
        # Otherwise, verify current password
        if user['first_login'] == 0:
            if not verify_password(current_password, user['password_hash']):
                conn.close()
                return jsonify({'success': False, 'error': 'Current password is incorrect'}), 401
        
        # Update password
        new_password_hash = hash_password(new_password)
        c.execute('''UPDATE users 
                     SET password_hash = ?, 
                         first_login = 0, 
                         password_changed_at = CURRENT_TIMESTAMP 
                     WHERE id = ?''',
                 (new_password_hash, user_info['user_id']))
        conn.commit()
        conn.close()
        
        return jsonify({'success': True, 'message': 'Password changed successfully'})
        
    except Exception as e:
        print(f"[AUTH ERROR] Password change failed: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/auth/forgot-password', methods=['POST'])
def forgot_password():
    """Request password reset (sends reset link - placeholder)"""
    try:
        data = request.get_json()
        email = data.get('email', '').strip().lower()
        
        if not email:
            return jsonify({'success': False, 'error': 'Email required'}), 400

        if not validate_email_domain(email):
            return jsonify({'success': False, 'error': 'Invalid email domain'}), 403
        
        init_users_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'users.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        c.execute('SELECT id, email FROM users WHERE email = ?', (email,))
        user = c.fetchone()

        reset_token = None
        if user:
            reset_token = secrets.token_urlsafe(32)
            expires_at = int(time.time()) + (60 * 60)  # 1 hour
            c.execute('''UPDATE users
                         SET reset_token = ?, reset_token_expires = ?
                         WHERE id = ?''',
                      (reset_token, expires_at, user['id']))
            conn.commit()

        conn.close()

        # In production, send password reset email.
        # For now, return token when available so UI can redirect to reset.
        return jsonify({
            'success': True,
            'message': 'If an account exists with this email, a password reset link has been sent.',
            'resetToken': reset_token
        })
            
    except Exception as e:
        print(f"[AUTH ERROR] Forgot password failed: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/auth/reset-password', methods=['POST'])
def reset_password():
    """Reset password using a reset token (no current password required)."""
    try:
        data = request.get_json()
        email = data.get('email', '').strip().lower()
        reset_token = data.get('resetToken', '').strip()
        new_password = data.get('newPassword', '')

        if not email or not reset_token or not new_password:
            return jsonify({'success': False, 'error': 'Email, reset token, and new password are required'}), 400

        if len(new_password) < 8:
            return jsonify({'success': False, 'error': 'New password must be at least 8 characters'}), 400

        if not validate_email_domain(email):
            return jsonify({'success': False, 'error': 'Invalid email domain'}), 403

        init_users_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'users.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        c.execute('SELECT * FROM users WHERE email = ?', (email,))
        user = c.fetchone()
        if not user:
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid reset token'}), 400

        expires_at = user['reset_token_expires'] or 0
        if user['reset_token'] != reset_token or int(time.time()) > int(expires_at):
            conn.close()
            return jsonify({'success': False, 'error': 'Invalid or expired reset token'}), 400

        new_password_hash = hash_password(new_password)
        c.execute('''UPDATE users
                     SET password_hash = ?,
                         first_login = 0,
                         password_changed_at = CURRENT_TIMESTAMP,
                         reset_token = NULL,
                         reset_token_expires = NULL
                     WHERE id = ?''',
                 (new_password_hash, user['id']))
        conn.commit()
        conn.close()

        return jsonify({'success': True, 'message': 'Password reset successfully'})

    except Exception as e:
        print(f"[AUTH ERROR] Reset password failed: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/auth/verify', methods=['POST'])
def verify_auth():
    """Verify authentication token"""
    try:
        data = request.get_json()
        token = data.get('token', '')
        
        if not token:
            return jsonify({'success': False, 'authenticated': False}), 401
        
        user_info = verify_token(token)
        if not user_info:
            return jsonify({'success': False, 'authenticated': False}), 401
        
        # Get user details
        init_users_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'users.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        
        c.execute('SELECT * FROM users WHERE id = ?', (user_info['user_id'],))
        user = c.fetchone()
        conn.close()
        
        if not user:
            return jsonify({'success': False, 'authenticated': False}), 404
        
        return jsonify({
            'success': True,
            'authenticated': True,
            'user': {
                'id': user['id'],
                'email': user['email'],
                'displayName': user['display_name'] or user['email'].split('@')[0],
                'firstLogin': user['first_login'] == 1
            }
        })
        
    except Exception as e:
        print(f"[AUTH ERROR] Verify failed: {e}")
        return jsonify({'success': False, 'authenticated': False}), 500

# ========================================
# ENDPOINT: Activity Tracking (Live Feed)
# ========================================

# In-memory activity store (last 100 activities)
# In production, this should be in a database
ACTIVITY_STORE = []
MAX_ACTIVITIES = 100

@app.route('/api/activity', methods=['GET', 'POST'])
def handle_activity():
    """Store and retrieve live activity feed"""
    global ACTIVITY_STORE
    
    if request.method == 'POST':
        # Store new activity
        try:
            activity = request.json
            if not activity:
                return jsonify({'error': 'No activity data'}), 400
            
            # Add timestamp if not present
            if 'timestamp' not in activity:
                activity['timestamp'] = get_cst_timestamp()
            
            # Add to store
            ACTIVITY_STORE.insert(0, activity)
            
            # Keep only last MAX_ACTIVITIES
            if len(ACTIVITY_STORE) > MAX_ACTIVITIES:
                ACTIVITY_STORE = ACTIVITY_STORE[:MAX_ACTIVITIES]
            
            safe_print(f"[ACTIVITY] {activity.get('username', 'Unknown')} - {activity.get('type', 'action')} - {activity.get('siteName', 'N/A')}")
            
            return jsonify({'success': True})
        except Exception as e:
            safe_print(f"[ACTIVITY ERROR] {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    else:
        # Retrieve activities
        try:
            # Return all activities, sorted by timestamp (newest first)
            sorted_activities = sorted(
                ACTIVITY_STORE,
                key=lambda x: x.get('timestamp', ''),
                reverse=True
            )
            return jsonify(sorted_activities[:50])  # Return last 50
        except Exception as e:
            safe_print(f"[ACTIVITY ERROR] {str(e)}")
            return jsonify({'error': str(e)}), 500

# ========================================
# RUN SERVER
# ========================================

# ========================================
# ENDPOINT 6: Completed Configs Storage
# ========================================

# CONFIGS_DB_PATH will be set lazily when ensure_configs_db() is called
CONFIGS_DB_PATH = None

def init_feedback_db():
    """Initialize feedback database in secure location and return its path."""
    global SECURE_DATA_DIR, FEEDBACK_DB_PATH
    SECURE_DATA_DIR = ensure_secure_data_dir()
    FEEDBACK_DB_PATH = SECURE_DATA_DIR / "feedback.db"

    conn = sqlite3.connect(str(FEEDBACK_DB_PATH))
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            feedback_type TEXT NOT NULL,
            subject TEXT NOT NULL,
            category TEXT,
            experience TEXT,
            details TEXT NOT NULL,
            name TEXT,
            email TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            status TEXT DEFAULT 'new',
            admin_notes TEXT
        )
    ''')

    cursor.execute('CREATE INDEX IF NOT EXISTS idx_feedback_type ON feedback(feedback_type)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_feedback_timestamp ON feedback(timestamp)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_feedback_status ON feedback(status)')

    conn.commit()
    conn.close()
    safe_print(f"[FEEDBACK] Database initialized: {FEEDBACK_DB_PATH}")
    return FEEDBACK_DB_PATH

def init_configs_db():
    """Initialize completed configs database in secure location"""
    global SECURE_DATA_DIR, CONFIGS_DB_PATH
    SECURE_DATA_DIR = ensure_secure_data_dir()  # Ensure directory exists
    CONFIGS_DB_PATH = SECURE_DATA_DIR / "completed_configs.db"
    conn = sqlite3.connect(str(CONFIGS_DB_PATH))
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS completed_configs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            config_type TEXT NOT NULL,
            device_name TEXT,
            device_type TEXT,
            customer_code TEXT,
            loopback_ip TEXT,
            routeros_version TEXT,
            config_content TEXT NOT NULL,
            port_mapping TEXT,
            metadata TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            created_by TEXT
        )
    ''')
    
    # Create indexes for faster searching
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_config_type ON completed_configs(config_type)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_customer_code ON completed_configs(customer_code)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON completed_configs(created_at)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_device_type ON completed_configs(device_type)')
    
    conn.commit()
    conn.close()
    print(f"[CONFIGS] Database initialized: {CONFIGS_DB_PATH}")

def extract_port_mapping(config_content):
    """Extract port mapping information from config with IP addresses and backhaul calculations.

    This parser must handle RouterOS tokens with quoted values (including spaces), e.g.:
      - comment="ZAYO DF to ALEDO-NO-1"
      - interface="vlan444-IA-MISSOURIVALLEY-SE-3 BBU MGMT"
    """
    port_mapping = {}

    def _safe_shlex_split(line: str):
        import shlex
        try:
            return shlex.split(line, posix=True)
        except Exception:
            return line.split()

    def _parse_kv(tokens):
        kv = {}
        for t in tokens:
            if '=' in t:
                k, v = t.split('=', 1)
                kv[k.strip()] = v.strip()
        return kv

    def _infer_port_type(name: str) -> str:
        n = (name or '').strip()
        if n.startswith('ether'):
            return 'ethernet'
        if 'sfp28' in n:
            return 'sfp28'
        if 'sfp' in n:
            return 'sfp'
        return 'unknown'

    # Step 1: Extract interface comments (primarily from /interface ethernet set [ find default-name=... ])
    current_section = None
    for raw in (config_content or '').splitlines():
        line = raw.strip()
        if not line or line.startswith('#'):
            continue
        if line.startswith('/'):
            # Handle one-line export forms like: "/interface ethernet set ..."
            if line.startswith('/interface ethernet') and ' set ' in line:
                current_section = '/interface ethernet'
                inline = line[len('/interface ethernet'):].strip()
                if inline.startswith('set '):
                    tokens = _safe_shlex_split(inline)
                    kv = _parse_kv(tokens)
                    default_name = None
                    for t in tokens:
                        if t.startswith('default-name='):
                            default_name = t.split('=', 1)[1].strip()
                            break
                    comment = kv.get('comment')
                    if default_name and comment:
                        if default_name not in port_mapping:
                            port_mapping[default_name] = {}
                        port_mapping[default_name]['comment'] = comment.strip().strip('"').strip("'")
                        port_mapping[default_name]['type'] = _infer_port_type(default_name)
                continue

            current_section = line
            continue
        if current_section == '/interface ethernet' and line.startswith('set '):
            tokens = _safe_shlex_split(line)
            kv = _parse_kv(tokens)
            # Find default-name in tokens (it may appear as default-name=... inside the [ find ... ])
            default_name = None
            for t in tokens:
                if t.startswith('default-name='):
                    default_name = t.split('=', 1)[1].strip()
                    break
            comment = kv.get('comment')
            if default_name and comment:
                if default_name not in port_mapping:
                    port_mapping[default_name] = {}
                port_mapping[default_name]['comment'] = comment.strip().strip('"').strip("'")
                port_mapping[default_name]['type'] = _infer_port_type(default_name)

    # Step 2: Extract IP addresses assigned to interfaces from /ip address section
    all_ip_matches = []
    current_section = None
    for raw in (config_content or '').splitlines():
        line = raw.strip()
        if not line or line.startswith('#'):
            continue
        if line.startswith('/'):
            # Handle one-line export forms like: "/ip address add ..."
            if line.startswith('/ip address') and ' add ' in line:
                current_section = '/ip address'
                inline = line[len('/ip address'):].strip()
                if inline.startswith('add '):
                    tokens = _safe_shlex_split(inline)
                    kv = _parse_kv(tokens)
                    ip_cidr = kv.get('address')
                    interface = kv.get('interface')
                    comment = kv.get('comment', '')
                    if ip_cidr and interface:
                        all_ip_matches.append((ip_cidr.strip(), interface.strip(), (comment or '').strip()))
                continue

            current_section = line
            continue
        if current_section == '/ip address' and line.startswith('add '):
            tokens = _safe_shlex_split(line)
            kv = _parse_kv(tokens)
            ip_cidr = kv.get('address')
            interface = kv.get('interface')
            comment = kv.get('comment', '')
            if ip_cidr and interface:
                all_ip_matches.append((ip_cidr.strip(), interface.strip(), (comment or '').strip()))
    
    # Remove duplicates
    seen = set()
    unique_matches = []
    for ip_cidr, interface, comment in all_ip_matches:
        key = (ip_cidr, interface)
        if key not in seen:
            seen.add(key)
            unique_matches.append((ip_cidr, interface, comment))
    all_ip_matches = unique_matches
    
    # Group IPs by subnet to calculate backhaul IPs
    subnet_groups = {}  # subnet -> list of (interface, ip, comment)
    direct_ips = {}  # interface -> ip (for non-/29 subnets)
    
    for ip_cidr, interface, ip_comment in all_ip_matches:
        interface_clean = interface.strip()
        ip_cidr_clean = ip_cidr.strip()
        ip_comment_clean = ip_comment.strip().strip('"').strip("'") if ip_comment else None
        
        if '/' in ip_cidr_clean:
            ip_addr, prefix = ip_cidr_clean.split('/')
            
            # Parse IP to integer for calculations
            try:
                ip_parts = ip_addr.split('.')
                if len(ip_parts) == 4:
                    ip_int = (int(ip_parts[0]) << 24) + (int(ip_parts[1]) << 16) + (int(ip_parts[2]) << 8) + int(ip_parts[3])
                    prefix_num = int(prefix)
                    
                    # Calculate network address
                    mask = (0xFFFFFFFF << (32 - prefix_num)) & 0xFFFFFFFF
                    network_int = ip_int & mask
                    
                    # Convert network back to IP string
                    network_ip = f"{((network_int >> 24) & 0xFF)}.{((network_int >> 16) & 0xFF)}.{((network_int >> 8) & 0xFF)}.{network_int & 0xFF}"
                    
                    # For /29 subnets, group them to calculate all backhaul IPs
                    if prefix_num == 29:
                        subnet_key = f"{network_ip}/{prefix}"
                        if subnet_key not in subnet_groups:
                            subnet_groups[subnet_key] = []
                        
                        # Calculate which IP this is in the subnet
                        ip_offset = ip_int - network_int
                        subnet_groups[subnet_key].append({
                            'interface': interface_clean,
                            'ip': ip_cidr_clean,
                            'ip_int': ip_int,
                            'offset': ip_offset,
                            'comment': ip_comment_clean
                        })
                    else:
                        # For other subnets, store directly
                        direct_ips[interface_clean] = {
                            'ip': ip_cidr_clean,
                            'comment': ip_comment_clean
                        }
            except (ValueError, IndexError) as e:
                # If IP parsing fails, just store as-is
                direct_ips[interface_clean] = {
                    'ip': ip_cidr_clean,
                    'comment': ip_comment_clean
                }
    
    # Step 3: Process /29 subnets and calculate all backhaul IPs
    for subnet_cidr, ip_list in subnet_groups.items():
        if len(ip_list) > 0:
            # Sort by IP offset to get sequential order
            ip_list.sort(key=lambda x: x['offset'])
            
            # Get network IP and prefix
            network_ip, prefix = subnet_cidr.split('/')
            network_parts = network_ip.split('.')
            network_int = (int(network_parts[0]) << 24) + (int(network_parts[1]) << 16) + (int(network_parts[2]) << 8) + int(network_parts[3])
            
            # For /29, calculate all sequential IPs: network+1, network+2, network+3, network+4
            # Backhaul pattern: network+1 (tower gateway), network+2 (feeding), network+3 (landing), network+4 (customer gateway)
            for ip_info in ip_list:
                interface = ip_info['interface']
                # Use the actual IP from config (already calculated correctly)
                calculated_ip = ip_info['ip']  # Keep original IP format
                
                if interface not in port_mapping:
                    port_mapping[interface] = {}
                port_mapping[interface]['ip_address'] = calculated_ip
                # Prefer IP comment, fallback to interface comment
                if ip_info['comment']:
                    port_mapping[interface]['ip_comment'] = ip_info['comment']
                elif 'comment' not in port_mapping[interface]:
                    # If no IP comment, keep interface comment if it exists
                    pass
    
    # Step 4: Add direct IPs (non-/29 subnets)
    for interface, ip_info in direct_ips.items():
        if interface not in port_mapping:
            port_mapping[interface] = {}
        port_mapping[interface]['ip_address'] = ip_info['ip']
        if ip_info['comment']:
            port_mapping[interface]['ip_comment'] = ip_info['comment']
    
    # Step 5: Extract bridge port assignments to map bridge IPs to physical interfaces
    # Pattern: /interface bridge port add bridge=nat-bridge interface=ether8
    bridge_port_matches = []
    current_section = None
    for raw in (config_content or '').splitlines():
        line = raw.strip()
        if not line or line.startswith('#'):
            continue
        if line.startswith('/'):
            # Handle one-line export forms like: "/interface bridge port add ..."
            if line.startswith('/interface bridge port') and ' add ' in line:
                current_section = '/interface bridge port'
                inline = line[len('/interface bridge port'):].strip()
                if inline.startswith('add '):
                    tokens = _safe_shlex_split(inline)
                    kv = _parse_kv(tokens)
                    br = kv.get('bridge')
                    iface = kv.get('interface')
                    if br and iface:
                        bridge_port_matches.append((br.strip(), iface.strip()))
                continue

            current_section = line
            continue
        if current_section == '/interface bridge port' and line.startswith('add '):
            tokens = _safe_shlex_split(line)
            kv = _parse_kv(tokens)
            br = kv.get('bridge')
            iface = kv.get('interface')
            if br and iface:
                bridge_port_matches.append((br.strip(), iface.strip()))
    
    # Create mapping: bridge_name -> list of member interfaces
    bridge_to_interfaces = {}
    interface_to_bridge = {}
    for bridge, interface in bridge_port_matches:
        bridge_clean = bridge.strip()
        interface_clean = interface.strip()
        if bridge_clean not in bridge_to_interfaces:
            bridge_to_interfaces[bridge_clean] = []
        bridge_to_interfaces[bridge_clean].append(interface_clean)
        interface_to_bridge[interface_clean] = bridge_clean
    
    # Step 6: Map bridge IPs to their member interfaces
    # If an IP is assigned to a bridge, assign it to all member interfaces
    # This handles: public-bridge (CX HANDOFF), nat-bridge (NAT), and other bridges
    for bridge_name, member_interfaces in bridge_to_interfaces.items():
        # Find IPs assigned to this bridge
        for ip_cidr, interface_name, ip_comment in all_ip_matches:
            if interface_name.strip() == bridge_name:
                ip_comment_clean = ip_comment.strip().strip('"').strip("'") if ip_comment else None
                # Assign this IP to all member interfaces
                for member_if in member_interfaces:
                    if member_if not in port_mapping:
                        port_mapping[member_if] = {}
                        # Infer type from interface name
                        if member_if.startswith('ether'):
                            port_mapping[member_if]['type'] = 'ethernet'
                        elif 'sfp28' in member_if:
                            port_mapping[member_if]['type'] = 'sfp28'
                        elif 'sfp' in member_if:
                            port_mapping[member_if]['type'] = 'sfp'
                    # Only assign if interface doesn't already have an IP
                    if 'ip_address' not in port_mapping[member_if]:
                        port_mapping[member_if]['ip_address'] = ip_cidr.strip()
                        if ip_comment_clean:
                            port_mapping[member_if]['ip_comment'] = ip_comment_clean
                        # If interface has a comment, keep it (e.g., "CX HANDOFF" for public port)
                        # The IP comment will be used for display, but interface comment is preserved
    
    # Step 7: For interfaces without IPs but with comments, try to find matching IPs
    # This handles cases where IP is assigned but interface comment extraction missed it
    for interface, info in port_mapping.items():
        if 'ip_address' not in info and 'comment' in info:
            # Try to find IP by matching interface name in IP assignments
            for ip_cidr, interface_name, ip_comment in all_ip_matches:
                if interface_name.strip() == interface:
                    info['ip_address'] = ip_cidr.strip()
                    if ip_comment:
                        info['ip_comment'] = ip_comment.strip().strip('"').strip("'")
                    break
            # If still no IP, check if interface is a bridge member
            if 'ip_address' not in info and interface in interface_to_bridge:
                bridge_name = interface_to_bridge[interface]
                # Look for IPs assigned to this bridge
                for ip_cidr, interface_name, ip_comment in all_ip_matches:
                    if interface_name.strip() == bridge_name:
                        info['ip_address'] = ip_cidr.strip()
                        if ip_comment:
                            info['ip_comment'] = ip_comment.strip().strip('"').strip("'")
                        break
    
    return port_mapping

def format_port_mapping_text(port_mapping, device_name='', customer_code=''):
    """
    Format port mapping in the requested format:
    
    ===================================================================
    BH IPs/Port Map
    ===================================================================
    
    NXLink160535.ether#: 10.45.250.65/28
    ROBINSON-NXLink160535: 10.45.250.66/28
    ...
    """
    if not port_mapping:
        return ""
    
    lines = []
    lines.append("=" * 67)
    lines.append("BH IPs/Port Map")
    lines.append("=" * 67)
    lines.append("")
    
    # Sort ports for consistent output
    # Priority: interfaces with IPs first, then by IP address value (for sequential ordering), then by port type
    def get_sort_key(x):
        port, info = x
        ip_addr = info.get('ip_address', '')
        
        # Extract IP value for sorting (convert to integer for proper numeric sort)
        ip_value = 0
        if ip_addr and '/' in ip_addr:
            try:
                ip_str = ip_addr.split('/')[0]
                ip_parts = ip_str.split('.')
                if len(ip_parts) == 4:
                    ip_value = (int(ip_parts[0]) << 24) + (int(ip_parts[1]) << 16) + (int(ip_parts[2]) << 8) + int(ip_parts[3])
            except:
                pass
        
        return (
            0 if ip_addr else 1,  # Interfaces with IPs first
            ip_value,  # Then by IP address value (sequential ordering)
            {'ethernet': 0, 'sfp': 1, 'sfp28': 1}.get(info.get('type', ''), 2),  # Then by port type
            int(re.search(r'\d+', port).group()) if re.search(r'\d+', port) else 999  # Finally by port number
        )
    
    sorted_ports = sorted(port_mapping.items(), key=get_sort_key)
    
    for port, info in sorted_ports:
        comment = info.get('comment', '').strip()
        ip_comment = info.get('ip_comment', '').strip()
        ip_address = info.get('ip_address', '')
        
        # Prefer IP comment if available, otherwise use interface comment
        display_comment = ip_comment if ip_comment else comment
        
        # Format: comment.port: ip_address
        if display_comment and ip_address:
            # Clean up comment (remove quotes, extra spaces)
            comment_clean = display_comment.strip('"').strip("'").strip()
            # Format as requested: comment.port: ip/subnet
            # Replace port number with # if it's ethernet (e.g., ether7 -> ether#)
            port_display = port
            if port.startswith('ether') and re.search(r'\d+', port):
                port_display = re.sub(r'\d+', '#', port)
            line = f"{comment_clean}.{port_display}: {ip_address}"
            lines.append(line)
        elif comment and ip_address:
            # Fallback to interface comment
            comment_clean = comment.strip('"').strip("'").strip()
            port_display = port
            if port.startswith('ether') and re.search(r'\d+', port):
                port_display = re.sub(r'\d+', '#', port)
            line = f"{comment_clean}.{port_display}: {ip_address}"
            lines.append(line)
        elif display_comment:
            # Port with comment but no IP
            comment_clean = display_comment.strip('"').strip("'").strip()
            port_display = port
            if port.startswith('ether') and re.search(r'\d+', port):
                port_display = re.sub(r'\d+', '#', port)
            line = f"{comment_clean}.{port_display}: (no IP)"
            lines.append(line)
        elif ip_address:
            # Port with IP but no comment
            port_display = port
            if port.startswith('ether') and re.search(r'\d+', port):
                port_display = re.sub(r'\d+', '#', port)
            line = f"{port_display}: {ip_address}"
            lines.append(line)
    
    lines.append("")
    return "\n".join(lines)

@app.route('/api/save-completed-config', methods=['POST'])
def save_completed_config():
    """Save a completed configuration to the database"""
    try:
        ensure_configs_db()  # Lazy init
        data = request.get_json(force=True)
        
        config_type = data.get('config_type', 'unknown')  # 'tower', 'enterprise', 'mpls-enterprise'
        device_name = data.get('device_name', '')
        device_type = data.get('device_type', '')
        customer_code = data.get('customer_code', '')
        loopback_ip = data.get('loopback_ip', '')
        routeros_version = data.get('routeros_version', '')
        config_content = data.get('config_content', '')
        
        if not config_content:
            return jsonify({'error': 'No configuration content provided'}), 400
        
        # Extract port mapping
        port_mapping = extract_port_mapping(config_content)
        port_mapping_json = json.dumps(port_mapping) if port_mapping else None
        
        # Store metadata - merge standard fields with any additional metadata
        metadata = {
            'site_name': data.get('site_name', ''),
            'router_id': data.get('router_id', ''),
            'lan_bridge_ip': data.get('lan_bridge_ip', ''),
            'ospf_area': data.get('ospf_area', ''),
            'bgp_peers': data.get('bgp_peers', []),
            'uplinks': data.get('uplinks', [])
        }
        # Merge any additional metadata passed from frontend (e.g., migration metadata)
        if 'metadata' in data and isinstance(data.get('metadata'), dict):
            metadata.update(data['metadata'])
        metadata_json = json.dumps(metadata)
        
        # Get current time in Central Standard Time (CST/CDT)
        cst_timestamp = get_cst_datetime_string()
        
        conn = sqlite3.connect(str(CONFIGS_DB_PATH))
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO completed_configs 
            (config_type, device_name, device_type, customer_code, loopback_ip, routeros_version, 
             config_content, port_mapping, metadata, created_by, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (config_type, device_name, device_type, customer_code, loopback_ip, routeros_version,
              config_content, port_mapping_json, metadata_json, data.get('created_by', 'user'), cst_timestamp))
        
        config_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return jsonify({
            'success': True,
            'config_id': config_id,
            'message': 'Configuration saved successfully'
        })
        
    except Exception as e:
        print(f"[ERROR] Failed to save config: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/get-completed-configs', methods=['GET'])
def get_completed_configs():
    """Get all completed configurations with optional filtering"""
    try:
        ensure_configs_db()  # Lazy init
        
        search_term = request.args.get('search', '').strip()
        year_filter = request.args.get('year', '').strip()
        type_filter = request.args.get('type', '').strip()
        
        conn = sqlite3.connect(str(CONFIGS_DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        query = 'SELECT * FROM completed_configs WHERE 1=1'
        params = []
        
        if type_filter:
            query += ' AND config_type = ?'
            params.append(type_filter)
        
        if year_filter:
            query += ' AND strftime("%Y", created_at) = ?'
            params.append(year_filter)
        
        if search_term:
            query += ' AND (device_name LIKE ? OR customer_code LIKE ? OR device_type LIKE ? OR loopback_ip LIKE ? OR config_content LIKE ?)'
            search_pattern = f'%{search_term}%'
            params.extend([search_pattern] * 5)
        
        query += ' ORDER BY created_at DESC'
        
        cursor.execute(query, params)
        configs = [dict(row) for row in cursor.fetchall()]
        
        # Get unique years
        cursor.execute('SELECT DISTINCT strftime("%Y", created_at) as year FROM completed_configs ORDER BY year DESC')
        years = [row[0] for row in cursor.fetchall() if row[0]]
        
        conn.close()
        
        return jsonify({
            'configs': configs,
            'years': years
        })
        
    except Exception as e:
        print(f"[ERROR] Failed to get configs: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/get-completed-config/<int:config_id>', methods=['GET'])
def get_completed_config(config_id):
    """Get a specific completed configuration by ID"""
    try:
        ensure_configs_db()  # Lazy init
        
        conn = sqlite3.connect(str(CONFIGS_DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM completed_configs WHERE id = ?', (config_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            return jsonify({'error': 'Configuration not found'}), 404
        
        config = dict(row)

        # Always return a prettified config for display/copy/download.
        # This does not change the stored config/history (DB content is untouched).
        if config.get('config_content'):
            try:
                config['config_content'] = format_config_spacing(config['config_content'])
            except Exception:
                pass
        
        # Parse JSON fields (and re-extract port map if stored map is missing or clearly incomplete)
        port_mapping = {}
        if config.get('port_mapping'):
            try:
                port_mapping = json.loads(config['port_mapping'])
            except Exception:
                port_mapping = {}

        def _looks_incomplete(pm: dict, content: str) -> bool:
            if not isinstance(pm, dict) or not pm:
                # If config has /ip address entries but no port map, it's incomplete.
                return bool(re.search(r'(?m)^/ip address\\s*$', content or ''))
            # If every entry lacks an IP, treat as incomplete.
            has_any_ip = any(isinstance(v, dict) and v.get('ip_address') for v in pm.values())
            if not has_any_ip:
                return bool(re.search(r'(?m)^/ip address\\s*$', content or ''))
            return False

        if _looks_incomplete(port_mapping, config.get('config_content', '')):
            try:
                port_mapping = extract_port_mapping(config.get('config_content', '') or '')
                # Best-effort: persist refreshed map (does not alter config content/history)
                try:
                    conn2 = sqlite3.connect(str(CONFIGS_DB_PATH))
                    c2 = conn2.cursor()
                    c2.execute('UPDATE completed_configs SET port_mapping = ? WHERE id = ?', (json.dumps(port_mapping), config_id))
                    conn2.commit()
                    conn2.close()
                except Exception:
                    pass
            except Exception:
                pass

        config['port_mapping'] = port_mapping
        
        # Add formatted port mapping text
        config['port_mapping_text'] = format_port_mapping_text(
            port_mapping, 
            config.get('device_name', ''), 
            config.get('customer_code', '')
        )
        
        if config.get('metadata'):
            try:
                config['metadata'] = json.loads(config['metadata'])
            except:
                config['metadata'] = {}
        else:
                config['metadata'] = {}
        
        return jsonify(config)
        
    except Exception as e:
        print(f"[ERROR] Failed to get config: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/download-port-map/<int:config_id>', methods=['GET'])
def download_port_map(config_id):
    """Download a plain-text port map for a completed configuration."""
    try:
        ensure_configs_db()  # Lazy init

        conn = sqlite3.connect(str(CONFIGS_DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute('SELECT id, device_name, customer_code, port_mapping FROM completed_configs WHERE id = ?', (config_id,))
        row = cursor.fetchone()
        conn.close()

        if not row:
            return jsonify({'error': 'Configuration not found'}), 404

        port_mapping = {}
        if row['port_mapping']:
            try:
                port_mapping = json.loads(row['port_mapping'])
            except Exception:
                port_mapping = {}

        text = format_port_mapping_text(port_mapping, row['device_name'] or '', row['customer_code'] or '')
        if not text.strip():
            text = "No port mapping available for this configuration.\n"

        import io
        safe_name = (row['device_name'] or row['customer_code'] or f"config-{row['id']}").replace(' ', '_')
        filename = f"{safe_name}-port-map.txt"
        return send_file(
            io.BytesIO(text.encode('utf-8')),
            mimetype='text/plain; charset=utf-8',
            as_attachment=True,
            download_name=filename,
        )

    except Exception as e:
        print(f"[ERROR] Failed to download port map: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/extract-port-map', methods=['POST'])
def extract_port_map():
    """Extract a port map from raw config content without saving a config."""
    try:
        data = request.get_json(force=True)
        config_content = (data.get('config_content') or '').strip()
        if not config_content:
            return jsonify({'error': 'No configuration content provided'}), 400

        device_name = data.get('device_name', '')
        customer_code = data.get('customer_code', '')
        port_mapping = extract_port_mapping(config_content)
        port_map_text = format_port_mapping_text(port_mapping, device_name, customer_code)
        return jsonify({
            'port_mapping': port_mapping,
            'port_map_text': port_map_text
        })
    except Exception as e:
        print(f"[ERROR] Failed to extract port map: {e}")
        return jsonify({'error': str(e)}), 500

# Lazy initialize configs database - only when first accessed
_configs_db_initialized = False
def ensure_configs_db():
    """Ensure configs database is initialized (lazy)"""
    global _configs_db_initialized
    if not _configs_db_initialized:
        try:
            ensure_migration()  # Run migration first
            init_configs_db()
            _configs_db_initialized = True
        except Exception as e:
            print(f"[CONFIGS] Error initializing database: {e}")
            _configs_db_initialized = True  # Mark as attempted to prevent loops

# ========================================
# ACTIVITY TRACKING ENDPOINTS
# ========================================
def init_activity_db():
    """Initialize activity tracking database"""
    secure_dir = 'secure_data'
    if not os.path.exists(secure_dir):
        os.makedirs(secure_dir)
    
    db_path = os.path.join(secure_dir, 'activity_log.db')
    conn = sqlite3.connect(db_path)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS activities
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  username TEXT,
                  activity_type TEXT,
                  device TEXT,
                  site_name TEXT,
                  routeros_version TEXT,
                  success INTEGER,
                  timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
                  timestamp_unix INTEGER)''')
    conn.commit()

    # Ensure older databases get the new columns/indexes (safe to re-run).
    try:
        cols = {row[1] for row in c.execute("PRAGMA table_info(activities)").fetchall()}
        if 'timestamp_unix' not in cols:
            c.execute("ALTER TABLE activities ADD COLUMN timestamp_unix INTEGER")
        if 'timestamp' not in cols:
            c.execute("ALTER TABLE activities ADD COLUMN timestamp TEXT")
    except Exception as e:
        safe_print(f"[ACTIVITY] Column migration skipped: {e}")

    try:
        c.execute('CREATE INDEX IF NOT EXISTS idx_activity_timestamp_unix ON activities(timestamp_unix)')
    except Exception:
        pass
    conn.commit()
    conn.close()
    print(f"[ACTIVITY] Database initialized at {db_path}")

@app.route('/api/log-activity', methods=['POST'])
def log_activity():
    """Log user activity for live feed with authenticated user info"""
    try:
        data = request.get_json()
        
        # Get user from token if provided
        token = data.get('token') or request.headers.get('Authorization', '').replace('Bearer ', '')
        user_email = 'Anonymous'
        user_display_name = 'Anonymous'
        
        if token:
            user_info = verify_token(token)
            if user_info:
                user_email = user_info.get('email', 'Anonymous')
                # Get display name from database
                init_users_db()
                secure_dir = 'secure_data'
                db_path = os.path.join(secure_dir, 'users.db')
                conn = sqlite3.connect(db_path)
                conn.row_factory = sqlite3.Row
                c = conn.cursor()
                c.execute('SELECT display_name, email FROM users WHERE id = ?', (user_info['user_id'],))
                user = c.fetchone()
                if user:
                    user_display_name = user['display_name'] or user['email'].split('@')[0]
                conn.close()
        
        # Fallback to username from data if no token
        username = data.get('username') or user_display_name
        activity_type = data.get('type', 'unknown')
        device = data.get('device', '')
        site_name = data.get('siteName', '')
        routeros = data.get('routeros', '')
        success = 1 if data.get('success', True) else 0
        
        # Initialize DB if needed
        init_activity_db()
        
        # Store activity with user email
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'activity_log.db')
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        ts_unix = get_unix_timestamp()
        ts_iso = get_utc_timestamp()
        c.execute('''INSERT INTO activities 
                     (username, activity_type, device, site_name, routeros_version, success, timestamp, timestamp_unix)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                  (username, activity_type, device, site_name, routeros, success, ts_iso, ts_unix))
        conn.commit()
        conn.close()
        
        # Format activity message (use short username for display)
        display_user = _short_username(username)
        activity_msg = f"{display_user}"
        if activity_type == 'migration':
            activity_msg += f" migrated {site_name}"
        elif activity_type == 'new-config':
            activity_msg += f" configured {site_name}"
        else:
            activity_msg += f" - {activity_type} - {site_name}"
        if device:
            activity_msg += f" ({device})"
        
        print(f"[ACTIVITY] {activity_msg}")
        return jsonify({'success': True})
    except Exception as e:
        print(f"[ERROR] Failed to log activity: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/get-activity', methods=['GET'])
def get_activity():
    """Get recent activities for live feed with formatted messages"""
    try:
        limit = request.args.get('limit', 50, type=int)
        all_activities = request.args.get('all', 'false').lower() == 'true'  # For log history tab
        
        # Initialize DB if needed
        init_activity_db()
        
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'activity_log.db')
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        cols = {row[1] for row in c.execute("PRAGMA table_info(activities)").fetchall()}
        has_unix = 'timestamp_unix' in cols
        
        if all_activities:
            # Get all activities for log history
            if has_unix:
                c.execute('''SELECT * FROM activities 
                             ORDER BY timestamp_unix DESC 
                             LIMIT ?''', (limit,))
            else:
                c.execute('''SELECT * FROM activities 
                             ORDER BY timestamp DESC 
                             LIMIT ?''', (limit,))
        else:
            # Get recent activities (last 24 hours)
            if has_unix:
                cutoff = int(time.time()) - (24 * 60 * 60)
                c.execute('''SELECT * FROM activities 
                             WHERE timestamp_unix >= ?
                             ORDER BY timestamp_unix DESC 
                             LIMIT ?''', (cutoff, limit))
            else:
                c.execute('''SELECT * FROM activities 
                             WHERE timestamp >= datetime('now', '-24 hours')
                             ORDER BY timestamp DESC 
                             LIMIT ?''', (limit,))
        
        rows = c.fetchall()
        conn.close()
        
        activities = []
        for row in rows:
            # Format activity message
            username = row['username']
            display_user = _short_username(username)
            activity_type = row['activity_type']
            site_name = row['site_name'] or 'Unknown'
            device = row['device'] or ''

            if activity_type == 'migration':
                message = f"{display_user} migrated {site_name}"
            elif activity_type == 'new-config':
                message = f"{display_user} configured {site_name}"
            else:
                message = f"{display_user} - {activity_type} - {site_name}"

                if device:
                    message += f" ({device})"

            # Normalize timestamp to ISO UTC for reliable frontend parsing; also return a CST display string.
            dt_utc = None
            ts_unix = row['timestamp_unix'] if has_unix else None
            if ts_unix:
                try:
                    dt_utc = datetime.fromtimestamp(int(ts_unix), tz=timezone.utc)
                except Exception:
                    dt_utc = None
            if dt_utc is None:
                raw_ts = row['timestamp']
                try:
                    dt_utc = datetime.fromisoformat(raw_ts.replace('Z', '+00:00')).astimezone(timezone.utc)
                except Exception:
                    try:
                        naive = datetime.strptime(raw_ts, '%Y-%m-%d %H:%M:%S')
                        if CST_ZONEINFO is not None:
                            dt_local = naive.replace(tzinfo=CST_ZONEINFO)
                        elif CST_PYTZ is not None:
                            dt_local = CST_PYTZ.localize(naive)
                        else:
                            dt_local = naive.replace(tzinfo=timezone(timedelta(hours=-6)))
                        dt_utc = dt_local.astimezone(timezone.utc)
                    except Exception:
                        dt_utc = None

            if dt_utc is not None:
                timestamp_iso = dt_utc.isoformat().replace('+00:00', 'Z')
                try:
                    formatted_time = dt_utc.astimezone(get_cst_now().tzinfo).strftime('%m/%d/%Y %I:%M%p')
                except Exception:
                    formatted_time = timestamp_iso
            else:
                timestamp_iso = row['timestamp']
                formatted_time = row['timestamp']

            activities.append({
                'id': row['id'],
                'username': display_user,
                'type': row['activity_type'],
                'device': row['device'],
                'siteName': row['site_name'],
                'routeros': row['routeros_version'],
                'success': bool(row['success']),
                'timestamp': timestamp_iso,
                'timestamp_unix': int(ts_unix) if ts_unix else None,
                'formattedTime': formatted_time,
                'message': message
            })
        
        return jsonify({'success': True, 'activities': activities})
    except Exception as e:
        print(f"[ERROR] Failed to get activities: {e}")
        return jsonify({'success': False, 'error': str(e), 'activities': []}), 500


# ========================================
# AVIAT BACKHAUL UPDATER API
# ========================================

def _aviat_should_log(result):
    status = (result or {}).get('status')
    if status in ('scheduled', 'manual', 'aborted', 'loading', 'reboot_required', 'reboot_pending', 'rebooting'):
        return False
    return True

def _log_aviat_activity(result):
    try:
        if not _aviat_should_log(result):
            return
        init_activity_db()
        secure_dir = 'secure_data'
        db_path = os.path.join(secure_dir, 'activity_log.db')
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        ts_unix = get_unix_timestamp()
        ts_iso = get_utc_timestamp()
        username = _short_username(result.get('username') or 'aviat-tool')
        activity_type = 'aviat-upgrade'
        site_name = result.get('ip') or 'Unknown'
        device = 'Aviat Backhaul'
        routeros = result.get('firmware_version_after') or result.get('firmware_version_before') or ''
        success = 1 if result.get('success') else 0
        c.execute('''INSERT INTO activities 
                     (username, activity_type, device, site_name, routeros_version, success, timestamp, timestamp_unix)
                     VALUES (?, ?, ?, ?, ?, ?, ?, ?)''',
                  (username, activity_type, device, site_name, routeros, success, ts_iso, ts_unix))
        conn.commit()
        conn.close()
    except Exception as e:
        safe_print(f"[AVIAT] Failed to log activity: {e}")

def _aviat_result_dict(result, username=None):
    payload = {
        'ip': result.ip,
        'success': result.success,
        'status': getattr(result, 'status', 'completed'),
        'firmware_downloaded': result.firmware_downloaded,
        'firmware_downloaded_version': getattr(result, 'firmware_downloaded_version', None),
        'firmware_scheduled': result.firmware_scheduled,
        'firmware_activated': result.firmware_activated,
        'password_changed': result.password_changed,
        'snmp_configured': result.snmp_configured,
        'buffer_configured': result.buffer_configured,
        'sop_checked': result.sop_checked,
        'sop_passed': result.sop_passed,
        'sop_results': result.sop_results,
        'firmware_version_before': result.firmware_version_before,
        'firmware_version_after': result.firmware_version_after,
        'error': result.error
    }
    if username:
        payload['username'] = username
    return payload

def _aviat_broadcast_log(message, level="info", task_id=None):
    entry = {
        "message": message,
        "level": level,
        "task_id": task_id,
        "ts": datetime.utcnow().isoformat() + "Z",
    }
    aviat_global_log_history.append(entry)
    if len(aviat_global_log_history) > AVIAT_GLOBAL_LOG_LIMIT:
        del aviat_global_log_history[: len(aviat_global_log_history) - AVIAT_GLOBAL_LOG_LIMIT]
    for q in list(aviat_global_log_queues):
        try:
            q.put(entry)
        except Exception:
            pass

def _aviat_background_task(task_id, ips, task_types, maintenance_params=None, username=None):
    aviat_tasks[task_id]['status'] = 'running'
    maintenance_params = maintenance_params or {}
    activation_mode = maintenance_params.get('activation_mode')

    def log_callback(message, level):
        _aviat_broadcast_log(message, level, task_id=task_id)
        if task_id in aviat_log_queues:
            aviat_log_queues[task_id].put({'message': message, 'level': level})

    results = []

    def should_abort():
        return aviat_tasks.get(task_id, {}).get('abort') is True

    if activation_mode == "scheduled":
        result_list = aviat_process_radios_parallel(
            ips,
            task_types,
            maintenance_params,
            should_abort=should_abort,
            callback=log_callback,
        )
        results.extend([_aviat_result_dict(r, username=username) for r in result_list])
    elif activation_mode == "immediate":
        activation_limit = int(os.environ.get("AVIAT_ACTIVATION_MAX", "20"))
        result_list = aviat_process_radios_parallel(
            ips,
            task_types,
            maintenance_params,
            should_abort=should_abort,
            callback=log_callback,
            max_workers=activation_limit,
        )
        results.extend([_aviat_result_dict(r, username=username) for r in result_list])
    else:
        for ip in ips:
            if task_id not in aviat_tasks:
                break
            if should_abort():
                aviat_tasks[task_id]['status'] = 'aborted'
                break
            result = aviat_process_radio(
                ip,
                task_types,
                callback=log_callback,
                maintenance_params=maintenance_params,
                should_abort=should_abort,
            )
            results.append(_aviat_result_dict(result, username=username))

    if activation_mode == "scheduled":
        remaining_tasks = _aviat_clean_remaining_tasks([t for t in task_types if t != "all"])
        for res in results:
            if res.get("status") == "loading":
                aviat_loading_queue.append({
                    "ip": res["ip"],
                    "remaining_tasks": remaining_tasks,
                    "maintenance_params": maintenance_params,
                    "activation_at": maintenance_params.get("activation_at"),
                    "username": username or "aviat-tool",
                    "target_version": res.get("firmware_downloaded_version"),
                    "started_at": datetime.utcnow().isoformat() + "Z",
                })
                _aviat_queue_upsert(res["ip"], {
                    "status": "loading",
                    "firmwareStatus": "loading",
                    "username": username or "aviat-tool",
                })
            elif res.get("status") == "scheduled":
                aviat_scheduled_queue.append({
                    "ip": res["ip"],
                    "remaining_tasks": remaining_tasks,
                    "maintenance_params": maintenance_params,
                    "activation_at": maintenance_params.get("activation_at"),
                    "username": username or "aviat-tool",
                })
                _aviat_queue_upsert(res["ip"], {
                    "status": "scheduled",
                    "firmwareStatus": "scheduled",
                    "username": username or "aviat-tool",
                })
            elif res.get("status") == "reboot_required":
                aviat_reboot_queue.append({
                    "ip": res["ip"],
                    "reason": res.get("error") or "Uptime exceeds limit",
                    "remaining_tasks": _aviat_remaining_tasks_for_reboot(task_types),
                    "maintenance_params": maintenance_params,
                    "username": username or "aviat-tool",
                    "started_at": datetime.utcnow().isoformat() + "Z",
                })
                _aviat_queue_upsert(res["ip"], {
                    "status": "reboot_required",
                    "username": username or "aviat-tool",
                })
        _aviat_save_scheduled_queue()
        _aviat_save_loading_queue()
        _aviat_save_reboot_queue()

    if aviat_tasks.get(task_id, {}).get('abort'):
        remaining = [ip for ip in ips if ip not in [r['ip'] for r in results]]
        for ip in remaining:
            results.append({
                'ip': ip,
                'success': False,
                'status': 'aborted',
                'firmware_downloaded': False,
                'firmware_scheduled': False,
                'firmware_activated': False,
                'password_changed': False,
                'snmp_configured': False,
                'buffer_configured': False,
                'sop_checked': False,
                'sop_passed': False,
                'sop_results': [],
                'firmware_version_before': None,
                'firmware_version_after': None,
                'error': 'Aborted'
            })
        aviat_tasks[task_id]['status'] = 'aborted'
    else:
        aviat_tasks[task_id]['status'] = 'completed'

    # Ensure reboot-required devices are captured even outside scheduled mode.
    for res in results:
        if res.get("status") == "reboot_required":
            if not any(e.get("ip") == res.get("ip") for e in aviat_reboot_queue):
                aviat_reboot_queue.append({
                    "ip": res.get("ip"),
                    "reason": res.get("error") or "Uptime exceeds limit",
                    "remaining_tasks": _aviat_remaining_tasks_for_reboot(task_types),
                    "maintenance_params": maintenance_params,
                    "username": username or "aviat-tool",
                    "started_at": datetime.utcnow().isoformat() + "Z",
                })
    _aviat_save_reboot_queue()

    aviat_tasks[task_id]['results'] = results
    for res in results:
        _aviat_queue_update_from_result(res, username=username)
        _log_aviat_activity(res)
    _aviat_save_shared_queue()
    _aviat_save_reboot_queue()

    if task_id in aviat_log_queues:
        aviat_log_queues[task_id].put(None)


@app.route('/api/aviat/run', methods=['POST'])
def aviat_run_tasks():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    data = request.json or {}
    ips = data.get('ips', [])
    task_types = data.get('tasks', [])
    m_params = data.get('maintenance_params', {})
    username = data.get('username')

    if not ips:
        return jsonify({'error': 'No IPs provided'}), 400

    for ip in ips:
        _aviat_queue_upsert(ip, {
            "status": "processing",
            "username": username or "aviat-tool",
        })
    _aviat_save_shared_queue()

    task_id = str(uuid.uuid4())
    aviat_tasks[task_id] = {
        'status': 'pending',
        'abort': False,
        'ips': ips,
        'tasks': task_types,
        'results': []
    }
    aviat_log_queues[task_id] = queue.Queue()

    thread = threading.Thread(
        target=_aviat_background_task, args=(task_id, ips, task_types, m_params, username)
    )
    thread.start()

    return jsonify({'task_id': task_id})


@app.route('/api/aviat/activate-scheduled', methods=['POST'])
def aviat_activate_scheduled():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    data = request.json or {}
    force = data.get("force", True)
    request_ips = data.get("ips") or []
    request_remaining = _aviat_clean_remaining_tasks(data.get("remaining_tasks") or [])
    request_maintenance = data.get("maintenance_params") or {}
    request_activation_at = data.get("activation_at")
    client_hour = data.get("client_hour")
    client_minute = data.get("client_minute")

    # Only enforce activation window when force is explicitly disabled.
    if not force:
        if client_hour is not None:
            try:
                client_hour = int(client_hour)
                if not (2 <= client_hour < 5):
                    return jsonify({"error": "Outside activation window (2:00 AM - 5:00 AM)"}), 400
            except Exception:
                pass
        else:
            now = datetime.now()
            if not (2 <= now.hour < 5):
                return jsonify({"error": "Outside activation window (2:00 AM - 5:00 AM)"}), 400

    if not aviat_scheduled_queue and not request_ips:
        return jsonify({"error": "No scheduled devices"}), 400

    task_id = str(uuid.uuid4())
    username = data.get("username")

    if request_ips:
        scheduled_map = {entry.get("ip"): entry for entry in aviat_scheduled_queue}
        to_activate = []
        for ip in request_ips:
            scheduled_entry = scheduled_map.get(ip) or {}
            to_activate.append({
                "ip": ip,
                "remaining_tasks": scheduled_entry.get("remaining_tasks", request_remaining),
                "maintenance_params": scheduled_entry.get("maintenance_params", request_maintenance),
                "activation_at": scheduled_entry.get("activation_at", request_activation_at),
                "username": scheduled_entry.get("username") or username,
            })
    else:
        to_activate = list(aviat_scheduled_queue)

    if request_ips:
        request_set = set(request_ips)
        aviat_scheduled_queue[:] = [
            entry for entry in aviat_scheduled_queue
            if entry.get("ip") not in request_set
        ]
        _aviat_save_scheduled_queue()

    for entry in to_activate:
        _aviat_queue_upsert(entry["ip"], {
            "status": "processing",
            "firmwareStatus": "processing",
            "username": entry.get("username") or username or "aviat-tool",
        })
    _aviat_save_shared_queue()

    aviat_tasks[task_id] = {
        'status': 'pending',
        'abort': False,
        'ips': [x["ip"] for x in to_activate],
        'tasks': ['activate'],
        'results': []
    }
    aviat_log_queues[task_id] = queue.Queue()

    def activation_task():
        aviat_tasks[task_id]['status'] = 'running'

        def log_callback(message, level):
            _aviat_broadcast_log(message, level, task_id=task_id)
            if task_id in aviat_log_queues:
                aviat_log_queues[task_id].put({'message': message, 'level': level})

        local_to_activate = list(to_activate)
        if not force and not request_ips:
            now = datetime.now()
            filtered = []
            remaining = []
            for entry in aviat_scheduled_queue:
                activation_at = entry.get("activation_at")
                if activation_at:
                    try:
                        if datetime.fromisoformat(activation_at.replace("Z", "")) <= now:
                            filtered.append(entry)
                        else:
                            remaining.append(entry)
                    except Exception:
                        filtered.append(entry)
                else:
                    filtered.append(entry)
            local_to_activate = filtered
            aviat_scheduled_queue[:] = remaining
            _aviat_save_scheduled_queue()
        elif not request_ips:
            aviat_scheduled_queue.clear()
            _aviat_save_scheduled_queue()

        _aviat_activate_entries(task_id, local_to_activate, username=username)

    thread = threading.Thread(target=activation_task)
    thread.start()
    return jsonify({'task_id': task_id})


@app.route('/api/aviat/scheduled', methods=['GET'])
def aviat_get_scheduled():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    return jsonify({
        "scheduled": [item.get("ip") for item in aviat_scheduled_queue]
    })


@app.route('/api/aviat/loading', methods=['GET'])
def aviat_get_loading():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    return jsonify({
        "loading": [item.get("ip") for item in aviat_loading_queue]
    })


@app.route('/api/aviat/reboot-required', methods=['GET'])
def aviat_get_reboot_required():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    return jsonify({
        "reboot_required": aviat_reboot_queue
    })


@app.route('/api/aviat/reboot-required/run', methods=['POST'])
def aviat_run_reboot_required():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    data = request.json or {}
    request_ips = data.get("ips") or []
    username = data.get("username") or "aviat-tool"

    if request_ips:
        target_entries = [e for e in aviat_reboot_queue if e.get("ip") in set(request_ips)]
    else:
        target_entries = list(aviat_reboot_queue)

    if not target_entries:
        return jsonify({"error": "No reboot-required devices"}), 400

    for entry in target_entries:
        _aviat_queue_upsert(entry["ip"], {
            "status": "rebooting",
            "username": username,
        })
    _aviat_save_shared_queue()

    def reboot_task():
        for entry in target_entries:
            ip = entry.get("ip")
            if not ip:
                continue
            def log_cb(message, level):
                _aviat_broadcast_log(message, level)
            success, err = _aviat_reboot_device(ip, callback=log_cb)
            if not success:
                # Don't fail the workflow outright; keep it in reboot queue for retry.
                _aviat_queue_upsert(ip, {"status": "reboot_pending", "username": username, "error": err})
                continue

            _aviat_queue_upsert(ip, {"status": "rebooting", "username": username})

            # Wait for device to come back before continuing tasks
            try:
                client = wait_for_device_ready_and_reconnect(
                    ip,
                    username=AVIAT_CONFIG.default_username,
                    password=AVIAT_CONFIG.default_password,
                    fallback_password=AVIAT_CONFIG.new_password,
                    callback=log_cb,
                    initial_delay=int(os.environ.get("AVIAT_REBOOT_INITIAL_DELAY", "60")),
                )
                if client:
                    try:
                        client.close()
                    except Exception:
                        pass
                else:
                    raise TimeoutError("Device did not return after reboot.")
            except Exception as e:
                # Keep in reboot queue; do not mark failed.
                _aviat_queue_upsert(ip, {"status": "reboot_pending", "username": username, "error": str(e)})
                continue

            # Continue remaining tasks after reboot
            # Reboot-required entries already store full remaining tasks (including firmware/activate).
            remaining = entry.get("remaining_tasks", [])
            maintenance_params = entry.get("maintenance_params", {}) or {}
            if not remaining:
                _aviat_queue_upsert(ip, {"status": "pending", "username": username})
            else:
                try:
                    result = aviat_process_radio(
                        ip,
                        remaining,
                        callback=log_cb,
                        maintenance_params=maintenance_params,
                    )
                    res_dict = _aviat_result_dict(result, username=username)
                    _aviat_queue_update_from_result(res_dict, username=username)
                    _log_aviat_activity(res_dict)
                except Exception as e:
                    # Don't mark failed due to transient reconnect issues.
                    _aviat_queue_upsert(ip, {"status": "reboot_pending", "username": username, "error": str(e)})
                    continue

            # remove from reboot queue on success
            aviat_reboot_queue[:] = [e for e in aviat_reboot_queue if e.get("ip") != ip]
        _aviat_save_shared_queue()
        _aviat_save_reboot_queue()

    thread = threading.Thread(target=reboot_task)
    thread.start()
    return jsonify({"status": "rebooting", "count": len(target_entries)})


@app.route('/api/aviat/scheduled/sync', methods=['POST'])
def aviat_sync_scheduled():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    data = request.json or {}
    ips = data.get("ips", [])
    remaining_tasks = _aviat_clean_remaining_tasks(data.get("remaining_tasks", []))
    maintenance_params = data.get("maintenance_params", {})
    username = data.get("username") or "aviat-tool"
    activation_at = data.get("activation_at")

    if not isinstance(ips, list) or not ips:
        return jsonify({"error": "No scheduled IPs provided"}), 400

    aviat_scheduled_queue.clear()
    for ip in ips:
        aviat_scheduled_queue.append({
            "ip": ip,
            "remaining_tasks": remaining_tasks,
            "maintenance_params": maintenance_params,
            "activation_at": activation_at,
            "username": username,
        })
    _aviat_save_scheduled_queue()
    return jsonify({"scheduled": [item.get("ip") for item in aviat_scheduled_queue]})

@app.route('/api/aviat/queue', methods=['GET', 'POST'])
def aviat_queue_state():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    if request.method == 'GET':
        return jsonify({"radios": aviat_shared_queue})

    data = request.json or {}
    mode = (data.get("mode") or "replace").lower()
    radios = data.get("radios") or []
    username = data.get("username") or "aviat-tool"

    if mode == "replace":
        aviat_shared_queue.clear()
    if mode in ("replace", "add"):
        for radio in radios:
            ip = radio.get("ip") if isinstance(radio, dict) else str(radio)
            if not ip:
                continue
            updates = radio if isinstance(radio, dict) else {}
            updates.setdefault("status", "pending")
            updates.setdefault("username", username)
            _aviat_queue_upsert(ip, updates)
    if mode == "remove":
        for radio in radios:
            ip = radio.get("ip") if isinstance(radio, dict) else str(radio)
            if ip:
                _aviat_queue_remove(ip)

    _aviat_save_shared_queue()
    return jsonify({"radios": aviat_shared_queue})


@app.route('/api/aviat/check-status', methods=['POST'])
def aviat_check_status():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    data = request.json or {}
    ips = data.get('ips', [])
    if not ips:
        return jsonify({'error': 'No IPs provided'}), 400

    results = []
    with ThreadPoolExecutor(max_workers=AVIAT_CONFIG.max_workers) as executor:
        futures = {executor.submit(aviat_check_device_status, ip): ip for ip in ips}
        for future in as_completed(futures):
            results.append(future.result())

    for res in results:
        ip = res.get("ip")
        if not ip:
            continue
        current = _aviat_queue_find(ip) or {}
        current_status = current.get("status")
        if current_status in ("loading", "scheduled", "manual", "success", "error"):
            continue
        if current_status == "processing" and (res.get("error") or not res.get("reachable", True)):
            continue
        firmware_ok = bool(res.get("firmware") and str(res.get("firmware")).startswith("6."))
        snmp_ok = bool(res.get("snmp_ok"))
        buffer_ok = bool(res.get("buffer_ok"))
        license_ok = res.get("license_ok")
        stp_ok = res.get("stp_ok")
        subnet_ok = res.get("subnet_ok")
        status = "success" if firmware_ok and snmp_ok and buffer_ok else "pending"
        if res.get("error"):
            status = "error"
        _aviat_queue_upsert(ip, {
            "status": status,
            "firmwareStatus": "success" if firmware_ok else "pending",
            "snmpStatus": "success" if snmp_ok else "pending",
            "bufferStatus": "success" if buffer_ok else "pending",
            "sopStatus": "success" if firmware_ok and snmp_ok and buffer_ok else "pending",
            "licenseStatus": "success" if license_ok is True else ("error" if license_ok is False else "pending"),
            "stpStatus": "success" if stp_ok is True else ("error" if stp_ok is False else "pending"),
            "subnetStatus": "success" if subnet_ok is True else ("error" if subnet_ok is False else "pending"),
            "licenseDetail": res.get("license_detail"),
            "stpDetail": res.get("stp_detail"),
            "subnetDetail": res.get("subnet_actual"),
        })
    _aviat_save_shared_queue()

    return jsonify({'results': results})


@app.route('/api/aviat/fix-stp', methods=['POST'])
def aviat_fix_stp():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    data = request.json or {}
    ip = data.get("ip")
    if not ip:
        return jsonify({"error": "Missing ip"}), 400

    def log_cb(message, level):
        _aviat_broadcast_log(message, level)

    try:
        client = _aviat_connect_with_fallback(ip, callback=log_cb)
        client.send_command("config terminal", timeout=8)
        client.send_command("spanning-tree administrative-status down", timeout=8)
        client.send_command("commit", timeout=10)
        client.send_command("exit", timeout=5)
        client.close()
        _aviat_broadcast_log(f"[{ip}] STP administrative-status set to down.", "success")
        return jsonify({"status": "ok"})
    except Exception as exc:
        _aviat_broadcast_log(f"[{ip}] STP fix failed: {exc}", "error")
        return jsonify({"error": str(exc)}), 500


@app.route('/api/aviat/abort/<task_id>', methods=['POST'])
def aviat_abort_task(task_id):
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    if task_id not in aviat_tasks:
        return jsonify({'error': 'Task not found'}), 404
    aviat_tasks[task_id]['abort'] = True
    return jsonify({'status': 'aborting'})


@app.route('/api/aviat/stream/<task_id>')
def aviat_stream_logs(task_id):
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    if task_id not in aviat_log_queues:
        return jsonify({'error': 'Task not found'}), 404

    def generate():
        q = aviat_log_queues[task_id]
        while True:
            try:
                data = q.get(timeout=15)
            except queue.Empty:
                # Keep the connection alive to avoid proxy timeouts.
                yield ": keep-alive\n\n"
                continue
            if data is None:
                break
            yield f"data: {json.dumps(data)}\n\n"

    response = Response(generate(), mimetype='text/event-stream')
    response.headers['Cache-Control'] = 'no-cache'
    response.headers['X-Accel-Buffering'] = 'no'
    return response


@app.route('/api/aviat/stream/global')
def aviat_stream_global():
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    q = queue.Queue()
    aviat_global_log_queues.add(q)

    def generate():
        # Send backlog first
        for entry in aviat_global_log_history[-200:]:
            yield f"data: {json.dumps(entry)}\n\n"
        while True:
            try:
                data = q.get(timeout=15)
            except queue.Empty:
                yield ": keep-alive\n\n"
                continue
            if data is None:
                break
            yield f"data: {json.dumps(data)}\n\n"

    response = Response(generate(), mimetype='text/event-stream')
    response.headers['Cache-Control'] = 'no-cache'
    response.headers['X-Accel-Buffering'] = 'no'

    @response.call_on_close
    def _cleanup():
        aviat_global_log_queues.discard(q)

    return response


@app.route('/api/aviat/status/<task_id>')
def aviat_get_status(task_id):
    if not HAS_AVIAT:
        return jsonify({'error': 'Aviat backend not available'}), 503
    if task_id not in aviat_tasks:
        return jsonify({'error': 'Task not found'}), 404
    return jsonify(aviat_tasks[task_id])


@app.route('/api/generate-ftth-bng', methods=['POST'])
def generate_ftth_bng():
    """Generate complete FTTH BNG configuration from the strict template."""
    try:
        data = request.get_json() or {}
        print(f"[FTTH BNG] Received configuration request: {data.get('deployment_type', 'unknown')}")
        config = render_ftth_config(data)
        print(f"[FTTH BNG] Generated configuration: {len(config)} characters")
        return jsonify({
            'success': True,
            'config': config,
        })
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"[ERROR] FTTH BNG generation failed: {e}")
        print(error_details)
        return jsonify({
            'success': False,
            'error': str(e),
            'details': error_details
        }), 500


@app.route('/api/ftth-home/mf2-package', methods=['POST'])
def generate_ftth_home_mf2_package():
    """Generate MF2 ZIP with updated gateway and primary IP in 2-ihub-startup 831.xml."""
    data = request.get_json() or {}
    gateway_ip = str(data.get('gateway_ip', '')).strip()
    primary_ip = str(data.get('primary_ip', '')).strip()
    olt_name = str(data.get('olt_name', '')).strip()

    if not gateway_ip or not primary_ip or not olt_name:
        return jsonify({'error': 'gateway_ip, primary_ip, and olt_name are required'}), 400

    try:
        if '/' in gateway_ip:
            interface = ipaddress.ip_interface(gateway_ip)
            if interface.network.prefixlen != 29:
                return jsonify({'error': 'gateway_ip must be a /29'}), 400
            gateway_ip = str(interface.ip)
        else:
            ipaddress.ip_address(gateway_ip)
    except ValueError:
        return jsonify({'error': 'gateway_ip must be a valid IPv4 address'}), 400

    primary_addr = None
    try:
        if '/' in primary_ip:
            interface = ipaddress.ip_interface(primary_ip)
            if interface.network.prefixlen != 29:
                return jsonify({'error': 'primary_ip must be a /29'}), 400
            primary_addr = str(interface.ip)
        else:
            ipaddress.ip_address(primary_ip)
            primary_addr = primary_ip
    except ValueError:
        return jsonify({'error': 'primary_ip must be a valid IPv4 address'}), 400

    base_dir = Path(__file__).parent / 'MF2'
    if not base_dir.exists():
        return jsonify({'error': 'MF2 template directory not found'}), 500

    startup_path = base_dir / '2-ihub-startup 831.xml'
    if not startup_path.exists():
        return jsonify({'error': '2-ihub-startup 831.xml not found'}), 500

    xml_text = startup_path.read_text(encoding='utf-8')
    gateway_re = re.compile(
        r'(<static-routes>.*?<next-hop>\s*<ip-address>)([^<]+)(</ip-address>)',
        re.DOTALL
    )
    primary_re = re.compile(
        r'(<ipv4>\s*<primary>\s*<address>)([^<]+)(</address>)',
        re.DOTALL
    )

    if not gateway_re.search(xml_text):
        return jsonify({'error': 'gateway ip-address tag not found in XML'}), 500
    if not primary_re.search(xml_text):
        return jsonify({'error': 'primary address tag not found in XML'}), 500

    xml_text = gateway_re.sub(rf'\g<1>{gateway_ip}\g<3>', xml_text, count=1)
    xml_text = primary_re.sub(rf'\g<1>{primary_addr}\g<3>', xml_text, count=1)

    safe_olt = re.sub(r'[^A-Za-z0-9._-]+', '_', olt_name).strip('_') or 'MF2'
    zip_name = f"MF2_{safe_olt}.zip"

    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file_path in base_dir.iterdir():
            if not file_path.is_file():
                continue
            arcname = f"MF2/{file_path.name}"
            if file_path.name == startup_path.name:
                zipf.writestr(arcname, xml_text)
            else:
                zipf.write(file_path, arcname)

    zip_buffer.seek(0)
    return send_file(
        zip_buffer,
        mimetype='application/zip',
        as_attachment=True,
        download_name=zip_name
    )

UI_DIR = Path(__file__).parent


def _send_ui_file(filename: str, mimetype: str = 'text/html'):
    path = UI_DIR / filename
    if not path.exists():
        abort(404)
    resp = make_response(send_file(str(path), mimetype=mimetype))
    resp.headers['Cache-Control'] = 'no-store, max-age=0'
    resp.headers['Pragma'] = 'no-cache'
    resp.headers['Expires'] = '0'
    return resp


@app.route('/')
@app.route('/app')
@app.route('/app/')
@app.route('/NOC-configMaker.html')
def serve_app_html():
    """Serve the main SPA HTML (and support /app so nginx doesn't need a separate static root)."""
    return _send_ui_file('NOC-configMaker.html')


@app.route('/login')
@app.route('/login.html')
def serve_login_html():
    return _send_ui_file('login.html')


@app.route('/change-password')
@app.route('/change-password.html')
def serve_change_password_html():
    return _send_ui_file('change-password.html')


@app.route('/<path:path>')
def serve_ui_catchall(path: str):
    # Never catch API routes.
    if path.startswith('api/'):
        abort(404)
    # Serve known HTML pages directly.
    if path.endswith('.html'):
        name = Path(path).name
        if name in {'NOC-configMaker.html', 'login.html', 'change-password.html'}:
            return _send_ui_file(name)
    # Everything else falls back to the SPA HTML so deep links work.
    return serve_app_html()


if __name__ == '__main__':
    print("\n" + "=" * 50)
    print("NOC Config Maker - AI Backend Server")
    print("=" * 50)
    print(f"AI Provider: {AI_PROVIDER.upper()}")

    if AI_PROVIDER == 'ollama':
        print(f"Ollama Model: {OLLAMA_MODEL}")
        print(f"Ollama URL: {OLLAMA_API_URL}")
        print("\n[!] Make sure Ollama is installed and running!")
        print("    Install: https://ollama.com/download")
        print(f"    Then run: ollama pull {OLLAMA_MODEL}")
    else:
        print(f"OpenAI API Key: {'[CONFIGURED]' if OPENAI_API_KEY else '[MISSING]'}")

    print("\nEndpoints:")
    print("  POST /api/validate-config      - Validate RouterOS config")
    print("  POST /api/suggest-config       - Get AI suggestions")
    print("  POST /api/translate-config     - Translate configurations")
    print("  POST /api/apply-compliance     - Apply RFC-09-10-25 standards")
    print("  POST /api/autofill-from-export - Parse exported config")
    print("  POST /api/explain-config       - Explain config sections")
    print("  GET  /api/get-config-policies  - List config policies")
    print("  GET  /api/health               - Health check")

    if HAS_COMPLIANCE:
        print("\n[COMPLIANCE] RFC-09-10-25 enforcement is ENABLED")
    else:
        print("\n[WARN] RFC-09-10-25 enforcement is DISABLED (reference not found)")

    if HAS_AVIAT and AVIAT_AUTO_ACTIVATE:
        print("\n[AVIAT] Auto-activation scheduler is ENABLED")
        threading.Thread(target=_aviat_auto_activate_loop, daemon=True).start()
    elif HAS_AVIAT:
        print("\n[AVIAT] Auto-activation scheduler is DISABLED")
    if HAS_AVIAT:
        print("[AVIAT] Firmware loading checker is ENABLED")
        threading.Thread(target=_aviat_loading_check_loop, daemon=True).start()

    print("\nStarting server on http://0.0.0.0:5000")
    print("=" * 50 + "\n")
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)
